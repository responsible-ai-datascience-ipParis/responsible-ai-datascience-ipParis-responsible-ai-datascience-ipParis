{
  "version": "https://jsonfeed.org/version/1",
  "title": "Bloggin on Responsible AI",
  "home_page_url": "http://localhost:1313/",
  "feed_url": "http://localhost:1313/feed.json",
  "description": "Bloggin on Responsible AI",
  "favicon": "http://localhost:1313//assets/favicon.ico",
  "expired": false,
  "author": {
    "name": "Students from M2 Data Science IP Paris",
    "url": "http://localhost:1313/"
  },
  "items": [
    
    

    
    {
      "id": "9f7cea94a1198e99d2efbbf0a65bb95637f7f7b0",
      "title": "Robust or Fair",
      "summary": "",
      "content_text": "To be Robust or to be Fair: Towards Fairness in Adversarial Training\rAuthors: Maryem Hajji \u0026 Cément Teulier\rTable of Contents Abstract Introduction Initial Analysis Previous Studies Theoretical Demonstration Model Fairness Requirements Practical Algorithms Experimentation Conclusion References Abstract This blog post retraces the study conducted in the paper \u0026ldquo;To be Robust or to be Fair: Towards Fairness in Adversarial Training\u0026rdquo; and written by Han Xu, Xiaorui Liu, Yaxin Li, Yaxin Li, Anil K. Jain and Jiliang Tang.\nTheir study is based on a simple observation: while adversarial training has been shown to improve model\u0026rsquo;s robustness, it also introduces several performances disparities among different data groups.\nTo address this issue, the authors present the Fair-Robust-Learning (FRL) framework that aims to reduce such unfairness.\nIntroduction Nowadays, Machine Learning algorithms and Artificial Intelligence are becoming more and more omnipresent in all kinds of jobs. If many of these models are developed to replace human tasks, it is of key importance that they do not reproduce the same mistakes. In fact, human decision making can sometimes be considered \u0026ldquo;unfair\u0026rdquo;, a trait that must not be present in Machine Learning. But as we push our models to be as precise as possible, one question stands out: can we find the good balance between accuracy and equity ?\nDiving into this topic, we focus our study on adversarial training algorithms. Indeed, it has been shown that there is a significant issue in adversarial training for deep neural networks: while such training boosts the model\u0026rsquo;s defenses against adversarial attacks, it unfortunately leads to significant differences in how well the model performs across various types of data. For instance, detailed observations on CIFAR-10 dataset show a non-negligeable difference in the model\u0026rsquo;s performance between \u0026ldquo;car\u0026rdquo; and \u0026ldquo;cat\u0026rdquo; classes (details of this example in our section 1.1).\nThis phenomenon raises concern on concrete topics like the safety of autonomous driving vehicules or facial recognition while also creating ethical problems by discriminating certain classes. To put a word on it, the authors have identified this issue as the robust-fairness problem of adversarial training.\n1. Initial Analysis We recall here the previous studies conducted by the authors that allowed them to identify the existence of the robust-fairness problem.\n1.1 Previous Studies For their first analysis, the authors have decided to study algorithms like the PGD ( Projected Gradient Descent) adversarial training and TRADES ( Theoretically Principled Trade-off between Robustness and Accuracy for Deep Learning ) on the CIFAR-10 dataset. The investigation is made using a PreAct-ResNet18 model structure under specific adversarial attack constraints. The results they obtained are as follows:\nAs we can see, natural training maintains a consistent standard error across classes and a consistent robust error rate when faced with 8/255 PGD attacks. However, in the case of adversarial training, some huge disparities appear. Going back to our introduction\u0026rsquo;s example with \u0026ldquo;cats\u0026rdquo; and \u0026ldquo;cars\u0026rdquo;, we observe that the standard and robust errors for \u0026ldquo;car\u0026rdquo; class ( respectively 6% and 34% ) are significantly lower than those of the \u0026ldquo;cat\u0026rdquo; class ( respectively 33% and 82% ). The results on the TRADES, altough not depicted here, also show some great disparities between certain classes.\nTo support this graphical study, the authors also present statistical evidence of this phenomenom throughout metrics like the Standard Deviation (SD) or the Normalized SD (NSD) of class-wide error. Once again, these metrics reveal that adversarial training indeed results in greater disparities across classes in both standard and robust performance compared to natural training.\nPotential Causes While the authors succeeded in identifying the problem of fairness, they also aimed to understand where it was coming from. From what they observed, it seems that the fairness issue particularly disadvantages classes that are inherently more challenging to classify. Adversarial training in fact tends to increase the standard errors for \u0026ldquo;harder\u0026rdquo; classes (like \u0026ldquo;cat\u0026rdquo;) significantly more than for \u0026ldquo;easier\u0026rdquo; classes (such as \u0026ldquo;car\u0026rdquo;).\n1.2 Theoretical Demonstration From the experiments on the potential causes of the fairness issue, the authors made the following hypotetis: Adversarial training makes hard classes even harder to classify or classify robustly. In this section, we review the theoretical proof of this hypothesis.\nFor this analysis, we place ourselves in the case of a binary classification task, using a mixed Gaussian distribution to create two classes with distinct levels of classification difficulty. Thus, adversarial training does not notably lower the average standard error but it shifts the decision boundary in a way that favours the \u0026rsquo;easier\u0026rsquo; class at the expense of the \u0026lsquo;harder\u0026rsquo; class.\nPrerequisites The classification model, denoted $f$, is a mapping $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$ from input data space $\\mathcal{X}$ and output labels $\\mathcal{Y}$ defined as $f(x) = \\text{sign}(\\mathbf{w} \\cdot \\mathbf{x} + b)$ with parameters $\\mathbf{w}$ and $b$ The standard error for a classifier $f$ generally is: $R_{\\text{nat}}(f) = \\Pr(f(\\mathbf{x}) \\neq y)$ The robust error for a classifier $f$ generally is: $R_{\\text{rob}}(f) = \\Pr(\\exists \\delta, |\\delta| \\leq \\epsilon, \\text{s.t. } f(\\mathbf{x} + \\delta) \\neq y)$ (the probability of a perturbation existing that would cause the model to produce an incorrect prediction) The standard error conditional on a specific class $\\{Y = y\\}$ is represented by $R_{\\text{nat}}(f; y)$ Theoretical Experiment We generate a simple example of the binary classification task that we presented at the beginning of section 1.2. The data therefore comes from two classes $\\mathcal{Y} = { \\{-1, +1\\}}$, with each class\u0026rsquo; data following a Gaussian distribution $\\mathcal{D}$ centered on $-\\theta$ and $\\theta$ respectively. It is important to specify that there is a $K$-factor difference between the variance of the two classes defined as follows: $\\sigma_{+1} : \\sigma_{-1} = K : 1$ and $K \u0026gt; 1$.\nThe authors then use the theorem stating that:\nTheorem: In the case of a data distribution $D$ like the one above, the optimal linear classifier $f_{\\text{nat}}$ which minimizes the average standard classification error is: $$ f_{\\text{nat}} = \\arg\\min_f \\Pr(f(\\mathbf{x}) \\neq y) $$.\nWith that theorem and after computations, the authors prove that the class \u0026ldquo;$+1$\u0026rdquo; as a larger standard error than the class \u0026ldquo;$-1$\u0026rdquo;.\nOverall, this result shows well that the class \u0026ldquo;$+1$\u0026rdquo;, characterized by a larger variance, tends to be more challenging to classify than the class\u0026quot;$-1$\u0026quot;; a result confirming the hypothesis initially made.\n2. Model In this section, we present the Fair Robust Learning model (FRL).\n2.1 Fairness Requirements The authors introduced the concepts of Equalized Accuracy and Equalized Robustness, emphasizing the importance of providing equal prediction quality and resilience against adversarial attacks across different groups. To achieve this balance, the authors propose a Fair Robust Learning (FRL) strategy. This framework addresses fairness issues in adversarial training by aiming to minimize overall robust error while ensuring fairness constraints are met. They separate robust error into standard error and boundary error, allowing independent solving of the unfairness of both errors. [ref 7]\nThe training objective thus becomes minimizing the sum of standard error and boundary error while adhering to fairness constraints that ensure no significant disparities in error rates among classes. Techniques from prior research are leveraged to optimize boundary errors during training.\n2.2 Practical Algorithms This section explores effective methods to implement and address the challenges outlined in the training objective, such as the Reweight strategy. In order to implement it, Lagrange multipliers are introduced, denoted as $φ = (φ_{nat}^{\\text{i}}, φ_{bndy}^{\\text{i}})$ where each multiplier corresponds to a fairness constraint. These multipliers are non-negative and play a crucial role in the optimization process.\nThe approach involves forming a Lagrangian, represented by the function $L(f, φ)$, which combines the standard error ($R_{\\text{nat}}(f)$) and boundary error ($R_{\\text{bndy}}(f)$) terms along with the fairness constraints. The Lagrangian acts as a guide for the optimization process, helping to balance the trade-off between minimizing errors and satisfying fairness requirements.\n$$ \\scriptsize{ L(f, \\phi) = R_{\\text{nat}}(f) + R_{\\text{bndy}}(f) + \\sum_{i=1}^{Y} \\phi_{\\text{nat}}^i \\left( R_{\\text{nat}}(f, i) - R_{\\text{nat}}(f) - \\tau_1 \\right)^+ + \\sum_{i=1}^{Y} \\phi_{\\text{bndy}}^i \\left( R_{\\text{bndy}}(f, i) - R_{\\text{bndy}}(f) - \\tau_2 \\right)^+ } $$\nThe optimization problem is then framed as a max-min game between the classifier $f$ and the Lagrange multipliers $φ$. The objective is to maximize the fairness constraints while minimizing the Lagrangian function, which encapsulates both standard and boundary errors.\nOn the other hand, the Reweight strategy presents a limitation particularly in mitigating boundary errors for specific classes. While upweighting the cost for standard errors ($R_{\\text{nat}}(f, i)$) can penalize large errors and improve performance for disadvantaged groups, solely upweighting the boundary error ($R_{\\text{bndy}}(f, i)$) for a class doesn\u0026rsquo;t effectively reduce its boundary error.\nTo overcome this challenge, the Remargin strategy introduces an alternative approach by enlarging the perturbation margin ($\\epsilon$) during adversarial training. This strategy is inspired by previous research showing that increasing the margin during adversarial training can enhance a model\u0026rsquo;s robustness against attacks under the current intensity.[ref 8]\nSpecifically, the Remargin strategy involves adjusting the adversarial margin for generating adversarial examples during training, focusing on specific classes where boundary errors are significant. This adjustment aims to improve the robustness of these classes and reduce their large boundary errors ($R_{\\text{bndy}}(f, i)$).\n3. Experimentation In this section, we reproduce the experimental methodology and setup used to evaluate the effectiveness of the proposed Fair Robust Learning (FRL) framework in constructing robust deep neural network (DNN) models.\nFirstly, we train a fairly simple model on the Fashion MNIST dataset, then we test out torchattack\u0026rsquo;s PGD on our naturally trained model, Then we will adversarially train the same architecture to see if we can identify this unfairness.\nAs we can see above, the naturally trained model has low standard error, but high PGD error. The adversarially trained model, in contrast, has a much lower PGD error, but higher standard error, and higher disparity between the classes.\nSecond, we implement the FRL algorithm (Reweight strategy) which formulates the learning problem as a cost-sensitive classification that penalizes those classes which violate fairness. Essentially, we create multipliers that up or down weight the loss of classes based on how fair or unfair they are with respect to the average across all classes.\nThe following is the FRL Algorithm outlined in the paper:\nWe made a setup to run the process 3 times: once with equal alpha values, once with an alpha ratio that favors the natural error, and one with an alpha ratio that favors the boundary error.\nIn accordance with the authors of the paper, we find that the alpha ratio that favors the natural error is successful in preventing the unfairness of the standard error in the model, and does help somewhat with the unfairness of the PGD error. On the other hand, we notice that the algorithm struggles to improve the worst-case boundary error, leading to disparities in robustness performance across different classes.\nConclusion In conclusion, the studied article discusses the development and implementation of Fair Robust Learning (FRL) strategies to address fairness concerns in adversarial training of deep neural networks. The objective of these strategies is to achieve both equalized accuracy and robustness across different classes.\nThe Reweight strategy aims to minimize overall robust error while adhering to fairness constraints by adjusting training weights based on class-wise errors while the Remargin strategy enlarges the perturbation margin during adversarial training to improve robustness and reduce boundary errors.\nFinally, The FRL framework combines these strategies to mitigate fairness issues and improve model performance across various classes. These approaches represent promising steps towards achieving fairness in robust deep learning models.\nReferences [1] Han Xu, Xiaorui Liu, Yaxin Li, Anil K. Jain, Jiliang Tang1. To be Robust or to be Fair: Towards Fairness in Adversarial Training. 2021.\n[2] Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. 2014.\n[3] Morgulis, N., Kreines, A., Mendelowitz, S., and Weisglass, Y. Fooling a real car with adversarial traffic signs. 2019.\n[4] Sharif, M., Bhagavatula, S., Bauer, L., and Reiter, M. K. Accessorize to a crime: Real and stealthy attacks on state of-the-art face recognition. In Proceedings of the 2016 acm sigsac conference on computer and communications security, pp. 1528–1540, 2016.\n[5] Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.\n[6] He, H. and Garcia, E. A. Learning from imbalanced data. IEEE Transactions on knowledge and data engineering, 21(9):1263–1284. 2009.\n[7] Zhang, H., Yu, Y., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jordan, M. I. Theoretically principled trade-off between robustness and accuracy. 2019.\n[8] Tramer, F., Behrmann, J., Carlini, N., Papernot, N., and Ja- ` cobsen, J.-H. Fundamental tradeoffs between invariance and sensitivity to adversarial perturbations. In International Conference on Machine Learning, pp. 9561–9571. PMLR. 2020.\n",
      "content_html": "\u003ch1 style=\"font-size: 36px;\"\u003eTo be Robust or to be Fair: Towards Fairness in Adversarial Training\u003c/h1\u003e\r\n\u003ch1 style=\"font-size: 24px;\"\u003eAuthors: Maryem Hajji \u0026 Cément Teulier\u003c/h1\u003e\r\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eAbstract\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eInitial Analysis\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-2.1\"\u003ePrevious Studies\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2.2\"\u003eTheoretical Demonstration\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eModel\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-3.1\"\u003eFairness Requirements\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3.2\"\u003ePractical Algorithms\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eExperimentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eThis blog post retraces the study conducted in the paper \u0026ldquo;To be Robust or to be Fair: Towards Fairness in Adversarial Training\u0026rdquo; and written by Han Xu, Xiaorui Liu, Yaxin Li, Yaxin Li, Anil K. Jain and Jiliang Tang.\u003c/p\u003e\n\u003cp\u003eTheir study is based on a simple observation: while adversarial training has been shown to improve model\u0026rsquo;s robustness, it also introduces several performances disparities among different data groups.\u003c/p\u003e\n\u003cp\u003eTo address this issue, the authors present the Fair-Robust-Learning (FRL) framework that aims to reduce such unfairness.\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eNowadays, Machine Learning algorithms and Artificial Intelligence are becoming more and more omnipresent in all kinds of jobs. If many of these models are developed to replace human tasks, it is of key importance that they do not reproduce the same mistakes. In fact, human decision making can sometimes be considered \u0026ldquo;unfair\u0026rdquo;, a trait that must not be present in Machine Learning. But as we push our models to be as precise as possible, one question stands out: can we find the good balance between accuracy and equity ?\u003c/p\u003e\n\u003cp\u003eDiving into this topic, we focus our study on adversarial training algorithms.\nIndeed, it has been shown that there is a significant issue in adversarial training for deep neural networks: while such training boosts the model\u0026rsquo;s defenses against adversarial attacks, it unfortunately leads to significant differences in how well the model performs across various types of data.\nFor instance, detailed observations on CIFAR-10 dataset show a non-negligeable difference in the model\u0026rsquo;s performance between \u0026ldquo;car\u0026rdquo; and \u0026ldquo;cat\u0026rdquo; classes (details of this example in our section 1.1).\u003c/p\u003e\n\u003cp\u003eThis phenomenon raises concern on concrete topics like the safety of autonomous driving vehicules or facial recognition while also creating ethical problems by discriminating certain classes.\nTo put a word on it, the authors have identified this issue as the \u003cstrong\u003erobust-fairness\u003c/strong\u003e problem of adversarial training.\u003c/p\u003e\n\u003ch2 id=\"section-2\"\u003e1. Initial Analysis\u003c/h2\u003e\n\u003cp\u003eWe recall here the previous studies conducted by the authors that allowed them to identify the existence of the robust-fairness problem.\u003c/p\u003e\n\u003ch2 id=\"section-2.1\"\u003e1.1 Previous Studies\u003c/h2\u003e\n\u003cp\u003eFor their first analysis, the authors have decided to study algorithms like the PGD ( Projected Gradient Descent) adversarial training and TRADES ( Theoretically Principled Trade-off between Robustness and Accuracy for Deep Learning ) on the CIFAR-10 dataset.\nThe investigation is made using a PreAct-ResNet18 model structure under specific adversarial attack constraints.\nThe results they obtained are as follows:\u003c/p\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"/images/Hajji_Teulier/cat_car.png\"\r\n  alt=\"Paper Initial Results\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003eAs we can see, natural training maintains a consistent standard error across classes and a consistent robust error rate when faced with 8/255 PGD attacks. However, in the case of adversarial training, some huge disparities appear. Going back to our introduction\u0026rsquo;s example with \u0026ldquo;cats\u0026rdquo; and \u0026ldquo;cars\u0026rdquo;, we observe that the standard and robust errors for \u0026ldquo;car\u0026rdquo; class ( respectively 6% and 34% ) are significantly lower than those of the \u0026ldquo;cat\u0026rdquo; class ( respectively 33% and 82% ). The results on the TRADES, altough not depicted here, also show some great disparities between certain classes.\u003c/p\u003e\n\u003cp\u003eTo support this graphical study, the authors also present statistical evidence of this phenomenom throughout metrics like the Standard Deviation (SD) or the Normalized SD (NSD) of class-wide error. Once again, these metrics reveal that adversarial training indeed results in greater disparities across classes in both standard and robust performance compared to natural training.\u003c/p\u003e\n\u003ch3 id=\"potential-causes\"\u003ePotential Causes\u003c/h3\u003e\n\u003cp\u003eWhile the authors succeeded in identifying the problem of fairness, they also aimed to understand where it was coming from. From what they observed, it seems that the fairness issue particularly disadvantages classes that are inherently more challenging to classify. Adversarial training in fact tends to increase the standard errors for \u0026ldquo;harder\u0026rdquo; classes (like \u0026ldquo;cat\u0026rdquo;) significantly more than for \u0026ldquo;easier\u0026rdquo; classes (such as \u0026ldquo;car\u0026rdquo;).\u003c/p\u003e\n\u003ch2 id=\"section-2.2\"\u003e1.2 Theoretical Demonstration\u003c/h2\u003e\n\u003cp\u003eFrom the experiments on the potential causes of the fairness issue, the authors made the following hypotetis: Adversarial training makes hard classes even harder to classify or classify robustly.\nIn this section, we review the theoretical proof of this hypothesis.\u003c/p\u003e\n\u003cp\u003eFor this analysis, we place ourselves in the case of a binary classification task, using a mixed Gaussian distribution to create two classes with distinct levels of classification difficulty. Thus, adversarial training does not notably lower the average standard error but it shifts the decision boundary in a way that favours the \u0026rsquo;easier\u0026rsquo; class at the expense of the \u0026lsquo;harder\u0026rsquo; class.\u003c/p\u003e\n\u003ch3 id=\"prerequisites\"\u003ePrerequisites\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe \u003cstrong\u003eclassification model\u003c/strong\u003e, denoted $f$, is a mapping  $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$ from input data space $\\mathcal{X}$ and output labels $\\mathcal{Y}$ defined as $f(x) = \\text{sign}(\\mathbf{w} \\cdot \\mathbf{x} + b)$ with parameters $\\mathbf{w}$ and $b$\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003estandard error\u003c/strong\u003e for a classifier $f$ generally is: $R_{\\text{nat}}(f) = \\Pr(f(\\mathbf{x}) \\neq y)$\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003erobust error\u003c/strong\u003e for a classifier $f$ generally is: $R_{\\text{rob}}(f) = \\Pr(\\exists \\delta, |\\delta| \\leq \\epsilon, \\text{s.t. } f(\\mathbf{x} + \\delta) \\neq y)$ (the probability of a perturbation existing that would cause the model to produce an incorrect prediction)\u003c/li\u003e\n\u003cli\u003eThe standard error \u003cstrong\u003econditional\u003c/strong\u003e on a specific class $\\{Y = y\\}$ is represented by $R_{\\text{nat}}(f; y)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"theoretical-experiment\"\u003eTheoretical Experiment\u003c/h3\u003e\n\u003cp\u003eWe generate a simple example of the binary classification task that we presented at the beginning of section 1.2.\nThe data therefore comes from two classes $\\mathcal{Y} = { \\{-1, +1\\}}$, with each class\u0026rsquo; data following a Gaussian distribution $\\mathcal{D}$ centered on $-\\theta$ and $\\theta$ respectively.\nIt is important to specify that there is a $K$-factor difference between the variance of the two classes defined as follows: $\\sigma_{+1} : \\sigma_{-1} = K : 1$ and $K \u0026gt; 1$.\u003c/p\u003e\n\u003cp\u003eThe authors then use the theorem stating that:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTheorem:\u003c/strong\u003e In the case of a data distribution $D$ like the one above, the optimal linear classifier $f_{\\text{nat}}$ which minimizes the average standard classification error is:\n$$ f_{\\text{nat}} = \\arg\\min_f \\Pr(f(\\mathbf{x}) \\neq y) $$.\u003c/p\u003e\n\u003cp\u003eWith that theorem and after computations, the authors prove that the class \u0026ldquo;$+1$\u0026rdquo; as a larger standard error than the class \u0026ldquo;$-1$\u0026rdquo;.\u003c/p\u003e\n\u003cp\u003eOverall, this result shows well that the class \u0026ldquo;$+1$\u0026rdquo;, characterized by a larger variance, tends to be more challenging to classify than the class\u0026quot;$-1$\u0026quot;; a result confirming the hypothesis initially made.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003e2. Model\u003c/h2\u003e\n\u003cp\u003eIn this section, we present the Fair Robust Learning model (FRL).\u003c/p\u003e\n\u003ch2 id=\"section-3.1\"\u003e2.1 Fairness Requirements\u003c/h2\u003e\n\u003cp\u003eThe authors introduced the concepts of Equalized Accuracy and Equalized Robustness, emphasizing the importance of providing equal prediction quality and resilience against adversarial attacks across different groups.\nTo achieve this balance, the authors propose a Fair Robust Learning (FRL) strategy.\nThis framework addresses fairness issues in adversarial training by aiming to minimize overall robust error while ensuring fairness constraints are met. They separate robust error into standard error and boundary error, allowing independent  solving of the unfairness of both errors. [ref 7]\u003c/p\u003e\n\u003cp\u003eThe training objective thus becomes minimizing the sum of standard error and boundary error while adhering to fairness constraints that ensure no significant disparities in error rates among classes. Techniques from prior research are leveraged to optimize boundary errors during training.\u003c/p\u003e\n\u003ch2 id=\"section-3.2\"\u003e2.2 Practical Algorithms\u003c/h2\u003e\n\u003cp\u003eThis section explores effective methods to implement and address the challenges outlined in the training objective, such as the Reweight strategy.\nIn order to implement it, Lagrange multipliers are introduced, denoted as $φ = (φ_{nat}^{\\text{i}}, φ_{bndy}^{\\text{i}})$ where each multiplier corresponds to a fairness constraint. These multipliers are non-negative and play a crucial role in the optimization process.\u003c/p\u003e\n\u003cp\u003eThe approach involves forming a Lagrangian, represented by the function $L(f, φ)$, which combines the standard error ($R_{\\text{nat}}(f)$) and boundary error ($R_{\\text{bndy}}(f)$) terms along with the fairness constraints. The Lagrangian acts as a guide for the optimization process, helping to balance the trade-off between minimizing errors and satisfying fairness requirements.\u003c/p\u003e\n\u003cp\u003e$$\n\\scriptsize{\nL(f, \\phi) = R_{\\text{nat}}(f) + R_{\\text{bndy}}(f) + \\sum_{i=1}^{Y} \\phi_{\\text{nat}}^i \\left( R_{\\text{nat}}(f, i) - R_{\\text{nat}}(f) - \\tau_1 \\right)^+ + \\sum_{i=1}^{Y} \\phi_{\\text{bndy}}^i \\left( R_{\\text{bndy}}(f, i) - R_{\\text{bndy}}(f) - \\tau_2 \\right)^+\n}\n$$\u003c/p\u003e\n\u003cp\u003eThe optimization problem is then framed as a max-min game between the classifier $f$ and the Lagrange multipliers $φ$. The objective is to maximize the fairness constraints while minimizing the Lagrangian function, which encapsulates both standard and boundary errors.\u003c/p\u003e\n\u003cp\u003eOn the other hand, the Reweight strategy presents a limitation particularly in mitigating boundary errors for specific classes. While upweighting the cost for standard errors ($R_{\\text{nat}}(f, i)$) can penalize large errors and improve performance for disadvantaged groups, solely upweighting the boundary error ($R_{\\text{bndy}}(f, i)$) for a class doesn\u0026rsquo;t effectively reduce its boundary error.\u003c/p\u003e\n\u003cp\u003eTo overcome this challenge, the Remargin strategy introduces an alternative approach by enlarging the perturbation margin ($\\epsilon$) during adversarial training. This strategy is inspired by previous research showing that increasing the margin during adversarial training can enhance a model\u0026rsquo;s robustness against attacks under the current intensity.[ref 8]\u003c/p\u003e\n\u003cp\u003eSpecifically, the Remargin strategy involves adjusting the adversarial margin for generating adversarial examples during training, focusing on specific classes where boundary errors are significant. This adjustment aims to improve the robustness of these classes and reduce their large boundary errors ($R_{\\text{bndy}}(f, i)$).\u003c/p\u003e\n\u003ch2 id=\"section-4\"\u003e3. Experimentation\u003c/h2\u003e\n\u003cp\u003eIn this section, we reproduce the experimental methodology and setup used to evaluate the effectiveness of the proposed Fair Robust Learning (FRL) framework in constructing robust deep neural network (DNN) models.\u003c/p\u003e\n\u003cp\u003eFirstly, we train a fairly simple model on the Fashion MNIST dataset, then we test out torchattack\u0026rsquo;s PGD on our naturally trained model, Then we will adversarially train the same architecture to see if we can identify this unfairness.\u003c/p\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"/images/Hajji_Teulier/result1.png\"\r\n  alt=\"Paper Initial Results\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003eAs we can see above, the naturally trained model has low standard error, but high PGD error. The adversarially trained model, in contrast, has a much lower PGD error, but higher standard error, and higher disparity between the classes.\u003c/p\u003e\n\u003cp\u003eSecond, we implement the FRL algorithm (Reweight strategy) which formulates the learning problem as a cost-sensitive classification that penalizes those classes which violate fairness. Essentially, we create multipliers that up or down weight the loss of classes based on how fair or unfair they are with respect to the average across all classes.\u003c/p\u003e\n\u003cp\u003eThe following is the FRL Algorithm outlined in the paper:\u003c/p\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\r\n  \u003cimg src=\"/images/Hajji_Teulier/algo1.png\" alt=\"Paper Initial Results\" width=\"400\" /\u003e\r\n\u003c/div\u003e\r\n\u003cp\u003eWe made a setup to run the process 3 times: once with equal alpha values, once with an alpha ratio that favors the natural error, and one with an alpha ratio that favors the boundary error.\u003c/p\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"/images/Hajji_Teulier/result2.png\"\r\n  alt=\"Paper Initial Results\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003eIn accordance with the authors of the paper, we find that the alpha ratio that favors the natural error is successful in preventing the unfairness of the standard error in the model, and does help somewhat with the unfairness of the PGD error. On the other hand, we notice that the algorithm struggles to improve the worst-case boundary error, leading to disparities in robustness performance across different classes.\u003c/p\u003e\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIn conclusion, the studied article discusses the development and implementation of Fair Robust Learning (FRL) strategies to address fairness concerns in adversarial training of deep neural networks. The objective of these strategies is to achieve both equalized accuracy and robustness across different classes.\u003c/p\u003e\n\u003cp\u003eThe Reweight strategy aims to minimize overall robust error while adhering to fairness constraints by adjusting training weights based on class-wise errors while the Remargin strategy enlarges the perturbation margin during adversarial training to improve robustness and reduce boundary errors.\u003c/p\u003e\n\u003cp\u003eFinally, The FRL framework combines these strategies to mitigate fairness issues and improve model performance across various classes. These approaches represent promising steps towards achieving fairness in robust deep learning models.\u003c/p\u003e\n\u003chr\u003e\n\u003chr\u003e\n\u003ch2 id=\"references\"\u003eReferences\u003c/h2\u003e\n\u003cp\u003e[1]  Han Xu, Xiaorui Liu, Yaxin Li, Anil K. Jain, Jiliang Tang1. To be Robust or to be Fair: Towards Fairness in Adversarial Training. 2021.\u003c/p\u003e\n\u003cp\u003e[2] Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. 2014.\u003c/p\u003e\n\u003cp\u003e[3]  Morgulis, N., Kreines, A., Mendelowitz, S., and Weisglass, Y. Fooling a real car with adversarial traffic signs. 2019.\u003c/p\u003e\n\u003cp\u003e[4]  Sharif, M., Bhagavatula, S., Bauer, L., and Reiter, M. K. Accessorize to a crime: Real and stealthy attacks on state of-the-art face recognition. In Proceedings of the 2016 acm sigsac conference on computer and communications security, pp. 1528–1540, 2016.\u003c/p\u003e\n\u003cp\u003e[5] Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.\u003c/p\u003e\n\u003cp\u003e[6] He, H. and Garcia, E. A. Learning from imbalanced data. IEEE Transactions on knowledge and data engineering, 21(9):1263–1284. 2009.\u003c/p\u003e\n\u003cp\u003e[7] Zhang, H., Yu, Y., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jordan, M. I. Theoretically principled trade-off between robustness and accuracy. 2019.\u003c/p\u003e\n\u003cp\u003e[8] Tramer, F., Behrmann, J., Carlini, N., Papernot, N., and Ja- ` cobsen, J.-H. Fundamental tradeoffs between invariance and sensitivity to adversarial perturbations. In International Conference on Machine Learning, pp. 9561–9571. PMLR. 2020.\u003c/p\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\r\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\r\n\u003c/style\u003e\r\n\u003cscript type=\"text/x-mathjax-config\"\u003e\r\nMathJax.Hub.Config({\r\n    tex2jax: {\r\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\r\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\r\n    }\r\n});\r\nMathJax.Hub.Queue(function() {\r\n    var all = MathJax.Hub.getAllJax(), i;\r\n    for(i = 0; i \u003c all.length; i += 1) {\r\n        all[i].SourceElement().parentNode.className += ' has-jax';\r\n    }\r\n});\r\n\u003c/script\u003e\r\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\r\n",
      "url": "http://localhost:1313/posts/robust-or-fair/",
      "date_published": "27036-27-09T337:2727:00+01:00",
      "date_modified": "27036-27-09T337:2727:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "37b39cffb06a5944d96062052d7f779421cde6be",
      "title": "RobustAI_RegMixup",
      "summary": "",
      "content_text": "\r\u003c!DOCTYPE html\u003e\rStyled Table\rRegMixup : Regularizer for robust AI\rImprove accuracy and Out-of-Distribution Robustness\rAuthors: Marius Ortega, Ly An CHHAY Paper : RegMixup by Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania\rTable of Contents Abstract Introduction Prerequisites Empirical Risk Minimization Vicinal Risk Minimization Mixup RegMixup in theory RegMixup in practice Conclusion Abstract In this blog post, we will present the paper \u0026ldquo;RegMixup: Regularizer for robust AI\u0026rdquo; by Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania. This paper introduces a new regularizer called RegMixup, which is designed to improve the accuracy and out-of-distribution robustness of deep neural networks. The authors show that RegMixup can be used to improve the performance of state-of-the-art models on various datasets, including CIFAR-10, CIFAR-100, and ImageNet. The paper also provides an extensive empirical evaluation of RegMixup, demonstrating its effectiveness in improving the robustness of deep neural networks to out-of-distribution samples.\nIn this blong post, we will provide an overview of the paper, explain the theoretical background of RegMixup, and finally, perform a toy example to demonstrate how to use RegMixup with the torch-uncertainty library.\nIntroduction Most real-world machine algorithm applications are good when it comes to predicting new data following the train distribution. However, they are not robust to out-of-distribution (OOD) samples (i.e. when the test data distribution is different from the train data distribution). This is a major problem in machine learning as it can lead to catastrophic predictions.\nThe question is how to improve the robustness of machine learning algorithms to OOD samples ? Many researchers have tried such as Liu et al. (2020a, 2020b), Wen et al. (2021), Lakshminarayanan et al. (2017). Even though they have shown some improvements, their approaches use expensive ensemble methods or propose non-trivial modifications of the neural network architecture. What if we could improve the robustness of deep neural networks with respect to OOD samples while utilizing much simpler and cost-effective methods?\nThe first step toward the method presented in this blog is Mixup, proposed by Zang and al (2018). This method is quite good when it comes to dealing with slight perturbations in the data distribution. However, Mixup has the tendency to emphasize difference in labels from very similar samples (high predictive entropy). This is not ideal for OOD samples as the model do not differentiate ID (In-distribution) and OOD samples very well.\nRegMixup adds a new layer to Mixup by using it as a regularizer. From there, we will present the theoretical background of the paper, the implementation so as to easily use it in practice.\n1. Prerequisites In order to understand the paper, we need to understand what is Empirical and Vicinal Risk Minimization (ERM and VRM) as well as Mixup.\n1.1. Empirical Risk Minimization (ERM) Empirical Risk Minimization is an inference principle which consists in finding the model $\\hat{f}$ that minimizes the empirical risk $R_{emp}(\\hat{f})$ on the training set. The empirical risk is defined as the average loss over the training set :\n$$ R_{emp}(\\hat{f}) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{f}(x_i), y_i) \\tag{1} $$\nwhere $L$ is the loss function, $x_i$ is the input, $y_i$ is the label and $n$ is the number of samples in the training set. However, ERM contains a very strong assumption which is that $\\hat{f} \\approx f$ where $f$ is the true (and unknown) distribution for all points of the dataset. Thereby, if the testing set distribution differs even slighly from the training set one, ERM is unable to explain or provide generalization. Vicinal Risk is a way to relax this assumption.\n1.2. Vicinal Risk Minimization (VRM) Vicinal Risk Minimization (VRM) is a generalization of ERM. Instead of having a single distribution estimate $\\hat{f}$, VRM uses a set of distributions $\\hat{f}_{x_i, y_i}$ for each training sample $(x_i, y_i)$. The goal is to minimize the average loss over the training set, but with respect to the vicinal distribution of each sample.\n$$ R_{vrm}(\\hat{f}) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{f}_{x_i, y_i}(x_i), y_i) \\tag{2} $$\nConsequently, each training point has its own distribution estimate. This is a way to relax the strong assumption of ERM explained above.\n1.3. Mixup Mixup is a data augmentation technique that generates new samples by mixing pairs of training samples. By doing so, Mixup regularizes models to favor simple linear behavior in-between training examples. Experimentally speaking, Mixup has been shown to improve the generalization of deep neural networks, increase their robustness to adversarial attacks, reduce the memorization of corrupt labels as well as stabilize the training of generative adversarial networks.\nIn essence, Mixup can be thought as a learning objective designed for robustness and accountability of the model. Now, let\u0026rsquo;s see how Mixup works.\nFirst, we take two samples $(x_i, y_i)$ and $(x_j, y_j)$ from the training set. Then, we generate a new sample $(\\tilde{x}, \\tilde{y})$ by taking a convex combination of the two samples with a mixup coefficient $\\lambda \\sim \\text{Beta}(\\alpha, \\alpha)$ :\n$$ \\tilde{x} = \\lambda x_i + (1 - \\lambda) x_j \\hspace{1cm} \\tilde{y} = \\lambda y_i + (1 - \\lambda) y_j $$\nWe can then define the vicinal distribution of the mixed sample $(\\tilde{x}, \\tilde{y})$ as :\n$$ P_{x_i, y_i} = \\mathbb{E}_\\lambda[( \\delta {\\tilde{x}_i}(x), \\delta{\\tilde{y}_i}(y))] \\tag{3} $$\nMixup is an interesting method to consider but it possesses some limitations :\nSmall $\\alpha$ issues : With our setup, $\\alpha \\approx 1$ encourages $\\tilde{x}$ to be perceptually different from $x$. Consequently, training and testing distribution will also grow appart from each other. When $\\alpha \\ll 1$, the mixup convex interpolation factor λ leads to a sharp peaks of 0 and 1. Therefore, Mixup will produce samples close to the initial ones (in case λ close to 1) or in the direction of another sample (in case of λ close to 0). Look at the figure below, one of the two interpolating images dominates the interpolated one. What is noticed after cross-validation of alpha is that the best values are $\\alpha \\approx 0.2$ which is very small. Consequently, the final sample effectively presents only a small perturbation in comparison to the original one while the vicinal distribution exploration space is much larger. We could say that Mixup does not allow to use the full potential of the vicinal distributions of the data. Model underconfidence : When a neural network is trained with Mixup, it is only exposed to interpolated samples. Consequently, the model learns to predict smoothed labels which is the very root cause of its underconfidence. This results in a high predictive entropy for both ID and OOD samples. Mixup vs RegMixup, underconfidence and space exploration.\n2. RegMixup in theory Now that we have understood the path that led to RegMixup, we will explore its theoretical background and see how and why it is a good regularizer for robust AI.\nWhile Mixup utilizes data points\u0026rsquo; vicinal distribution only, RegMixup uses both the vicinal and the empirical one (refering respectively to VRM and ERM). This can seem far-fetched or even counter-intuitive but produces very interesting properties.\n$$ P(x, y) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\gamma \\delta_{x_i}(x) \\delta_{y_i}(y) + (1-\\gamma) P_{x_i, y_i}(x, y) \\right) \\tag{4} $$\nHere, $\\gamma$ is the hyperparameter controlling the mixup between the empirical and vicinal distribution. In fact, we see that the distribution $P(x, y)$ for RegMixup is a convex combination of the empirical distribution (left term of the addition in equation 4) and the vicinal distribution defined with equations (2) and (3).\nFrom there, we can define a new loss function $\\mathcal{L}$ based on the Cross Entropy Loss ($\\text{CE}$)\n$$ \\mathcal{L}(\\hat{y}, y) = \\text{CE}(p_\\theta(\\hat{y} \\vert x), y) + \\eta \\text{CE}(p_\\theta(\\hat{y} \\vert \\tilde{x}), \\tilde{y}) \\tag{5} $$\nWith $ \\eta \\in R_{+}^{\\ast} $ being the hyperparameter controlling the importance of the vicinal cross entropy sub-loss and $p_\\theta$ the activation function of the model parameterized by $\\theta$. In the paper, the value of $\\eta$ is set to 1 and its variation seem negligible. Consequently, we will not focus on it in this blog post.\nSuch a model (equation 4) exhibits properties that lacked in Mixup :\nValues of $\\alpha$ and underconfidence : As we explicitly add the empirical distribution to the vicinal one, the ERM term will encourage the model to predict the true labels of the training set while the VRM term, motivated by the interpolation factor $\\lambda$, will explore the vicinal distribution space in a much more thorough way than what was possible with Mixup. For instance, if λ $\\approx$ 0.5, a wide variety of images containing features from both the images in the pair are obtained (look at the figure). Consequently, the ERM term allows to better predict in-distribution samples while the VRM term, with a larger $\\alpha$, will allow to better predict OOD samples. This is a very interesting property as it allows to have a model that is both confident and accurate. Prediction entropy : Through their experiments and observations, researchers found that a cross-validated value of $\\alpha$ leads to a maximum likelihood estimation having high entropy for ODD samples only. While Mixup demonstrated high entropy for both ID and OOD samples, RegMixup is able to differentiate between the two. This is an highly desirable properties indicating us that RegMixup acts as a regularizer in essense. As a preliminary conclusion, RegMixup is a very powerful, cost-efficient and simple-to-implement regularizer that allows to improve the robustness and accuracy of deep neural networks for both in-distribution and out-of-distribution samples. In the next section, we will see how to use RegMixup in practice trough a toy example.\n3. RegMixup in practice (implementation) Now, our objective will be to demonstrate the effectiveness of RegMixup through a very simple example. We will use the CIFAR-10-C dataset (corrupted version of CIFAR-10) and a standard ResNet-18 model. We will compare performances of 3 models :\nA baseline model trained with ERM A model trained with Mixup A model trained with RegMixup To do so, we have two possibilities :\nUse the official implementation of RegMixup available on Francesco Pinto's GitHub. Use the torch-uncertainty library which provides a simple and efficient way to use RegMixup. Note, the library is developed by researchers from ENSTA Paris and is available on GitHub. In this blog post, we will use the torch-uncertainty library as it is very simple to use and provides a very well-implemented version of RegMixup.\n3.1. Installation First, we need to install the torch-uncertainty library. To do so, we can use pip :\npip install torch-uncertainty Note: If you use a gpu, torch-uncertainty will automatically install a cpu version of torch and torchvision, you can compile the following lines to install the gpu version of torch and torchvision (took from PyTorch website) :\npip unistall torch torchvision pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118 To check if the installation was successful, you can run the following code, it should return True if you have a gpu and False if you don\u0026rsquo;t have one :\nimport torch print(torch.cuda.is_available()) 3.2. Training the models with torch-uncertainty Now that we have installed torch-uncertainty, we can train the models. First, we need to import the necessary libraries :\nfrom torch_uncertainty import cli_main, init_args from torch_uncertainty.baselines.classification import ResNet from torch_uncertainty.optimization_procedures import optim_cifar10_resnet18 from torch_uncertainty.datamodules import CIFAR10DataModule from torchvision.datasets import CIFAR10 from torchvision import transforms from torch.nn import CrossEntropyLoss import torch import os from pathlib import Path from cli_test_helpers import ArgvContext Then, we can define the 3 models we discussed earlier :\nbaseline = ResNet(num_classes=10, loss=CrossEntropyLoss, optimization_procedure=optim_cifar10_resnet18, version=\u0026#34;std\u0026#34;, in_channels=3, arch=18).cuda() mixup = ResNet(num_classes=10, loss=CrossEntropyLoss, optimization_procedure=optim_cifar10_resnet18, version=\u0026#34;std\u0026#34;, in_channels=3, arch=18, mixup=True, mixup_alpha=0.2).cuda() regmixup = ResNet(num_classes=10, loss=CrossEntropyLoss, optimization_procedure=optim_cifar10_resnet18, version=\u0026#34;std\u0026#34;, in_channels=3, arch=18, reg_mixup=True, mixup_alpha=15).cuda() Before training the models, we need to define important arguments such as training parameters (epochs, estimators, etc.) and the datamodule. We can do so with the following code:\nroot = Path(os.path.abspath(\u0026#34;\u0026#34;)) # We mock the arguments for the trainer with ArgvContext( \u0026#34;file.py\u0026#34;, \u0026#34;--max_epochs\u0026#34;, \u0026#34;20\u0026#34;, \u0026#34;--enable_progress_bar\u0026#34;, \u0026#34;False\u0026#34;, \u0026#34;--num_estimators\u0026#34;, \u0026#34;8\u0026#34; ): args = init_args(network=ResNet, datamodule=CIFAR10DataModule) net_name = \u0026#34;logs/reset18-cifar10\u0026#34; # datamodule args.root = str(root / \u0026#34;data\u0026#34;) dm = CIFAR10DataModule(**vars(args)) Finally, we can train the models using the cli_main function from torch-uncertainty :\nresults_baseline = cli_main(baseline, dm, root, net_name, args=args) results_mixup = cli_main(mixup, dm, root, net_name, args=args) results_regmixup = cli_main(regmixup, dm, root, net_name, args=args) Note: If you have a gpu, you can make a slight modification to the code to use it :\nClick on cli_main and press F12 to go to the function definition. Go to line 222 and replace the trainer definition by the following one : # trainer trainer = pl.Trainer.from_argparse_args( args, accelerator=\u0026#34;gpu\u0026#34;, devices=1, callbacks=callbacks, logger=tb_logger, deterministic=(args.seed is not None), inference_mode=not (args.opt_temp_scaling or args.val_temp_scaling), ) Save the file and you are all set. 3.3. Results So as to compare the performances of the 3 models, we use two corrupted versions of Cifar-10-C. The first version has a corruption severity factor of 5 (slight data corruption) and the second one has a corruption severity factor of 15 (more severe data corruption). Our study contains 5 metrics : entropy, accuracy, brier score, expected calibration error (ECE) and negative log-likelihood (NLL). In our explanation, we will focus on the accuracy and entropy to keep it simple.\nWith corruption severity factor of 5, we obtain the following results :\nentropy accuracy brier ece nll baseline 0.656294 0.7480 0.349862 0.032466 0.729336 mixup 0.640811 0.7578 0.335403 0.024429 0.703844 regmixup 0.676174 0.7564 0.340233 0.023135 0.711405 First of all, we can see that the accuracy is quite similar for the 3 models. This makes sense as the corruption severity factor is quite low, thus cifar-10-c is not very different from the original cifar-10. However, we can see that the entropy of the RegMixup model is higher than the one of the Mixup model. This is symptomatic of Mixup\u0026rsquo;s underconfidence. As stated previously, given the low corruption severity factor of cifar-10-c, the underconfidence of Mixup does not impact its performances in a visible manner.\nWith corruption severity factor of 15, we obtain the following results :\nentropy accuracy brier ece nll baseline 0.615607 0.7402 0.358522 0.048414 0.750933 mixup 0.698558 0.7558 0.338540 0.014760 0.709190 regmixup 0.702599 0.7614 0.327945 0.008439 0.687550 Here the results are much more unequivocal. As the severity factor increases, the baseline model drops in accuracy and entropy, Mixup also drops in accuracy but increases in entropy and RegMixup increases in accuracy and entropy. Here, RegMixup has the higher entropy as the model has higher entropy for OOD samples which are more frequent at this corruption level. Mixup shows a greater delta increase in entropy due to its higher predictive entropy tendency whether or not samples are OOD or ID. Consequently, RegMixup is more confident and accurate than the Mixup model eventhough Mixup is not fully underperforming.\n4. Conclusion As a conclusion, we have seen that RegMixup is a powerful method to regularize deep neural networks. Despite being very simple and cost-effective, it is important to specify that the paper does not provide a theoretical explanation of the method. These experimental grounds are very promising but it appears important to stay cautious while utilizing RegMixup.\n",
      "content_html": "\u003cstyle\r\nTYPE=\"text/css\"\u003e\r\n\r\ncode.has-jax {font:\r\ninherit;\r\nfont-size:\r\n100%; \r\nbackground: \r\ninherit; \r\nborder: \r\ninherit;}\r\n\r\n\u003c/style\u003e\r\n\u003cscript\r\ntype=\"text/x-mathjax-config\"\u003e\r\n\r\nMathJax.Hub.Config({\r\n\r\n    tex2jax: {\r\n\r\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\r\n\r\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\r\n\r\n    }\r\n\r\n});\r\n\r\nMathJax.Hub.Queue(function() {\r\n\r\n    var all = MathJax.Hub.getAllJax(), i;\r\n\r\n    for(i = 0; i \u003c all.length; i += 1) {\r\n\r\n        all[i].SourceElement().parentNode.className += ' has-jax';\r\n\r\n    }\r\n\r\n});\r\n\r\n\u003c/script\u003e\r\n\u003cscript\r\ntype=\"text/javascript\"\r\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\r\n\u003c!DOCTYPE html\u003e\r\n\u003chtml lang=\"en\"\u003e\r\n\u003chead\u003e\r\n\u003cmeta charset=\"UTF-8\"\u003e\r\n\u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e\r\n\u003ctitle\u003eStyled Table\u003c/title\u003e\r\n\u003cstyle\u003e\r\n    table {\r\n        border-collapse: collapse;\r\n        width: 100%;\r\n    }\r\n    th, td {\r\n        padding: 8px;\r\n        text-align: center;\r\n        border-bottom: 1px solid #ddd;\r\n    }\r\n    th {\r\n        background-color: #f2f2f2;\r\n    }\r\n    tr:hover {\r\n        background-color: #f5f5f5;\r\n    }\r\n\u003c/style\u003e\r\n\u003c/head\u003e\r\n\u003c/html\u003e\r\n\u003ch1 style=\"font-size: 36px;\"\u003eRegMixup : Regularizer for robust AI\u003c/h1\u003e\r\n\u003ch1 style=\"font-size: 24px;\"\u003eImprove accuracy and Out-of-Distribution Robustness\u003ch1\u003e\r\n\u003ch1 style=\"font-size: 18px;\"\u003eAuthors: Marius Ortega, Ly An CHHAY \u003cbr /\u003e\r\nPaper : \u003ca href=\"https://arxiv.org/abs/2206.14502\"\u003eRegMixup\u003c/a\u003e  by Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania\u003c/h1\u003e\r\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0.0\"\u003eAbstract\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-0.1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003ePrerequisites\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-1.1\"\u003eEmpirical Risk Minimization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1.2\"\u003eVicinal Risk Minimization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1.3\"\u003eMixup\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eRegMixup in theory\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eRegMixup in practice \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0.0\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eIn this blog post, we will present the paper \u0026ldquo;RegMixup: Regularizer for robust AI\u0026rdquo; by Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania. This paper introduces a new regularizer called RegMixup, which is designed to improve the accuracy and out-of-distribution robustness of deep neural networks. The authors show that RegMixup can be used to improve the performance of state-of-the-art models on various datasets, including CIFAR-10, CIFAR-100, and ImageNet. The paper also provides an extensive empirical evaluation of RegMixup, demonstrating its effectiveness in improving the robustness of deep neural networks to out-of-distribution samples.\u003c/p\u003e\n\u003cp\u003eIn this blong post, we will provide an overview of the paper, explain the theoretical background of RegMixup, and finally, perform a toy example to demonstrate how to use RegMixup with the torch-uncertainty library.\u003c/p\u003e\n\u003ch2 id=\"section-0.1\"\u003eIntroduction \u003c/h2\u003e\n\u003cp\u003eMost real-world machine algorithm applications are good when it comes to predicting new data following the train distribution. However, they are not robust to out-of-distribution (OOD) samples (i.e. when the test data distribution is different from the train data distribution). This is a major problem in machine learning as it can lead to catastrophic predictions.\u003c/p\u003e\n\u003cp\u003eThe question is how to improve the robustness of machine learning algorithms to OOD samples ?\nMany researchers have tried such as Liu et al. (2020a, 2020b), Wen et al. (2021), Lakshminarayanan et al. (2017). Even though they have shown some improvements, their approaches use expensive ensemble methods or propose non-trivial modifications of the neural network architecture. What if we could improve the robustness of deep neural networks with respect to OOD samples while utilizing much simpler and cost-effective methods?\u003c/p\u003e\n\u003cp\u003eThe first step toward the method presented in this blog is Mixup, proposed by Zang and al (2018). This method is quite good when it comes to dealing with slight perturbations in the data distribution. However, Mixup has the tendency to emphasize difference in labels from very similar samples (high predictive entropy). This is not ideal for OOD samples as the model do not differentiate ID (In-distribution) and OOD samples very well.\u003c/p\u003e\n\u003cp\u003eRegMixup adds a new layer to Mixup by using it as a regularizer. From there, we will present the theoretical background of the paper, the implementation so as to easily use it in practice.\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003e1. Prerequisites \u003c/h2\u003e\n\u003cp\u003eIn order to understand the paper, we need to understand what is Empirical and Vicinal Risk Minimization (ERM and VRM) as well as Mixup.\u003c/p\u003e\n\u003ch3 id=\"section-1.1\"\u003e1.1. Empirical Risk Minimization (ERM)\u003c/h3\u003e\n\u003cp\u003eEmpirical Risk Minimization is an inference principle which consists in finding the model $\\hat{f}$ that minimizes the empirical risk $R_{emp}(\\hat{f})$ on the training set. The empirical risk is defined as the average loss over the training set :\u003c/p\u003e\n\u003cp\u003e$$\nR_{emp}(\\hat{f}) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{f}(x_i), y_i) \\tag{1}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $L$ is the loss function, $x_i$ is the input, $y_i$ is the label and $n$ is the number of samples in the training set. However, ERM contains a very strong assumption which is that $\\hat{f} \\approx f$ where $f$ is the true (and unknown) distribution for all points of the dataset. Thereby, if the testing set distribution differs even slighly from the training set one, ERM is unable to explain or provide generalization. Vicinal Risk is a way to relax this assumption.\u003c/p\u003e\n\u003ch3 id=\"section-1.2\"\u003e1.2. Vicinal Risk Minimization (VRM)\u003c/h3\u003e\n\u003cp\u003eVicinal Risk Minimization (VRM) is a generalization of ERM. Instead of having a single distribution estimate $\\hat{f}$, VRM uses a set of distributions $\\hat{f}_{x_i, y_i}$ for each training sample $(x_i, y_i)$. The goal is to minimize the average loss over the training set, but with respect to the vicinal distribution of each sample.\u003c/p\u003e\n\u003cp\u003e$$\nR_{vrm}(\\hat{f}) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{f}_{x_i, y_i}(x_i), y_i) \\tag{2}\n$$\u003c/p\u003e\n\u003cp\u003eConsequently, each training point has its own distribution estimate. This is a way to relax the strong assumption of ERM explained above.\u003c/p\u003e\n\u003ch3 id=\"section-1.3\"\u003e1.3. Mixup\u003c/h3\u003e\n\u003cp\u003eMixup is a data augmentation technique that generates new samples by mixing pairs of training samples. By doing so, Mixup regularizes models to favor simple linear behavior in-between training examples. Experimentally speaking, Mixup has been shown to improve the generalization of deep neural networks, increase their robustness to adversarial attacks, reduce the memorization of corrupt labels as well as stabilize the training of generative adversarial networks.\u003c/p\u003e\n\u003cp\u003eIn essence, Mixup can be thought as a learning objective designed for robustness and accountability of the model. Now, let\u0026rsquo;s see how Mixup works.\u003c/p\u003e\n\u003cp\u003eFirst, we take two samples $(x_i, y_i)$ and $(x_j, y_j)$ from the training set. Then, we generate a new sample $(\\tilde{x}, \\tilde{y})$ by taking a convex combination of the two samples with a mixup coefficient $\\lambda \\sim \\text{Beta}(\\alpha, \\alpha)$ :\u003c/p\u003e\n\u003cp\u003e$$\n\\tilde{x} = \\lambda x_i + (1 - \\lambda) x_j \\hspace{1cm}\n\\tilde{y} = \\lambda y_i + (1 - \\lambda) y_j\n$$\u003c/p\u003e\n\u003cp\u003eWe can then define the vicinal distribution of the mixed sample $(\\tilde{x}, \\tilde{y})$ as :\u003c/p\u003e\n\u003cp\u003e$$\nP_{x_i, y_i} = \\mathbb{E}_\\lambda[( \\delta {\\tilde{x}_i}(x), \\delta{\\tilde{y}_i}(y))] \\tag{3}\n$$\u003c/p\u003e\n\u003cp\u003eMixup is an interesting method to consider but it possesses some limitations :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSmall $\\alpha$ issues :\u003c/strong\u003e With our setup, $\\alpha \\approx 1$ encourages $\\tilde{x}$ to be perceptually different from $x$. Consequently, training and testing distribution will also grow appart from each other. When $\\alpha \\ll 1$, the mixup convex interpolation factor λ leads to a sharp peaks of 0 and 1. Therefore, Mixup will produce samples close to the initial ones (in case λ close to 1) or in the direction of another sample (in case of λ close to 0). Look at the \u003cstrong\u003e\u003ca href=\"#my-fig\"\u003efigure\u003c/a\u003e\u003c/strong\u003e below, one of the two interpolating images dominates the interpolated one. What is noticed after cross-validation of alpha is that the best values are $\\alpha \\approx 0.2$ which is very small. Consequently, the final sample effectively presents only a small perturbation in comparison to the original one while the vicinal distribution exploration space is much larger. We could say that Mixup does not allow to use the full potential of the vicinal distributions of the data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel underconfidence :\u003c/strong\u003e When a neural network is trained with Mixup, it is only exposed to interpolated samples. Consequently, the model learns to predict smoothed labels which is the very root cause of its underconfidence. This results in a high predictive entropy for both ID and OOD samples.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure id=\"my-fig\" class=\"numbered\"\u003e\r\n    \u003cimg src=\"/images/regmixup/fig.png\" class=\"align-center\"\u003e\r\n    \u003cp style=\"text-align: center;\"\u003eMixup vs RegMixup, underconfidence and space exploration.\u003c/p\u003e\r\n\u003c/figure\u003e\r\n\u003ch2 id=\"section-2\"\u003e2. RegMixup in theory\u003c/h2\u003e\n\u003cp\u003eNow that we have understood the path that led to RegMixup, we will explore its theoretical background and see how and why it is a good regularizer for robust AI.\u003c/p\u003e\n\u003cp\u003eWhile Mixup utilizes data points\u0026rsquo; vicinal distribution only, RegMixup uses both the vicinal and the empirical one (refering respectively to VRM and ERM). This can seem far-fetched or even counter-intuitive but produces very interesting properties.\u003c/p\u003e\n\u003cp\u003e$$\nP(x, y) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\gamma \\delta_{x_i}(x) \\delta_{y_i}(y) + (1-\\gamma) P_{x_i, y_i}(x, y) \\right) \\tag{4}\n$$\u003c/p\u003e\n\u003cp\u003eHere, $\\gamma$ is the hyperparameter controlling the mixup between the empirical and vicinal distribution. In fact, we see that the distribution $P(x, y)$ for RegMixup is a convex combination of the empirical distribution (left term of the addition in equation 4) and the vicinal distribution defined with equations (2) and (3).\u003c/p\u003e\n\u003cp\u003eFrom there, we can define a new loss function $\\mathcal{L}$ based on the Cross Entropy Loss ($\\text{CE}$)\u003c/p\u003e\n\u003cp\u003e$$\n\\mathcal{L}(\\hat{y}, y) = \\text{CE}(p_\\theta(\\hat{y} \\vert x), y) + \\eta \\text{CE}(p_\\theta(\\hat{y} \\vert \\tilde{x}), \\tilde{y}) \\tag{5}\n$$\u003c/p\u003e\n\u003cp\u003eWith $ \\eta \\in R_{+}^{\\ast} $ being the hyperparameter controlling the importance of the vicinal cross entropy sub-loss and $p_\\theta$ the activation function of the model parameterized by $\\theta$. In the paper, the value of $\\eta$ is set to 1 and its variation seem negligible. Consequently, we will not focus on it in this blog post.\u003c/p\u003e\n\u003cp\u003eSuch a model (equation 4) exhibits properties that lacked in Mixup :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eValues of $\\alpha$ and underconfidence :\u003c/strong\u003e As we explicitly add the empirical distribution to the vicinal one, the ERM term will encourage the model to predict the true labels of the training set while the VRM term, motivated by the interpolation factor $\\lambda$, will explore the vicinal distribution space in a much more thorough way than what was possible with Mixup. For instance, if λ $\\approx$ 0.5, a wide variety of images containing features from both the images in the pair are obtained (look at the \u003cstrong\u003e\u003ca href=\"#my-fig\"\u003efigure\u003c/a\u003e\u003c/strong\u003e). Consequently, the ERM term allows to better predict in-distribution samples while the VRM term, with a larger $\\alpha$, will allow to better predict OOD samples. This is a very interesting property as it allows to have a model that is both confident and accurate.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePrediction entropy :\u003c/strong\u003e Through their experiments and observations, researchers found that a cross-validated value of $\\alpha$ leads to a maximum likelihood estimation having high entropy for ODD samples only. While Mixup demonstrated high entropy for both ID and OOD samples, RegMixup is able to differentiate between the two. This is an highly desirable properties indicating us that RegMixup acts as a \u003cstrong\u003eregularizer\u003c/strong\u003e in essense.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs a preliminary conclusion, RegMixup is a very powerful, cost-efficient and simple-to-implement regularizer that allows to improve the robustness and accuracy of deep neural networks for both in-distribution and out-of-distribution samples. In the next section, we will see how to use RegMixup in practice trough a toy example.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003e3. RegMixup in practice (implementation)\u003c/h2\u003e\n\u003cp\u003eNow, our objective will be to demonstrate the effectiveness of RegMixup through a very simple example. We will use the CIFAR-10-C dataset (corrupted version of CIFAR-10) and a standard ResNet-18 model. We will compare performances of 3 models :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA baseline model trained with ERM\u003c/li\u003e\n\u003cli\u003eA model trained with Mixup\u003c/li\u003e\n\u003cli\u003eA model trained with RegMixup\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo do so, we have two possibilities :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUse the official implementation of RegMixup available on \u003ca href=\"https://github.com/FrancescoPinto/RegMixup\"\u003eFrancesco Pinto's GitHub\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eUse the torch-uncertainty library which provides a simple and efficient way to use RegMixup. Note, the library is developed by researchers from ENSTA Paris and is available on \u003ca href=\"https://github.com/ENSTA-U2IS-AI/torch-uncertainty\"\u003eGitHub\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn this blog post, we will use the torch-uncertainty library as it is very simple to use and provides a very well-implemented version of RegMixup.\u003c/p\u003e\n\u003ch3 id=\"31-installation\"\u003e3.1. Installation\u003c/h3\u003e\n\u003cp\u003eFirst, we need to install the torch-uncertainty library. To do so, we can use pip :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip install torch-uncertainty\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNote: If you use a gpu, torch-uncertainty will automatically install a cpu version of torch and torchvision, you can compile the following lines to install the gpu version of torch and torchvision (took from \u003ca href=\"https://pytorch.org/get-started/locally/\"\u003ePyTorch website\u003c/a\u003e) :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip unistall torch torchvision\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTo check if the installation was successful, you can run the following code, it should return True if you have a gpu and False if you don\u0026rsquo;t have one :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eprint\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecuda\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eis_available\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"32-training-the-models-with-torch-uncertainty\"\u003e3.2. Training the models with torch-uncertainty\u003c/h3\u003e\n\u003cp\u003eNow that we have installed torch-uncertainty, we can train the models. First, we need to import the necessary libraries :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch_uncertainty\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_main\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003einit_args\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch_uncertainty.baselines.classification\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch_uncertainty.optimization_procedures\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eoptim_cifar10_resnet18\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch_uncertainty.datamodules\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eCIFAR10DataModule\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorchvision.datasets\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eCIFAR10\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorchvision\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etransforms\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch.nn\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eCrossEntropyLoss\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eos\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003epathlib\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ePath\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_test_helpers\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eArgvContext\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThen, we can define the 3 models we discussed earlier :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003ebaseline\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enum_classes\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eloss\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eCrossEntropyLoss\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eoptimization_procedure\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eoptim_cifar10_resnet18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eversion\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;std\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ein_channels\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003earch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecuda\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003emixup\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enum_classes\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eloss\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eCrossEntropyLoss\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eoptimization_procedure\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eoptim_cifar10_resnet18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eversion\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;std\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ein_channels\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003earch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003emixup\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#00a8c8\"\u003eTrue\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003emixup_alpha\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.2\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecuda\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eregmixup\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enum_classes\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eloss\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eCrossEntropyLoss\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eoptimization_procedure\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eoptim_cifar10_resnet18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eversion\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;std\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ein_channels\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003earch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ereg_mixup\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#00a8c8\"\u003eTrue\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003emixup_alpha\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e15\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecuda\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eBefore training the models, we need to define important arguments such as training parameters (epochs, estimators, etc.) and the datamodule. We can do so with the following code:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ePath\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eos\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003epath\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eabspath\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# We mock the arguments for the trainer\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#00a8c8\"\u003ewith\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eArgvContext\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;file.py\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;--max_epochs\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;20\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;--enable_progress_bar\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;False\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;--num_estimators\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;8\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003einit_args\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enetwork\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edatamodule\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eCIFAR10DataModule\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003enet_name\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#d88200\"\u003e\u0026#34;logs/reset18-cifar10\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# datamodule\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003estr\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e \u003cspan style=\"color:#d88200\"\u003e\u0026#34;data\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003edm\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eCIFAR10DataModule\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003e\u003cspan style=\"color:#111\"\u003evars\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eFinally, we can train the models using the \u003ccode\u003ecli_main\u003c/code\u003e function from torch-uncertainty :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eresults_baseline\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_main\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ebaseline\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edm\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enet_name\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eresults_mixup\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_main\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003emixup\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edm\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enet_name\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eresults_regmixup\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_main\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eregmixup\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edm\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enet_name\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNote: If you have a gpu, you can make a slight modification to the code to use it :\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eClick on \u003ccode\u003ecli_main\u003c/code\u003e and press \u003ccode\u003eF12\u003c/code\u003e to go to the function definition.\u003c/li\u003e\n\u003cli\u003eGo to line 222 and replace the trainer definition by the following one :\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# trainer\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003etrainer\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003epl\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eTrainer\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efrom_argparse_args\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eaccelerator\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;gpu\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003edevices\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003ecallbacks\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecallbacks\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003elogger\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etb_logger\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003edeterministic\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eseed\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eis\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eNone\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003einference_mode\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eopt_temp_scaling\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eval_temp_scaling\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"3\"\u003e\n\u003cli\u003eSave the file and you are all set.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"33-results\"\u003e3.3. Results\u003c/h3\u003e\n\u003cp\u003eSo as to compare the performances of the 3 models, we use two corrupted versions of Cifar-10-C. The first version has a corruption severity factor of 5 (slight data corruption) and the second one has a corruption severity factor of 15 (more severe data corruption). Our study contains 5 metrics : entropy, accuracy, brier score, expected calibration error (ECE) and negative log-likelihood (NLL). In our explanation, we will focus on the accuracy and entropy to keep it simple.\u003c/p\u003e\n\u003cp\u003eWith corruption severity factor of 5, we obtain the following results :\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eentropy\u003c/th\u003e\n\u003cth\u003eaccuracy\u003c/th\u003e\n\u003cth\u003ebrier\u003c/th\u003e\n\u003cth\u003eece\u003c/th\u003e\n\u003cth\u003enll\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003ebaseline\u003c/td\u003e\n\u003ctd\u003e0.656294\u003c/td\u003e\n\u003ctd\u003e0.7480\u003c/td\u003e\n\u003ctd\u003e0.349862\u003c/td\u003e\n\u003ctd\u003e0.032466\u003c/td\u003e\n\u003ctd\u003e0.729336\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003emixup\u003c/td\u003e\n\u003ctd\u003e0.640811\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003e0.7578\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e0.335403\u003c/td\u003e\n\u003ctd\u003e0.024429\u003c/td\u003e\n\u003ctd\u003e0.703844\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eregmixup\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003e0.676174\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e0.7564\u003c/td\u003e\n\u003ctd\u003e0.340233\u003c/td\u003e\n\u003ctd\u003e0.023135\u003c/td\u003e\n\u003ctd\u003e0.711405\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eFirst of all, we can see that the accuracy is quite similar for the 3 models. This makes sense as the corruption severity factor is quite low, thus cifar-10-c is not very different from the original cifar-10. However, we can see that the entropy of the RegMixup model is higher than the one of the Mixup model. This is symptomatic of Mixup\u0026rsquo;s underconfidence. As stated previously, given the low corruption severity factor of cifar-10-c, the underconfidence of Mixup does not impact its performances in a visible manner.\u003c/p\u003e\n\u003cp\u003eWith corruption severity factor of 15, we obtain the following results :\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eentropy\u003c/th\u003e\n\u003cth\u003eaccuracy\u003c/th\u003e\n\u003cth\u003ebrier\u003c/th\u003e\n\u003cth\u003eece\u003c/th\u003e\n\u003cth\u003enll\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003ebaseline\u003c/td\u003e\n\u003ctd\u003e0.615607\u003c/td\u003e\n\u003ctd\u003e0.7402\u003c/td\u003e\n\u003ctd\u003e0.358522\u003c/td\u003e\n\u003ctd\u003e0.048414\u003c/td\u003e\n\u003ctd\u003e0.750933\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003emixup\u003c/td\u003e\n\u003ctd\u003e0.698558\u003c/td\u003e\n\u003ctd\u003e0.7558\u003c/td\u003e\n\u003ctd\u003e0.338540\u003c/td\u003e\n\u003ctd\u003e0.014760\u003c/td\u003e\n\u003ctd\u003e0.709190\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eregmixup\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003e0.702599\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003e0.7614\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e0.327945\u003c/td\u003e\n\u003ctd\u003e0.008439\u003c/td\u003e\n\u003ctd\u003e0.687550\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eHere the results are much more unequivocal. As the severity factor increases, the baseline model drops in accuracy and entropy, Mixup also drops in accuracy but increases in entropy and RegMixup increases in accuracy and entropy. Here, RegMixup has the higher entropy as the model has higher entropy for OOD samples which are more frequent at this corruption level. Mixup shows a greater delta increase in entropy due to its higher predictive entropy tendency whether or not samples are OOD or ID. Consequently, RegMixup is more confident and accurate than the Mixup model eventhough Mixup is not fully underperforming.\u003c/p\u003e\n\u003ch2 id=\"section-4\"\u003e4. Conclusion\u003c/h2\u003e\n\u003cp\u003eAs a conclusion, we have seen that RegMixup is a powerful method to regularize deep neural networks. Despite being very simple and cost-effective, it is important to specify that the paper does not provide a theoretical explanation of the method. These experimental grounds are very promising but it appears important to stay cautious while utilizing RegMixup.\u003c/p\u003e\n",
      "url": "http://localhost:1313/posts/robustai_regmixup/",
      "date_published": "24036-24-09T338:2424:00+01:00",
      "date_modified": "24036-24-09T338:2424:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "9bec8a0b2d12c702729b80a3b910f16d3d73250f",
      "title": "Adversarially Reweighted Learning",
      "summary": "",
      "content_text": "Fairness without Demographics through Adversarially Reweighted Learning\rAuthors: Pierre Fihey \u0026 Guerlain Messin\rTable of Contents Fairness issues in ML and AI The privacy of demographic’s data The Adversarial Reweighted Learning Model An Hypothesis: Protected Groups are Correlated with Both Features and Labels Computational identifiability of protected groups The Rawlsian Max-Min Fairness principle The ARL objective The Model Architecture Results analysis Conclusion Fairness issues in ML and AI As Machine Learning and Artificial Intelligence algorithms are increasingly developed to aid and automate decision-making, it is crucial that they provide ethical, fair and discrimination-free results. However, discriminative biases are now found in many facets of AI and ML and affect many possible applications.\nSuch biases can be found in NLP applications, where we can see that generative AIs often associate certain genders or ethnic groups with professions. In computer vision, the lack of diversity in the training data also induces numerous discriminatory biases, since we can see that the algorithms\u0026rsquo; performances differ according to age, gender and ethnic group, which can lead to unfair treatments. Machine Learning models, used in decision-making processes from loan approvals to job applications, can inherit historical biases present in their training data, resulting in unfair outcomes.\nThe root of these biases lies in the historical prejudices and inequalities that are inadvertently encoded into the datasets used to train AI and ML models. These datasets often reflect the societal, cultural, and institutional biases that have existed over time. As a result, when AI and ML technologies are trained on such data, they risk mirroring and amplifying these biases instead of offering neutral, objective outputs. It is therefore vital to focus on AI fairness to enable the development of technologies that will benefit everyone fairly and equitably.\nThe privacy of demographic’s data Strict regulations established by laws such as the General Data Protection Regulation (GDPR) severely restrict the collection of demographic data, including age, gender, religion and other personal attributes. This legal framework, designed to protect individual privacy and data rights, poses a problem for the study of discriminatory bias in algorithms, since it becomes almost impossible to measure. This situation creates a real paradox, since protecting personal data conflicts with limiting discrimination and promoting fairness for ML and iA algorithms.\nIn this blog, we\u0026rsquo;ll look at the paper Fairness without Demographics through Adversarially Reweighted Learning, published by Google\u0026rsquo;s 2020 research team to propose a method for improving the fairness of AI models despite the lack of demographic data. Indeed, while much previous works have focused on improving fairness in AI and ML, most of these works assume that models have access to this protected data. Given the observations made above, the problem this paper attempts to address is as follows: How can we train a ML model to improve fairness when we do not have access to protected features neither at training nor inference time, i.e., we do not know protected group memberships?\nThe Adversarial Reweighted Learning Model An Hypothesis: Protected Groups are Correlated with Both Features and Labels While access to the protected features is often impossible, the authors of this paper assume that there is a strong correlation between these variables and the observable features X as well as the class labels Y. Although these correlations are the cause of the fairness problems faced by ML algorithms, they represent a real advantage here, as they can help to identify these protected groups and thus to evaluate and correct possible discrimination biases.\nThe authors have shown that this hypothesis is frequently verified. For example, they were able to predict the race and gender of individuals in the Adults and LSAC Datasets with high accuracy from unprotected features and labels.\nThis assumption therefore implies that protected groups can be computationally identifiable. It is on this notion of computational identifiability that the model proposed by Google\u0026rsquo;s research team is based to outperform previous work.\nComputational identifiability of protected groups Computational identifiability refers to the ability to algorithmically identify specific subgroups or patterns within a dataset based on certain criteria, using computable functions. Mathematically, this notion is defined as follows:\nFor a family of binary functions $F$, we say that a subgroup $S$ is computationally-identifiable if there is a function $f : X \\times Y \\rightarrow \\text{{0, 1}}$ in $F$ such that $f(x, y) = 1$ if and only if $(x, y) \\in S$.\nThis function typically maps input data to a binary outcome, indicating protected subgroup membership. While many previous works have used this principle of computational identifiability, the model presented in this article differs in that it does not require these subgroups to be present in the input space, but also in its objective. While most work has focused on reducing the efficiency gap between each subgroup, the ARL model aims to increase efficiency for these subgroups, while considering that this should not be at the expense of the other groups. Indeed, the authors have decided to follow the Rawlsian Max Min fairness principle, which we present below.\nThe Rawlsian Max-Min Fairness principle In philosophy, the Rawlsian Max Min principle of distributive justice is defined by John Rawls as maximizing the welfare of the most disadvantaged member of society. In a mathematical context, this can be translated as maximizing the minimum utility U a model has across all groups s ∈ S. We adopt the following definition:\nDefinition (Rawslan Max-Min Fairness): Suppose $H$ is a set of hypotheses, and $U_{D_s}(h)$ is the expected utility of the hypothesis $h$ for the individuals in group $s$, then a hypothesis $h^* $ is said to satisfy Rawlsian Max-Min fairness principle if it maximizes the utility of the worst-off group, i.e., the group with the lowest utility. $$h^* = argmax_{h \\in H} min_{s \\in S} U_{D_s}(h)$$\nThe Maxmin Rawlsian principle inherently accepts the existence of inequalities, as its core aim is not to ensure uniform outcomes across all groups but rather to maximize the overall utility, particularly focusing on enhancing the welfare of the least advantaged. This is what will enable our model to obtain truly relevant results, and we\u0026rsquo;ll now see how it adapts this principle to define a loss function to be minimized during training.\nThe ARL objective To adapt this Rawlsian principle to a Machine Learning task, the authors decided to set up a MinMax Problem. A minmax algorithm is a mathematical problem defined in game theory. Its aim is to optimize the worst possible scenario for a player, assuming that the opponent plays optimally. The aim is now to minimize the highest loss, i.e. the loss of the most disadvantaged protected group. This new objective function is defined as follows:\n$$J(\\theta, \\lambda) := min_{\\theta} max_{\\lambda} \\sum_{s \\in S} \\lambda_s L_{D_s}(h)$$ $$= min_{\\theta} max_{\\lambda} \\sum_{i=0}^{n} \\lambda_{s_i} l(h(x_i), y_i)$$\nWith $l(.,.)$ the cross-entropy loss and lambda the weights that maximize the weighted loss of protected groups. To solve this minmax problem, the authors set up a special architecture consisting of two neural networks, a learner and an adversary.\nThe Model Architecture As previously announced, the authors therefore decided to implement the Adversarial Reweighted Learning (ARL) approach, training two models alternately.\nThe learner optimizes for the main classification task, and aims to learn the best parameters θ that minimizes expected loss.\nThe adversary learns a function mapping $f_\\phi : X \\times Y \\rightarrow [0, 1]$ to computationally-identifiable regions with high loss, and makes an adversarial assignment of weight vector $\\lambda_\\phi : f_\\phi \\rightarrow \\mathbb{R}$ so as to maximize the expected loss.\nThe learner then adjusts itself to minimize the adversarial loss: $$J(\\theta, \\phi) = min_{\\theta} max_{\\phi} \\sum_{i=1}^{n} \\lambda_{\\phi}(x_i, y_i) \\cdot l_{ce}(h_\\theta(x_i), y_i)$$\nTo ensure that the loss function is well defined, it\u0026rsquo;s crucial to introduce specific constraints on the weights used in the loss function. Ensuring these weights are non-negative, prevent zero values to include all training examples, and are normalized, addresses potential instability and promotes uniform contribution across the dataset.\n$$\\lambda_{\\phi}(x_i, y_i) = 1 + n \\cdot \\frac{f_{\\phi}(x_i, y_i)}{\\sum_{i=1}^{n} f_{\\phi}(x_i, y_i)}$$\nThe authors have implemented these two networks using standard feed-forward network. The learner is a fully connected two-layer feed-forward network with 64 and 32 hidden units in the hidden layers, with ReLU activation function. For small datasets, the adversary which performs the best is a linear model.\nResults analysis This section provides a detailed examination of the results obtained from our implementation of the Adversarial Reweighted Learning (ARL) model. We replicate the experiments conducted by Lahoti et al. and present the outcomes of our implementation. Furthermore, we analyze the significance of the results through a comprehensive evaluation.\nReproducibility We first reproduce the results reported by Lahoti et al. using their TensorFlow implementation. However, due to the absence of optimal hyperparameters, we utilize default parameters for our runs. As a result, our AUC scores are lower than those reported in the original paper. For instance, the average AUC for the Adult dataset in Lahoti et al.\u0026rsquo;s work is 0.907, whereas our run yields an AUC of 0.497. Similarly, for the LSAC dataset, Lahoti et al. report an AUC of 0.823, whereas we obtain 0.518. The COMPAS dataset also exhibits a similar trend, with Lahoti et al. reporting an AUC of 0.748, compared to our result of 0.536. Subsequent experimentation with optimal parameters from TensorFlow implementation demonstrates improved performance, although AUC scores remain lower than those presented in the original paper.\nReplicability We replicate the experiments using our PyTorch implementation of the ARL model with optimal hyperparameters obtained through grid-search. Comparing the AUC scores with Lahoti et al.\u0026rsquo;s results reveals close alignment for the Adult and LSAC datasets. However, a slightly larger difference is observed for the COMPAS dataset. Notably, all AUC metrics for the COMPAS dataset are lower than the baseline model presented by Lahoti et al. This discrepancy suggests potential challenges with dataset size, leading to increased variance in results. Nonetheless, our PyTorch implementation demonstrates consistency with Lahoti et al.\u0026rsquo;s findings, highlighting the robustness of the ARL model across different implementations.\nSignificance Evaluation We conduct significance tests to evaluate the performance improvement of our PyTorch-implemented ARL model compared to a simple baseline model. Despite observing notable improvements in fairness metrics, none of the p-values obtained are less than 0.05. Consequently, according to established significance criteria, the performance enhancement achieved by our ARL model is not statistically significant. This finding underscores the need for further investigation into the efficacy of adversarial learning methods in enhancing fairness without demographic information.\nConclusion In this study, we critically examined the paper \u0026ldquo;Fairness without Demographics through Adversarially Reweighted Learning\u0026rdquo; by Lahoti et al., focusing on reproducibility, replicability, and the significance of reported results. While encountering challenges in reproducing Lahoti et al.\u0026rsquo;s results due to parameter settings and dataset characteristics, we successfully replicated the experiments using our PyTorch implementation. Despite demonstrating consistency with the original findings, our significance tests indicate a lack of statistical significance in the performance improvement achieved by the ARL model. This prompts further inquiry into the suitability of adversarial learning approaches for addressing fairness concerns in machine learning without relying on demographic data.\nAnnexes References [1] Lahoti, P., Beutel, A., Chen, J., Lee, K., Prost, F., Thain, N., Wang, X., \u0026amp; Chi, E. H. (2020). Fairness without demographics through adversarially reweighted learning. arXiv preprint arXiv:2006.13114.\n[2] Veale, M., \u0026amp; Binns, R. (2017). Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data. Big Data \u0026amp; Society, 4(2), 2053951717743530.\n[3] Hanley, J. A., \u0026amp; McNeil, B. J. (1982). The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology, 143(1), 29-36.\n[4] Hanley, J. A., \u0026amp; McNeil, B. J. (1983). A method of comparing the areas under receiver operating characteristic curves derived from the same cases. Radiology, 148(3), 839-843.\n[5] Dua, D., \u0026amp; Graff, C. (2019). UCI machine learning repository.\n[6] Kim, M. P., Ghorbani, A., \u0026amp; Zou, J. (2019). Multiaccuracy: Black-box post-processing for fairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (pp. 247-254).\n[7] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., \u0026hellip; \u0026amp; Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27, 2672-2680.\n[8] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., \u0026hellip; \u0026amp; Chintala, S. (2019). PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32 (pp. 8024-8035).\n[9] Kamishima, T., Akaho, S., \u0026amp; Sakuma, J. (2011). Fairness-aware learning through regularization approach. In 2011 IEEE 11th International Conference on Data Mining Workshops (pp. 643-650). IEEE.\n",
      "content_html": "\u003ch1 style=\"font-size: 36px;\"\u003eFairness without Demographics through Adversarially Reweighted Learning\u003c/h1\u003e\r\n\u003ch1 style=\"font-size: 24px;\"\u003eAuthors: Pierre Fihey \u0026 Guerlain Messin\u003c/h1\u003e\r\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eFairness issues in ML and AI\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eThe privacy of demographic’s data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eThe Adversarial Reweighted Learning Model\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eAn Hypothesis: Protected Groups are Correlated with Both Features and Labels\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eComputational identifiability of protected groups\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eThe Rawlsian Max-Min Fairness principle\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eThe ARL objective\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-7\"\u003eThe Model Architecture\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-8\"\u003eResults analysis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-9\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0\"\u003eFairness issues in ML and AI\u003c/h2\u003e\n\u003cp\u003eAs Machine Learning and Artificial Intelligence algorithms are increasingly developed to aid and automate decision-making, it is crucial that they provide ethical, fair and discrimination-free results. However, discriminative biases are now found in many facets of AI and ML and affect many possible applications.\u003c/p\u003e\n\u003cp\u003eSuch biases can be found in NLP applications, where we can see that generative AIs often associate certain genders or ethnic groups with professions. In computer vision, the lack of diversity in the training data also induces numerous discriminatory biases, since we can see that the algorithms\u0026rsquo; performances differ according to age, gender and ethnic group, which can lead to unfair treatments.\nMachine Learning models, used in decision-making processes from loan approvals to job applications, can inherit historical biases present in their training data, resulting in unfair outcomes.\u003c/p\u003e\n\u003cp\u003eThe root of these biases lies in the historical prejudices and inequalities that are inadvertently encoded into the datasets used to train AI and ML models. These datasets often reflect the societal, cultural, and institutional biases that have existed over time. As a result, when AI and ML technologies are trained on such data, they risk mirroring and amplifying these biases instead of offering neutral, objective outputs. It is therefore vital to focus on AI fairness to enable the development of technologies that will benefit everyone fairly and equitably.\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eThe privacy of demographic’s data\u003c/h2\u003e\n\u003cp\u003eStrict regulations established by laws such as the General Data Protection Regulation (GDPR) severely restrict the collection of demographic data, including age, gender, religion and other personal attributes. This legal framework, designed to protect individual privacy and data rights, poses a problem for the study of discriminatory bias in algorithms, since it becomes almost impossible to measure. This situation creates a real paradox, since protecting personal data conflicts with limiting discrimination and promoting fairness for ML and iA algorithms.\u003c/p\u003e\n\u003cp\u003eIn this blog, we\u0026rsquo;ll look at the paper Fairness without Demographics through Adversarially Reweighted Learning, published by Google\u0026rsquo;s 2020 research team to propose a method for improving the fairness of AI models despite the lack of demographic data. Indeed, while much previous works have focused on improving fairness in AI and ML, most of these works assume that models have access to this protected data. Given the observations made above, the problem this paper attempts to address is as follows: How can we train a ML model to improve fairness when we do not have access to protected features neither at training nor inference time, i.e., we do not know protected group memberships?\u003c/p\u003e\n\u003ch2 id=\"section-2\"\u003eThe Adversarial Reweighted Learning Model\u003c/h2\u003e\n\u003ch3 id=\"section-3\"\u003eAn Hypothesis: Protected Groups are Correlated with Both Features and Labels\u003c/h3\u003e\n\u003cp\u003eWhile access to the protected features is often impossible, the authors of this paper assume that there is a strong correlation between these variables and the observable features X as well as the class labels Y. Although these correlations are the cause of the fairness problems faced by ML algorithms, they represent a real advantage here, as they can help to identify these protected groups and thus to evaluate and correct possible discrimination biases.\u003c/p\u003e\n\u003cp\u003eThe authors have shown that this hypothesis is frequently verified. For example, they were able to predict the race and gender of individuals in the Adults and LSAC Datasets with high accuracy from unprotected features and labels.\u003c/p\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"/images/Fihey_Messin/Identifying_Groups.png\"\r\n  alt=\"Identifying Groups\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003eThis assumption therefore implies that protected groups can be computationally identifiable. It is on this notion of computational identifiability that the model proposed by Google\u0026rsquo;s research team is based to outperform previous work.\u003c/p\u003e\n\u003ch3 id=\"section-4\"\u003eComputational identifiability of protected groups\u003c/h3\u003e\n\u003cp\u003eComputational identifiability refers to the ability to algorithmically identify specific subgroups or patterns within a dataset based on certain criteria, using computable functions. Mathematically, this notion is defined as follows:\u003c/p\u003e\n\u003cp\u003eFor a family of binary functions $F$, we say that a subgroup $S$ is computationally-identifiable if there is a function $f : X \\times Y \\rightarrow \\text{{0, 1}}$ in $F$ such that $f(x, y) = 1$ if and only if $(x, y) \\in S$.\u003c/p\u003e\n\u003cp\u003eThis function typically maps input data to a binary outcome, indicating protected subgroup membership. While many previous works have used this principle of computational identifiability, the model presented in this article differs in that it does not require these subgroups to be present in the input space, but also in its objective. While most work has focused on reducing the efficiency gap between each subgroup, the ARL model aims to increase efficiency for these subgroups, while considering that this should not be at the expense of the other groups. Indeed, the authors have decided to follow the Rawlsian Max Min fairness principle, which we present below.\u003c/p\u003e\n\u003ch3 id=\"section-5\"\u003eThe Rawlsian Max-Min Fairness principle\u003c/h3\u003e\n\u003cp\u003eIn philosophy, the Rawlsian Max Min principle of distributive justice is defined by John Rawls as maximizing the welfare of the most disadvantaged member of society. In a mathematical context, this can be translated as maximizing the minimum utility U a model has across all groups s ∈ S. We adopt the following definition:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDefinition (Rawslan Max-Min Fairness):\u003c/strong\u003e Suppose $H$ is a set of hypotheses, and $U_{D_s}(h)$ is the expected utility of the hypothesis $h$ for the individuals in group $s$, then a hypothesis $h^* $ is said to satisfy Rawlsian Max-Min fairness principle if it maximizes the utility of the worst-off group, i.e., the group with the lowest utility.\n$$h^* = argmax_{h \\in H} min_{s \\in S} U_{D_s}(h)$$\u003c/p\u003e\n\u003cp\u003eThe Maxmin Rawlsian principle inherently accepts the existence of inequalities, as its core aim is not to ensure uniform outcomes across all groups but rather to maximize the overall utility, particularly focusing on enhancing the welfare of the least advantaged.  This is what will enable our model to obtain truly relevant results, and we\u0026rsquo;ll now see how it adapts this principle to define a loss function to be minimized during training.\u003c/p\u003e\n\u003ch3 id=\"section-6\"\u003eThe ARL objective\u003c/h3\u003e\n\u003cp\u003eTo adapt this Rawlsian principle to a Machine Learning task, the authors decided to set up a MinMax Problem. A minmax algorithm is a mathematical problem defined in game theory. Its aim is to optimize the worst possible scenario for a player, assuming that the opponent plays optimally.\nThe aim is now to minimize the highest loss, i.e. the loss of the most disadvantaged protected group. This new objective function is defined as follows:\u003c/p\u003e\n\u003cp\u003e$$J(\\theta, \\lambda) := min_{\\theta} max_{\\lambda} \\sum_{s \\in S} \\lambda_s L_{D_s}(h)$$\n$$= min_{\\theta} max_{\\lambda} \\sum_{i=0}^{n} \\lambda_{s_i} l(h(x_i), y_i)$$\u003c/p\u003e\n\u003cp\u003eWith $l(.,.)$ the cross-entropy loss and lambda the weights that maximize the weighted loss of protected groups. To solve this minmax problem, the authors set up a special architecture consisting of two neural networks, a learner and an adversary.\u003c/p\u003e\n\u003ch3 id=\"section-7\"\u003eThe Model Architecture\u003c/h3\u003e\n\u003cp\u003eAs previously announced, the authors therefore decided to implement the Adversarial Reweighted Learning (ARL) approach, training two models alternately.\u003c/p\u003e\n\u003cp\u003eThe learner optimizes for the main classification task, and aims to learn the best parameters θ that minimizes expected loss.\u003c/p\u003e\n\u003cp\u003eThe adversary learns a function mapping $f_\\phi : X \\times Y \\rightarrow [0, 1]$ to computationally-identifiable regions with high loss, and makes an adversarial assignment of weight vector $\\lambda_\\phi : f_\\phi \\rightarrow \\mathbb{R}$ so as to maximize the expected loss.\u003c/p\u003e\n\u003cp\u003eThe learner then adjusts itself to minimize the adversarial loss:\n$$J(\\theta, \\phi) = min_{\\theta} max_{\\phi} \\sum_{i=1}^{n} \\lambda_{\\phi}(x_i, y_i) \\cdot l_{ce}(h_\\theta(x_i), y_i)$$\u003c/p\u003e\n\u003cp\u003eTo ensure that the loss function is well defined, it\u0026rsquo;s crucial to introduce specific constraints on the weights used in the loss function. Ensuring these weights are non-negative, prevent zero values to include all training examples, and are normalized, addresses potential instability and promotes uniform contribution across the dataset.\u003c/p\u003e\n\u003cp\u003e$$\\lambda_{\\phi}(x_i, y_i) = 1 + n \\cdot \\frac{f_{\\phi}(x_i, y_i)}{\\sum_{i=1}^{n} f_{\\phi}(x_i, y_i)}$$\u003c/p\u003e\n\u003cp\u003eThe authors have implemented these two networks using standard feed-forward network. The learner is a fully connected two-layer feed-forward network with 64 and 32 hidden units in the hidden layers, with ReLU activation function. For small datasets, the adversary which performs the best is a linear model.\u003c/p\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"/images/Fihey_Messin/ARL_Computational_Graph.png\"\r\n  alt=\"ARL Computational Graph\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003ch2 id=\"section-8\"\u003eResults analysis\u003c/h2\u003e\n\u003cp\u003eThis section provides a detailed examination of the results obtained from our implementation of the Adversarial Reweighted Learning (ARL) model. We replicate the experiments conducted by Lahoti et al. and present the outcomes of our implementation. Furthermore, we analyze the significance of the results through a comprehensive evaluation.\u003c/p\u003e\n\u003ch3 id=\"reproducibility\"\u003eReproducibility\u003c/h3\u003e\n\u003cp\u003eWe first reproduce the results reported by Lahoti et al. using their TensorFlow implementation. However, due to the absence of optimal hyperparameters, we utilize default parameters for our runs. As a result, our AUC scores are lower than those reported in the original paper. For instance, the average AUC for the Adult dataset in Lahoti et al.\u0026rsquo;s work is 0.907, whereas our run yields an AUC of 0.497. Similarly, for the LSAC dataset, Lahoti et al. report an AUC of 0.823, whereas we obtain 0.518. The COMPAS dataset also exhibits a similar trend, with Lahoti et al. reporting an AUC of 0.748, compared to our result of 0.536. Subsequent experimentation with optimal parameters from TensorFlow implementation demonstrates improved performance, although AUC scores remain lower than those presented in the original paper.\u003c/p\u003e\n\u003ch3 id=\"replicability\"\u003eReplicability\u003c/h3\u003e\n\u003cp\u003eWe replicate the experiments using our PyTorch implementation of the ARL model with optimal hyperparameters obtained through grid-search. Comparing the AUC scores with Lahoti et al.\u0026rsquo;s results reveals close alignment for the Adult and LSAC datasets. However, a slightly larger difference is observed for the COMPAS dataset. Notably, all AUC metrics for the COMPAS dataset are lower than the baseline model presented by Lahoti et al. This discrepancy suggests potential challenges with dataset size, leading to increased variance in results. Nonetheless, our PyTorch implementation demonstrates consistency with Lahoti et al.\u0026rsquo;s findings, highlighting the robustness of the ARL model across different implementations.\u003c/p\u003e\n\u003ch3 id=\"significance-evaluation\"\u003eSignificance Evaluation\u003c/h3\u003e\n\u003cp\u003eWe conduct significance tests to evaluate the performance improvement of our PyTorch-implemented ARL model compared to a simple baseline model. Despite observing notable improvements in fairness metrics, none of the p-values obtained are less than 0.05. Consequently, according to established significance criteria, the performance enhancement achieved by our ARL model is not statistically significant. This finding underscores the need for further investigation into the efficacy of adversarial learning methods in enhancing fairness without demographic information.\u003c/p\u003e\n\u003ch2 id=\"section-9\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIn this study, we critically examined the paper \u0026ldquo;Fairness without Demographics through Adversarially Reweighted Learning\u0026rdquo; by Lahoti et al., focusing on reproducibility, replicability, and the significance of reported results. While encountering challenges in reproducing Lahoti et al.\u0026rsquo;s results due to parameter settings and dataset characteristics, we successfully replicated the experiments using our PyTorch implementation. Despite demonstrating consistency with the original findings, our significance tests indicate a lack of statistical significance in the performance improvement achieved by the ARL model. This prompts further inquiry into the suitability of adversarial learning approaches for addressing fairness concerns in machine learning without relying on demographic data.\u003c/p\u003e\n\u003chr\u003e\n\u003chr\u003e\n\u003ch2 id=\"annexes\"\u003eAnnexes\u003c/h2\u003e\n\u003ch3 id=\"references\"\u003eReferences\u003c/h3\u003e\n\u003cp\u003e[1] Lahoti, P., Beutel, A., Chen, J., Lee, K., Prost, F., Thain, N., Wang, X., \u0026amp; Chi, E. H. (2020). Fairness without demographics through adversarially reweighted learning. arXiv preprint arXiv:2006.13114.\u003c/p\u003e\n\u003cp\u003e[2] Veale, M., \u0026amp; Binns, R. (2017). Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data. Big Data \u0026amp; Society, 4(2), 2053951717743530.\u003c/p\u003e\n\u003cp\u003e[3] Hanley, J. A., \u0026amp; McNeil, B. J. (1982). The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology, 143(1), 29-36.\u003c/p\u003e\n\u003cp\u003e[4] Hanley, J. A., \u0026amp; McNeil, B. J. (1983). A method of comparing the areas under receiver operating characteristic curves derived from the same cases. Radiology, 148(3), 839-843.\u003c/p\u003e\n\u003cp\u003e[5] Dua, D., \u0026amp; Graff, C. (2019). UCI machine learning repository.\u003c/p\u003e\n\u003cp\u003e[6] Kim, M. P., Ghorbani, A., \u0026amp; Zou, J. (2019). Multiaccuracy: Black-box post-processing for fairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (pp. 247-254).\u003c/p\u003e\n\u003cp\u003e[7] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., \u0026hellip; \u0026amp; Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27, 2672-2680.\u003c/p\u003e\n\u003cp\u003e[8] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., \u0026hellip; \u0026amp; Chintala, S. (2019). PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32 (pp. 8024-8035).\u003c/p\u003e\n\u003cp\u003e[9] Kamishima, T., Akaho, S., \u0026amp; Sakuma, J. (2011). Fairness-aware learning through regularization approach. In 2011 IEEE 11th International Conference on Data Mining Workshops (pp. 643-650). IEEE.\u003c/p\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\r\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\r\n\u003c/style\u003e\r\n\u003cscript type=\"text/x-mathjax-config\"\u003e\r\nMathJax.Hub.Config({\r\n    tex2jax: {\r\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\r\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\r\n    }\r\n});\r\nMathJax.Hub.Queue(function() {\r\n    var all = MathJax.Hub.getAllJax(), i;\r\n    for(i = 0; i \u003c all.length; i += 1) {\r\n        all[i].SourceElement().parentNode.className += ' has-jax';\r\n    }\r\n});\r\n\u003c/script\u003e\r\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\r\n",
      "url": "http://localhost:1313/posts/adversarially_reweighted_learning/",
      "date_published": "4036-04-09T335:44:00+01:00",
      "date_modified": "4036-04-09T335:44:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "de45490fdb0386b037083151783da0d850a9940c",
      "title": "Do Perceptually Aligned Gradients imply Robustness?",
      "summary": "",
      "content_text": "\rRobustness and Perceptually Aligned Gradients : does the converse stand ?\rAuthor: Yohann Zerbib\rTable of Contents Introduction Adversarial Attacks Perceptually Aligned Gradients Experiment To go further Conclusion References Introduction In the context of image recognition in Machine Learning, one could quickly realize that building robust models is crucial. Having failures could potentially lead to worrying outcomes and it is part of the design to aim to implement models that would be prevented against adversarials attacks, that will be explained. At some point, when reaching models that are robust, it somehow occurs that small variations made are easily interpretable by humans, something which is not common in current ML models such as this one. Having noticed this phenomenon, the authors of the paper would try to verify the opposite assumption. By building models that verify this idea of alignment with human perception, do we create robust models ?\nAdversarial attacks But before explaining the article, it could be relevant to explain briefly what are adversarial attacks and how it led to the design of robustness.\nAdversarial attacks refer to a class of techniques in machine learning where intentionally crafted input data is used to deceive or mislead a model, leading it to make incorrect predictions or classifications. These attacks exploit vulnerabilities in the model\u0026rsquo;s decision-making process, taking advantage of the model\u0026rsquo;s sensitivity to small changes in input data that might be imperceptible to humans. They are most prominently associated with deep learning models, particularly neural networks, due to their high capacity and ability to learn complex patterns.\nConcretly, in a theoretical framework, the usual example is to make a model classify an image of a cat as a dog or another animal, without any way for the human to notice it. However, consequences can be more dreadful in real life as one could consider what would happen if an autonomous vehicles missclassified a stop sign as speed limit sign.\n(Eykholt et al. [1])\nNow, let\u0026rsquo;s dive a bit deeper to understand how these errors happen. Several points can be highlighted, such as the level of linearity of Neural Networks, but one acknowledged moot point dwells on the use of Loss function in Deep Learning methods. Indeed, especially when considering datasets of pictures, there are many directions where the loss is steep. It would mean that it can be highly delicate to propose a good minimization of the loss. Moreover, the main idea for our problem is that a small change of the input can cause abrupt shifts in the decision process of our model. This effect increases with the dimensionnality (quality of pictures\u0026hellip;) and therefore will still be relevant with time.\nThe basic modelisation of an attack would be the following. Let\u0026rsquo;s consider :\na model $f\\ :\\ \\mathcal{X} \\ \\rightarrow \\ \\mathcal{Y}$ the input to pertub : $x \\in \\mathcal{X}$ a potential target label : $t \\in \\mathcal{Y}$ a small perturbation : $\\eta$ Then, mathematically, the attacker would try to have something that verifies $f(x + \\eta) = t$ (or any other label than $f(x)$ for an untargeted attack).\nNow, as one can imagine, it is possible to compute attacking models related to this framework. Let\u0026rsquo;s understand two well-knowns algorithms that follow this goal.\nFast Gradient Sign Method (FGSM) : This method can be targeted or untargeted. Let\u0026rsquo;s study the targeted one. The algorithm is the following [3]: One compute the perturbation $\\eta \\ =\\ \\epsilon \\ \\cdotp \\ sign( \\ \\nabla x\\ L( x,\\ t) \\ )$ where $\\epsilon$ is the perturbation size. Then, one would have $x\u0026rsquo;\\ =\\ x\\ −\\ \\eta $ such that we remain espilon close from $x$ and that $f(x\u0026rsquo;) = t$. The perturbation has to remain small to ensure it will be undetected by human\u0026rsquo;s perception.\nBut, at this point, one question arises : how can we be sure that $x\u0026rsquo;$ is still close to $x$? How can we be sure that we have $||x\\ −\\ x\u0026rsquo;||_{p} \\ \\leq \\ \\epsilon $ where p is a particular norm? To answer this question, norms are introduced and two important ones, used in the article are the following.\n$L_{2 }$ norm : This norm captures the global quantity of changes. It is the euclidean distance.\n$L_{\\infty }$ : This norm captures the maximum change in the vector.\nSo, we have several ways to have a level of control over the changed features.\nNow that the first intuition for attack is understood, one should take a rapid look at PGD (Projected Gradient Descent) [4], which will be used for the results of this blog. Other more complex methods exist (AutoAttack), and they are taken into account by the authors but they will not be explained here.\nThe algorithm starts with an initial perturbation. At each iteration, the algorithm takes a step in the direction of the gradient of the loss function with respect to the input. The gradient is calculated using backpropagation, and represents the direction of steepest ascent in the loss function. However, since we\u0026rsquo;re trying to reach a specific target, we actually want to move in the opposite direction, so we multiply the gradient by -1 (it is a maximization). The step size is proportional to the norm of the gradient, so we don\u0026rsquo;t overshoot or undershoot our target. After taking a step, the perturbation is projected back onto the allowed range, which is defined by the epsilon parameter. This is done by calculating the difference between the current input and the original input, and then scaling this difference so that it falls within the allowed range. This process is repeated for a certain number of iterations. (In this version of the algorithm, there is no control that it will truly be missclassified : one has to set an improtant enough number of iterations).\nHowever, our role here is not to learn how to create the best attacks, but more to learn how to defend them! And suprisingly, what has been shown is that the best way to achieve this goal is to have a training that includes adversarial attacks. Then, it all comes down to this optimization problem :\n$\\min_{\\theta }$ $\\mathbb{E}_{(x, y)} $ [A] where\nA = $(\\max_{\\eta \\leqslant \\epsilon }$ $L( f_{\\theta}( x\\ +\\ \\eta ) ,\\ y))$\nThis is more or less an optimization problem to solve with $\\theta$ the parameters to be learnt and where each training sample has a perturbation (an attack). It is linked with adversarial accuracy. We can train a model to be more robust, but chances are it will be less performant. It is up to the trainer to choose the best trade-off on a model.\nPerceptually Aligned gradients Finally, it is possible to dive more in the subject of the article. Training models as presented before, with a particular care to robustness empirically leads to have perceptually aligned gradients. Here, one should understand \u0026ldquo;gradient\u0026rdquo; as the mathematical concept, a vector which points to the direction of the greatest increase of its function. In other words, Perceptually Aligned Gradients correspond to a property, a byproduct of robust models, where the gradients are meaningful to humans. When the input image is slightly modified, the corresponding gradient directions reflect the changes that are perceptually relevant. In other words, the gradients make sense from a human perspective.\nHere an example given by the author on the CIFAR dataset ([2], Ganz et al.). The intuition is that for models other than the vanilla one, the target class representative of the adversarial examples contains an information about the new class. For example, going from a bird to a frog will get the image much more green and in the shape of the frog. It looks like a ghost information.\nNow, is it a Bidirectional Connection ? Let\u0026rsquo;s try to have some hints about it.\nThe first step to tackle this issue is to create those Perceptually Aligned Gradients without adversarial training.\nThen, it is shown that models with aligneds gradients can be considered as robust.\nFinally, a demonstration of the improvement of robustness through the increase of gradient alignment is proposed.\n1. Algorithm of the Model\nTo disentangle the creation of PAG with the usual robust training, a new method is developed. It relies on two elements.\nthe classical cross-entropy loss from the usual categorization problem framework,\nan auxiliary loss on the input-gradients, differentiable.\nThen, our global loss function would look like this :\n$L( x,\\ y) \\ =LCE\\ ( f_{\\theta }( x) ,\\ y) \\ + \\lambda\\sum_{y_{t} =1}^{C}L_{cos}( \\nabla_{x}f_{\\theta }(x)_{y_t},\\ g( x,\\ y_t))$\nIt is similar to training with a regularization part ($\\lambda$ would control the power of the regularization). $L_{cos}$ is the cosine similarity loss (it gives information on the similarity of the arguments).\nThis does not use robust model of any sort, on the hypothesis that we have ground-true PAG in the input. This is a strong hypothesis, and it is crucial to choose well those grounds-truth. Indeed, a lack of rigor here could lead to a bias. If the ground-truth was obtained through adversarial training previously, then this new approach would only be an equivalent of adversarial training, and that is something that must be avoided. This hypotesis will be studied just a bit later.\nAfter minimizing the loss, the model is tested through adversarial attacks (here, targeted PGD on the test set) to see if there is clearly PAG and if the adversarial accuracy is good.\n2. Creation of Perceptually Aligned Gradients\nAs we have seen in the formula just above, it is mandatory to have a ground-truth perceptually gradient $g( x,\\ y_t)$ for each training image and for each target class. However, finding those gradients are difficult and they are approximated. Firstly, let\u0026rsquo;s consider the heuristics to understand what happens.\nWith this objective in mind, we follow a straightforward assumption: the gradient $g( x,\\ y_t)$ ought to align with the overall direction of images belonging to the target class $y_t$. Hence, when provided with a target class representative, $r_{y_t}$, we establish the gradient to direct away from the current image and towards the representative. In other words, $g( x,\\ y_t) = r_{y_t} - x$\nTo implement this heuristic, three setups are provided.\n$\\textbf{One Image (OI):}$ Choose an arbitrary training set image with label $y_t$, and set $r_{y_t}$ to be that image as a global destination for $y_t$-targeted gradients.\n$\\textbf{Class Mean (CM):}$ Set $r_{y_t}$ to be the mean of all the training images with label $y_t$. This mean can be multiplied by a constant to obtain an image-like norm.\n$\\textbf{Nearest Neighbor (NN):}$ For each image $x$ and each target class$\\ y_{t} \\ \\in \\ {{1,\\ 2\\ .\\ .\\ .\\ ,\\ C}}$, we set the class representative $r_{y_t}(x)$ (now dependent on the image) to be the image\u0026rsquo;s nearest neighbor amongst a limited set of samples from class $y_t$, using L2 distance in the pixel space. More formally, we define $r( x,\\ y_{t}) \\ \\ =\\ \\underset{ \\begin{array}{l} \\widehat{x\\ } \\in \\ D_{y_{t}} \\ s.t.\\ \\hat{x} =x \\end{array}}{\\arg\\min} ||x\\ −\\ \\hat{x} ||_{2}{}$\nwhere $ D_{y_{t}}$ is the set of sample images with class $y_t$.\nNow, the more theoretical approach is provided thanks to score-based gradients. Authors have used Denoising Diffusion Probabilistic Models (DDPMs), to generate approximations of PAG.\nLet\u0026rsquo;s consider noisy versions of an image $x$, noted as $({x_{t}})_{t=1}^{T}$ and their distribution\n$(p_t({x_{t})})_{t=1}^{T}$.\nAn iterative process is employed for sampling, which begins from Gaussian noise and proceeds along the direction of the score function, defined as $\\nabla_{x_t} \\log p(x_t)$ and approximated by a neural network. It is suggested to incorporate class information into these networks, allowing them to model a class-dependent score function $\\nabla_{x_t} \\log p(x_t|y)$. We identify a resemblance between the class-dependent score function and classification loss gradients with respect to the input image, leading us to propose that gradients derived from DDPM can serve as an enhanced source for perceptually aligned gradients. We would have (one term disappears with the gradient w.r.t the input image) using Bayes\u0026rsquo; formula.\n\\begin{equation} \\nabla_{x_t} \\log p(x_t|y) = \\nabla_{x_t} \\log p(y|x_t) + \\nabla_{x_t} \\log p(x_t), \\end{equation}\nwhich results in\n\\begin{equation} \\nabla_{x_t} \\log p(y|x_t) = \\nabla_{x_t} \\log p(x_t|y) - \\nabla_{x_t} \\log p(x_t). \\end{equation}\nThis formulation introduces a new application of diffusion models – a systematic approach to estimate the appropriate gradients for the expression $\\log p(y|x_t)$. However, classification networks operate on noise-free images ($x$) rather than noisy ones ($x_t$). To link classifier input-gradients with DDPMs, we assume that $\\log p(y|x) \\approx log p(y|x_t)$, for certain noise levels $t$. Consequently, the desired estimation of \u0026ldquo;ground-truth\u0026rdquo; classifier input-gradients can be acquired by subtracting an unconditional score function from a class-conditional one. The selection of $t$ when distilling gradients through this method presents a tradeoff – excessively large values yield gradients unrelated to the input image (too noisy), while excessively small values produce perceptually insignificant ones (in low noise levels, the conditional and unconditional scores are nearly identical). Therefore, we choose $t$ to be of moderate values, generating both perceptually and image-relevant gradients. We denote this method as Score-Based Gradients (SBG).\nTo understand a bit more how it works, one has to consider that the variations of the noise from every $x_t$ can be controlled. Indeed, each different iteration takes the direction of the distribution $\\log p(x_t)$ (with stochasticity). In other terms, it takes the direction of our score function that can be estimated thanks to Neural Networks. That\u0026rsquo;s how you obtain your set of ground-truth gradients related to the input images.\nAt this point, we have four ways to approximate ground-truth gradients. (Three heuristics and a more theoretical one). The experiments presented here will use the NN approach that are very intuitive. What was favoured for real datasets was the score-based approach.\nExperiment Now, let\u0026rsquo;s experiment a bit. In this article, to understand what is happening, we will play a bit with the toy dataset. A 2 dimensional synthetic dataset is built. It contains 6000 samples of 2 classes. Every sample is on the line of equation $x_2 -2x_1=0$. Finally, each class contains three mods (1000 samples per mode) drawn from a Gaussian distribution. The idea is to observe manifolds as decision boundaries. Background of the plan will be colored according to the predicted class. Evaluation will be made on a test set.\nThe code is available at this link.\nTo this prediction task, a simple 2 layers MLP with ReLU is used. Two training are made with the same seed. The first is based on the usual cross-entropy loss whereas the second is made on the explained new loss.\nAs expected, 100% accuracy is obtained for this very simple task for both models on the test set. However, what about predicting adversarial examples ?\nLet\u0026rsquo;s first try it out with a targeted $L2$ PGD. Vanilla is only correct for 35 out of 600 samples, whereas this new approach obtains 583 out of 600. How can this be explained ? One should observe the decision boundaries.\nThis is what is obtained for the regular neural network with cross-entropy Loss.\nHere is the result obtained for the particular neural network with the new loss.\nWhat one should notice is the decision boundaries. The vanilla neural network provides manifolds that really stick to the data points. Going just a bit further can on the graph really can create a shift in the prediction. And that is what is happening with a targeted pgd, where there is only a small variation (semantically invisible).\nHowever, in the case of the PAG Neural Network, one can observe that around a mode of points, there is a much greater margin of the same class. This can be understood from the setup to create perceptually aligned gradients. Indeed, as we have seen, a target class was set based on a nearest neighbour approach, and the gradient point away from the current image and towards the class representative. Only then the cosine similarity between this gradient and the ground-truth approximated one from DDPMs.\nAnother possibility would be to see the impact of the size of the perturbation on the performance. Indeed, here, the given results corresponded to an epsilon value of 15. Increasing it decreases the accuracy to 75%. However, at a certain point, an augmentation of epsilon will not change anything anymore, probably because of a normalizing step in the targeted PGD algorithm.\nTo go further What\u0026rsquo;s next ? Testing the hypothesis on real datasets. Among them, CIFAR-10, STL (higher resolution) and CIFAR-100 (higher number of classes). The architecture to achieve those tasks are classical (Resnet-18, ViT). Here are the main results that can be highlighted.\nPAG approach is often similar and sometimes outperforms adversarially training approach. Score-based gradient seems to be the most accurate ground-truth approximation setup. It is also more notable for the ViT architecture. It also globally performs well on STL and CIFAR-100 (sometimes even better than adversarially training).\nBut, the question is not yet answered : Do Perceptually Aligned Gradients imply Robustness?\nAnd that\u0026rsquo;s where the regularization aspect of the loss is very useful. One can make variation over the hyperparameter $\\lambda$ to see what brings a bigger focus on the PAG loss. The authors have done it and are summarized with this table.\nAs one can see, the robustness increases with the increase of the regularization hyperparameter. The more the ghost features of the target class are visible (even if it not always comprehensible), the more the model is robust.\nSo, it seems that yes, models with PAG would be more robust.\nConclusion To draw a conclusion, this paper has empirically shown that PAG lead to more robustness in models. It was also mentionned that it could potentially be combined with Adversarially Training to gain more robustness, and there are probably some experiments and tests that could optimize that. The performance are also good and can be seen as an alternative, potentially not too costly. Sometimes it ouperforms Adversarially Training and it would be up to the user to decide which framework to employ for creating robust models. Finally, approximating ground-truth PAG needs additionnal research and discussion as even if the results tend to favour Score-Based Gradients, it happens that heuristics function better and there are potentially other approaches that have yet to be discovered. One should shed light on the fact that the diffusion models used need to be trained, and the training time gained over adversarially training is not as significant as with other heuristics if we consider this aspect.\nReferences EYKHOLT, Kevin, EVTIMOV, Ivan, FERNANDES, Earlence, et al. Robust physical-world attacks on deep learning visual classification. In : Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. p. 1625-1634.\nGanz, R., Kawar, B., \u0026amp; Elad, M. (2023, July). Do perceptually aligned gradients imply robustness?. In International Conference on Machine Learning (pp. 10628-10648). PMLR.\nGoodfellow, I. J., Shlens, J., \u0026amp; Szegedy, C. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., \u0026amp; Vladu, A. (2017). Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083.\n",
      "content_html": "\u003cstyle TYPE=\"text/css\"\u003e\r\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\r\n\u003c/style\u003e\r\n\u003cscript type=\"text/x-mathjax-config\"\u003e\r\nMathJax.Hub.Config({\r\n    tex2jax: {\r\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\r\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\r\n    }\r\n});\r\nMathJax.Hub.Queue(function() {\r\n    var all = MathJax.Hub.getAllJax(), i;\r\n    for(i = 0; i \u003c all.length; i += 1) {\r\n        all[i].SourceElement().parentNode.className += ' has-jax';\r\n    }\r\n});\r\n\u003c/script\u003e\r\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\r\n\u003ch1 style=\"font-size: 36px;\"\u003eRobustness and Perceptually Aligned Gradients : does the converse stand ?\u003c/h1\u003e\r\n\u003ch3 style=\"font-size: 24px;\"\u003eAuthor: Yohann Zerbib\u003c/h3\u003e\r\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eAdversarial Attacks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003ePerceptually Aligned Gradients\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eExperiment\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eTo go further\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn the context of image recognition in Machine Learning, one could quickly realize that building \u003cem\u003erobust\u003c/em\u003e models is crucial. Having failures could potentially lead to worrying outcomes and it is part of the design to aim to implement models that would be prevented against \u003cem\u003e\u003cstrong\u003eadversarials attacks\u003c/strong\u003e\u003c/em\u003e, that will be explained. At some point, when reaching models that are robust, it somehow occurs that small variations made are easily \u003cstrong\u003einterpretable by humans\u003c/strong\u003e, something which is not common in current ML models such as this one. Having noticed this phenomenon, the authors of the paper would try to verify the opposite assumption. By building models that verify this idea of alignment with human perception, do we create robust models ?\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eAdversarial attacks\u003c/h2\u003e\n\u003cp\u003eBut before explaining the article, it could be relevant to explain briefly what are adversarial attacks and how it led to the design of robustness.\u003c/p\u003e\n\u003cp\u003eAdversarial attacks refer to a class of techniques in machine learning where \u003cstrong\u003eintentionally crafted input data\u003c/strong\u003e is used to deceive or mislead a model, leading it to make incorrect predictions or classifications. These attacks exploit vulnerabilities in the model\u0026rsquo;s decision-making process, taking advantage of the model\u0026rsquo;s sensitivity to small changes in input data that might be \u003cstrong\u003eimperceptible to humans\u003c/strong\u003e.\nThey are most prominently associated with deep learning models, particularly neural networks, due to their high capacity and ability to learn complex patterns.\u003c/p\u003e\n\u003cp\u003eConcretly, in a theoretical framework, the usual example is to make a model classify an image of a cat as a dog or another animal, without any way for the human to notice it. However, consequences can be more dreadful in real life as one could consider what would happen if an autonomous vehicles missclassified a \u003cem\u003e\u003cstrong\u003estop sign as speed limit sign\u003c/strong\u003e\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"/images/Yohann_Zerbib/stop.png\"\r\n  alt=\"stop\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003e(Eykholt et al. [1])\u003c/p\u003e\n\u003cp\u003eNow, let\u0026rsquo;s dive a bit deeper to understand how these errors happen.\nSeveral points can be highlighted, such as the level of linearity of Neural Networks, but one acknowledged moot point dwells on the use of Loss function in Deep Learning methods. Indeed, especially when considering datasets of pictures, there are many directions where the \u003cstrong\u003eloss is steep\u003c/strong\u003e. It would mean that it can be highly delicate to propose a good minimization of the loss. Moreover, the main idea for our problem is that a \u003cstrong\u003esmall change\u003c/strong\u003e of the input can cause \u003cstrong\u003eabrupt shifts\u003c/strong\u003e in the decision process of our model. This effect increases with the dimensionnality (quality of pictures\u0026hellip;) and therefore will still be relevant with time.\u003c/p\u003e\n\u003cp\u003eThe basic modelisation of an attack would be the following. Let\u0026rsquo;s consider :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea model $f\\ :\\ \\mathcal{X} \\ \\rightarrow \\ \\mathcal{Y}$\u003c/li\u003e\n\u003cli\u003ethe input to pertub : $x \\in \\mathcal{X}$\u003c/li\u003e\n\u003cli\u003ea potential target label : $t \\in  \\mathcal{Y}$\u003c/li\u003e\n\u003cli\u003ea small perturbation : $\\eta$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen, mathematically, the attacker would try to have something that verifies $f(x + \\eta) = t$ (or any other label than $f(x)$ for an untargeted attack).\u003c/p\u003e\n\u003cp\u003eNow, as one can imagine, it is possible to compute attacking models related to this framework. Let\u0026rsquo;s understand two well-knowns algorithms that follow this goal.\u003c/p\u003e\n\u003ch3 id=\"fast-gradient-sign-method-fgsm-\"\u003eFast Gradient Sign Method (FGSM) :\u003c/h3\u003e\n\u003cp\u003eThis method can be \u003cem\u003e\u003cstrong\u003etargeted\u003c/strong\u003e\u003c/em\u003e or \u003cem\u003e\u003cstrong\u003euntargeted\u003c/strong\u003e\u003c/em\u003e. Let\u0026rsquo;s study the targeted one. The algorithm is the following [3]:\nOne compute the perturbation $\\eta \\ =\\ \\epsilon \\ \\cdotp \\ sign( \\ \\nabla x\\ L( x,\\ t) \\ )$ where $\\epsilon$ is the perturbation size. Then, one would have $x\u0026rsquo;\\ =\\ x\\ −\\ \\eta $ such that we remain espilon close from $x$ and that $f(x\u0026rsquo;) = t$.\nThe perturbation has to remain small to ensure it will be undetected by human\u0026rsquo;s perception.\u003c/p\u003e\n\u003cp\u003eBut, at this point, one question arises : how can we be sure that $x\u0026rsquo;$ is still close to $x$? How can we be sure that we have $||x\\ −\\ x\u0026rsquo;||_{p} \\ \\leq \\ \\epsilon $ where p is a particular norm? To answer this question, norms are introduced and two important ones, used in the article are the following.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e$L_{2 }$ norm : This norm captures the \u003cstrong\u003eglobal quantity of changes\u003c/strong\u003e. It is the euclidean distance.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e$L_{\\infty }$ : This norm captures the \u003cstrong\u003emaximum change\u003c/strong\u003e in the vector.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSo, we have several ways to have a level of control over the changed features.\u003c/p\u003e\n\u003cp\u003eNow that the first intuition for attack is understood, one should take a rapid look at \u003cstrong\u003ePGD\u003c/strong\u003e (Projected Gradient Descent) [4], which will be used for the results of this blog. Other more complex methods exist (AutoAttack), and they are taken into account by the authors but they will not be explained here.\u003c/p\u003e\n\u003cp\u003eThe algorithm starts with an initial perturbation. At each iteration, the algorithm takes a step in the direction of the gradient of the loss function with respect to the input. The gradient is calculated using backpropagation, and represents the direction of steepest ascent in the loss function. However, since we\u0026rsquo;re trying to reach a specific target, we actually want to move in the \u003cstrong\u003eopposite direction\u003c/strong\u003e, so we multiply the gradient by -1 (it is a maximization). The step size is proportional to the norm of the gradient, so we don\u0026rsquo;t overshoot or undershoot our target.\nAfter taking a step, the perturbation is \u003cem\u003eprojected\u003c/em\u003e back onto the allowed range, which is defined by the epsilon parameter. This is done by calculating the difference between the current input and the original input, and then scaling this difference so that it falls within the allowed range.\nThis process is repeated for a certain number of iterations. (In this version of the algorithm, there is no control that it will truly be missclassified : one has to set an improtant enough number of iterations).\u003c/p\u003e\n\u003cp\u003eHowever, our role here is not to learn how to create the best attacks, but more to learn how to \u003cem\u003e\u003cstrong\u003edefend\u003c/strong\u003e\u003c/em\u003e them! And suprisingly, what has been shown is that the best way to achieve this goal is to have a training that includes adversarial attacks.\nThen, it all comes down to this optimization problem :\u003c/p\u003e\n\u003cp\u003e$\\min_{\\theta }$ $\\mathbb{E}_{(x, y)} $ [A] where\u003c/p\u003e\n\u003cp\u003eA = $(\\max_{\\eta \\leqslant \\epsilon }$ $L( f_{\\theta}( x\\ +\\ \\eta ) ,\\ y))$\u003c/p\u003e\n\u003cp\u003eThis is more or less an optimization problem to solve with $\\theta$ the parameters to be learnt and where each training sample has a perturbation (an attack). It is linked with adversarial accuracy. We can train a model to be more robust, but chances are it will be less performant. It is up to the trainer to choose the \u003cstrong\u003ebest trade-off\u003c/strong\u003e on a model.\u003c/p\u003e\n\u003ch2 id=\"section-2\"\u003ePerceptually Aligned gradients\u003c/h2\u003e\n\u003cp\u003eFinally, it is possible to dive more in the subject of the article. Training models as presented before, with a particular care to robustness empirically leads to have \u003cem\u003eperceptually aligned gradients\u003c/em\u003e.\nHere, one should understand \u0026ldquo;gradient\u0026rdquo; as the mathematical concept, a vector which points to the direction of the greatest increase of its function. In other words, Perceptually Aligned Gradients correspond to a property, a byproduct of robust models, where the gradients are meaningful to humans. When the input image is slightly modified, the corresponding gradient directions reflect the changes that are \u003cstrong\u003eperceptually relevant\u003c/strong\u003e. In other words, the gradients \u003cem\u003emake sense\u003c/em\u003e from a human perspective.\u003c/p\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"/images/Yohann_Zerbib/pagdemo.png\"\r\n  alt=\"demopag\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003eHere an example given by the author on the CIFAR dataset ([2], Ganz et al.). The intuition is that for models other than the vanilla one, the target class representative of the adversarial examples contains an information about the new class. For example, going from a bird to a frog will get the image much more green and in the shape of the frog. It looks like a \u003cem\u003eghost\u003c/em\u003e information.\u003c/p\u003e\n\u003cp\u003eNow, is it a Bidirectional Connection ? Let\u0026rsquo;s try to have some hints about it.\u003c/p\u003e\n\u003cp\u003eThe first step to tackle this issue is to create those Perceptually Aligned Gradients without adversarial training.\u003c/p\u003e\n\u003cp\u003eThen, it is shown that models with aligneds gradients can be considered as robust.\u003c/p\u003e\n\u003cp\u003eFinally, a demonstration of the improvement of robustness through the increase of gradient alignment is proposed.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Algorithm of the Model\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTo disentangle the creation of PAG with the usual robust training, a new method is developed. It relies on two elements.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ethe classical cross-entropy loss from the usual categorization problem framework,\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ean auxiliary loss on the input-gradients, differentiable.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen, our global loss function would look like this :\u003c/p\u003e\n\u003cp\u003e$L( x,\\ y) \\ =LCE\\ ( f_{\\theta }( x) ,\\ y) \\ + \\lambda\\sum_{y_{t} =1}^{C}L_{cos}( \\nabla_{x}f_{\\theta }(x)_{y_t},\\ g( x,\\ y_t))$\u003c/p\u003e\n\u003cp\u003eIt is similar to training with a regularization part ($\\lambda$ would control the power of the regularization). $L_{cos}$ is the cosine similarity loss (it gives information on the similarity of the arguments).\u003c/p\u003e\n\u003cp\u003eThis does not use robust model of any sort, on the hypothesis that we have \u003cstrong\u003eground-true PAG\u003c/strong\u003e in the input. This is a \u003cstrong\u003estrong hypothesis\u003c/strong\u003e, and it is crucial to choose well those grounds-truth. Indeed, a lack of rigor here could lead to a bias. If the ground-truth was obtained through adversarial training previously, then this new approach would only be an equivalent of adversarial training, and that is something that must be avoided. This hypotesis will be studied just a bit later.\u003c/p\u003e\n\u003cp\u003eAfter minimizing the loss, the model is tested through adversarial attacks (here, targeted PGD on the test set) to see if there is clearly PAG and if the adversarial accuracy is good.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Creation of Perceptually Aligned Gradients\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAs we have seen in the formula just above, it is mandatory to have a ground-truth perceptually gradient $g( x,\\ y_t)$ for each training image and for each target class. However, finding those gradients are difficult and they are \u003cstrong\u003eapproximated\u003c/strong\u003e. Firstly, let\u0026rsquo;s consider the heuristics to understand what happens.\u003c/p\u003e\n\u003cp\u003eWith this objective in mind, we follow a straightforward assumption: the gradient $g( x,\\ y_t)$ ought to align with the overall direction of images belonging to the target class $y_t$. Hence, when provided with a target class representative, $r_{y_t}$, we establish the gradient to direct away from the current image and towards the representative. In other words, $g( x,\\ y_t) = r_{y_t} - x$\u003c/p\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"/images/Yohann_Zerbib/target.png\"\r\n  alt=\"target\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003eTo implement this heuristic, three setups are provided.\u003c/p\u003e\n\u003cp\u003e$\\textbf{One Image (OI):}$ Choose an arbitrary training set image with label $y_t$, and set $r_{y_t}$ to be that image as a global destination for $y_t$-targeted gradients.\u003c/p\u003e\n\u003cp\u003e$\\textbf{Class Mean (CM):}$ Set $r_{y_t}$ to be the mean of all the training images with label $y_t$. This mean can be multiplied by a constant to obtain an image-like norm.\u003c/p\u003e\n\u003cp\u003e$\\textbf{Nearest Neighbor (NN):}$  For each image $x$ and each target class$\\ y_{t} \\ \\in \\ {{1,\\ 2\\ .\\ .\\ .\\ ,\\ C}}$, we set the class representative $r_{y_t}(x)$ (now dependent on the image) to be the image\u0026rsquo;s nearest neighbor amongst a limited set of samples from class $y_t$, using L2 distance in the pixel space. More formally, we define\n$r( x,\\ y_{t}) \\ \\ =\\ \\underset{ \\begin{array}{l}\n\\widehat{x\\ } \\in \\ D_{y_{t}} \\ s.t.\\ \\hat{x} =x\n\\end{array}}{\\arg\\min} ||x\\ −\\ \\hat{x} ||_{2}{}$\u003c/p\u003e\n\u003cp\u003ewhere $ D_{y_{t}}$\nis the set of sample images with class $y_t$.\u003c/p\u003e\n\u003cp\u003eNow, the more theoretical approach is provided thanks to score-based gradients. Authors have used \u003cstrong\u003eDenoising Diffusion Probabilistic Models\u003c/strong\u003e (DDPMs), to generate approximations of PAG.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s consider noisy versions of an image $x$, noted as $({x_{t}})_{t=1}^{T}$ and their distribution\u003c/p\u003e\n\u003cp\u003e$(p_t({x_{t})})_{t=1}^{T}$.\u003c/p\u003e\n\u003cp\u003eAn iterative process is employed for sampling, which begins from Gaussian noise and proceeds along the direction of the score function, defined as $\\nabla_{x_t} \\log p(x_t)$ and approximated by a neural network. It is suggested to incorporate class information into these networks, allowing them to model a class-dependent score function $\\nabla_{x_t} \\log p(x_t|y)$. We identify a resemblance between the class-dependent score function and classification loss gradients with respect to the input image, leading us to propose that gradients derived from DDPM can serve as an enhanced source for perceptually aligned gradients. We would have (one term disappears with the gradient w.r.t the input image) using Bayes\u0026rsquo; formula.\u003c/p\u003e\n\u003cp\u003e\\begin{equation}\n\\nabla_{x_t} \\log p(x_t|y) = \\nabla_{x_t} \\log p(y|x_t) + \\nabla_{x_t} \\log p(x_t),\n\\end{equation}\u003c/p\u003e\n\u003cp\u003ewhich results in\u003c/p\u003e\n\u003cp\u003e\\begin{equation}\n\\nabla_{x_t} \\log p(y|x_t) = \\nabla_{x_t} \\log p(x_t|y) - \\nabla_{x_t} \\log p(x_t).\n\\end{equation}\u003c/p\u003e\n\u003cp\u003eThis formulation introduces a new application of diffusion models – a systematic approach to estimate the appropriate gradients for the expression $\\log p(y|x_t)$. However, classification networks operate on noise-free images ($x$) rather than noisy ones ($x_t$). To link classifier input-gradients with DDPMs, we assume that $\\log p(y|x) \\approx log p(y|x_t)$, for certain noise levels $t$. Consequently, the desired estimation of \u0026ldquo;ground-truth\u0026rdquo; classifier input-gradients can be acquired by subtracting an unconditional score function from a class-conditional one. The selection of $t$ when distilling gradients through this method presents a tradeoff – excessively large values yield gradients unrelated to the input image (too noisy), while excessively small values produce perceptually insignificant ones (in low noise levels, the conditional and unconditional scores are nearly identical). Therefore, we choose $t$ to be of moderate values, generating both perceptually and image-relevant gradients. We denote this method as Score-Based Gradients (SBG).\u003c/p\u003e\n\u003cp\u003eTo understand a bit more how it works, one has to consider that the variations of the noise from every $x_t$ can be controlled. Indeed, each different iteration takes the direction of the distribution $\\log p(x_t)$ (with stochasticity). In other terms, it takes the direction of our score function that can be estimated thanks to Neural Networks. That\u0026rsquo;s how you obtain your set of ground-truth gradients related to the input images.\u003c/p\u003e\n\u003cp\u003eAt this point, we have four ways to approximate ground-truth gradients. (Three heuristics and a more theoretical one). The experiments presented here will use the NN approach that are very intuitive. What was favoured for real datasets was the score-based approach.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003eExperiment\u003c/h2\u003e\n\u003cp\u003eNow, let\u0026rsquo;s experiment a bit. In this article, to understand what is happening, we will play a bit with the toy dataset. A 2 dimensional synthetic dataset is built. It contains 6000 samples of 2 classes. Every sample is on the line of equation $x_2 -2x_1=0$. Finally, each class contains \u003cstrong\u003ethree mods\u003c/strong\u003e (1000 samples per mode) drawn from a Gaussian distribution. The idea is to observe manifolds as decision boundaries. Background of the plan will be colored according to the predicted class. Evaluation will be made on a test set.\u003c/p\u003e\n\u003cp\u003eThe code is available at this \u003ca href=\"https://github.com/YohannZe/responsible-ai-datascience-ipParis.github.io.git\"\u003elink\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eTo this prediction task, a simple 2 layers MLP with ReLU is used. Two training are made with the same seed. The first is based on the usual cross-entropy loss whereas the second is made on the explained new loss.\u003c/p\u003e\n\u003cp\u003eAs expected, 100% accuracy is obtained for this very simple task for both models on the test set. However, what about predicting adversarial examples ?\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s first try it out with a targeted $L2$ PGD. Vanilla is only correct for 35 out of 600 samples, whereas this new approach obtains 583 out of 600.\nHow can this be explained ? One should observe the decision boundaries.\u003c/p\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"/images/Yohann_Zerbib/vanilla_l2_toy.png\"\r\n  alt=\"vanillal2\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003eThis is what is obtained for the regular neural network with cross-entropy Loss.\u003c/p\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"/images/Yohann_Zerbib/nn_l2_toy.png\"\r\n  alt=\"nnl2\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003eHere is the result obtained for the particular neural network with the new loss.\u003c/p\u003e\n\u003cp\u003eWhat one should notice is the decision boundaries. The vanilla neural network provides manifolds that really \u003cem\u003e\u003cstrong\u003estick\u003c/strong\u003e\u003c/em\u003e to the data points. Going just a bit further can on the graph really can create a shift in the prediction. And that is what is happening with a targeted pgd, where there is only a small variation (semantically invisible).\u003c/p\u003e\n\u003cp\u003eHowever, in the case of the PAG Neural Network, one can observe that around a mode of points, there is a \u003cstrong\u003emuch greater margin\u003c/strong\u003e of the same class. This can be understood from the setup to create perceptually aligned gradients. Indeed, as we have seen, a target class was set based on a nearest neighbour approach, and the gradient point away from the current image and towards the class representative. Only then the cosine similarity between this gradient and the ground-truth approximated one from DDPMs.\u003c/p\u003e\n\u003cp\u003eAnother possibility would be to see the impact of the size of the perturbation on the performance. Indeed, here, the given results corresponded to an epsilon value of 15. Increasing it decreases the accuracy to 75%. However, at a certain point, an augmentation of epsilon will not change anything anymore, probably because of a normalizing step in the targeted PGD algorithm.\u003c/p\u003e\n\u003ch2 id=\"section-4\"\u003eTo go further\u003c/h2\u003e\n\u003cp\u003eWhat\u0026rsquo;s next ? Testing the hypothesis on real datasets. Among them, CIFAR-10, STL (higher resolution) and CIFAR-100 (higher number of classes). The architecture to achieve those tasks are classical (Resnet-18, ViT). Here are the main results that can be highlighted.\u003c/p\u003e\n\u003cp\u003ePAG approach is often similar and sometimes outperforms adversarially training approach. Score-based gradient seems to be the most accurate ground-truth approximation setup. It is also more notable for the ViT architecture. It also globally performs well on STL and CIFAR-100 (sometimes even better than adversarially training).\u003c/p\u003e\n\u003cp\u003eBut, the question is not yet answered : \u003cem\u003e\u003cstrong\u003eDo Perceptually Aligned Gradients imply Robustness?\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eAnd that\u0026rsquo;s where the regularization aspect of the loss is very useful. One can make variation over the hyperparameter $\\lambda$ to see what brings a bigger focus on the PAG loss. The authors have done it and are summarized with this table.\u003c/p\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"/images/Yohann_Zerbib/regu.png\"\r\n  alt=\"regu\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003eAs one can see, the robustness increases with the increase of the regularization hyperparameter. The more the \u003cem\u003eghost\u003c/em\u003e features of the target class are visible (even if it not always comprehensible), the more the model is robust.\u003c/p\u003e\n\u003cp\u003eSo, it seems that yes, models with \u003cstrong\u003ePAG would be more robust\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"section-5\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eTo draw a conclusion, this paper has empirically shown that \u003cstrong\u003ePAG lead to more robustness\u003c/strong\u003e in models. It was also mentionned that it could potentially be combined with Adversarially Training to gain more robustness, and there are probably some experiments and tests that could optimize that. The performance are also good and can be seen as an alternative, potentially not too costly. Sometimes it \u003cstrong\u003eouperforms Adversarially Training\u003c/strong\u003e and it would be up to the user to decide which framework to employ for creating robust models. Finally, approximating ground-truth PAG needs additionnal research and discussion as even if the results tend to favour Score-Based Gradients, it happens that heuristics function better and there are potentially other approaches that have yet to be discovered. One should shed light on the fact that the diffusion models used need to be trained, and the training time gained over adversarially training is not as significant as with other heuristics if we consider this aspect.\u003c/p\u003e\n\u003ch2 id=\"section-6\"\u003eReferences\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eEYKHOLT, Kevin, EVTIMOV, Ivan, FERNANDES, Earlence, et al. Robust physical-world attacks on deep learning visual classification. In : Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. p. 1625-1634.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGanz, R., Kawar, B., \u0026amp; Elad, M. (2023, July). Do perceptually aligned gradients imply robustness?. In International Conference on Machine Learning (pp. 10628-10648). PMLR.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGoodfellow, I. J., Shlens, J., \u0026amp; Szegedy, C. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMadry, A., Makelov, A., Schmidt, L., Tsipras, D., \u0026amp; Vladu, A. (2017). Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n",
      "url": "http://localhost:1313/posts/robustness-and-pag-the-converse/",
      "date_published": "7026-07-09T26:77:00+01:00",
      "date_modified": "7026-07-09T26:77:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "00988a4ecebb51af6cf28a7b318568929bf7a01d",
      "title": "Statistical Minimax Rates Under Privacy",
      "summary": "",
      "content_text": "Estimating Privacy in Data Science: A Comprehensive Guide\rAuthor: Antoine Klein Github Link\rTable of Contents Incentives Introduction Definition Theory The case of multinomial estimation The case of density estimation Experiment Conclusion Quizz Why do we care about privacy ? Imagine, you\u0026rsquo;re quietly at home when the doorbell rings. You open the door and a government official appears: population census. Even though he shows you his official badge and you\u0026rsquo;d like to help him in the public interest, you find it hard to answer his questions as you go along. Indeed, the first questions about the date of your move are easy and public. On the other hand, when he asks about the number of children, marital status or your salary and what you do with it, you struggle. Not because you don\u0026rsquo;t know the answer, but because you\u0026rsquo;re faced with an ethical dilemma: transparency towards the state versus protection of personal data.\n$$\\text{In short, transparency goes against your privacy. }$$\nThis stress has major consequences: as you doubt what could happen to you with this data, but you still want to answer it, you underestimate your answers. On a wider scale, this leads to a suffrage bias and therefore a lack of knowledge of the real situation of your population. Warner [1], the first to tackle this problem from a statistical angle talks of an evasive bias and says:\n\u0026ldquo;for reasons of modesty, fear of being thought bigoted, or merely a reluctance to confide secrets to strangers, respondents to surveys might prefer to be able to answer certain questions non-truthfully, or at least without the interviewer knowing their true response\u0026rdquo;\nThis situation presented a trusted agent, in that he wasn\u0026rsquo;t trying to harm you directly. Now imagine that you agree to give him your personal data, but that on the way home, this agent of the state is mugged and someone steals his documents. Not only is this an attack on his person, it\u0026rsquo;s also an attack on yours: as the guarantor of your data, it\u0026rsquo;s now at the mercy of the attacker. The problem here is not to have protected yourself against a malicious agent.\nAdmittedly, these situations are rare, but with the densification of data, their analogies are omnipresent: cookies on the Internet, cyber-attacks, datacenter crashes\u0026hellip;One area for improvement is quite simply to better certify usage by means of cyber protection labels and leads to such a norm to achieve trust: In this blog, we propose to tackle this problem from a completely different angle: how to both enable the agent to take global measures and prevent it and any subsequent malicious agents from being able to re-identify my personal data. We\u0026rsquo;ll also use minimax bounds to answer the question: for a given privacy criterion, what\u0026rsquo;s the loss in terms of estimation? (fundamental trade-offs between privacy and convergence rate)\nScientific introduction Our blog will follow the same plan as the article that inspired it (John C. Duchi [2]),i.e. to show that response randomization achieves optimal convergence in the case of multinomial estimation, and then that this process can be generalized to any nonparametric distribution estimation. To this end, we will introduce the notion of local differential privacy as well as the minimax theory for obtaining optimal limits. All this will shed light on the trade-off between privacy and estimation rates. We will also explain algorithms to implement these optimal strategies. Finally, we will propose some experimental results.\nSome key definitions Let assume that you want to make private $X_1 , \u0026hellip; , X_n \\in X$ random variable and, as the statistician, you only observe $Z_1, . . . , Z_n ∈ Z$. The paper assumes that there exist a markov kernel that links the true ramdom variables and the observed ones as follow: $Q_i(Z_i | X_i = x)$.\nThe privacy mechanism is to be said non interactive if each $Z_i$ is obtained only conditionnaly on $X_i$ (and not on the others). This represents the fact that the privacy mechanism is memory less. If not, the mechnism is said to be interactive.\nIn the following, we will work only with non-interactive privacy mechanism but in the conlusion we will claim that newer studies showed that it is not enough for some larger problems.\n$Z_i$ is said to be α-local-differentially private for the original data $X_i$ if $$sup(\\frac{Q(Z | X_i = x)}{Q(Z | X_i = x\u0026rsquo;)} | x, x\u0026rsquo; ∈ X) ≤ exp(α)$$.\nAn intuitive way of understanding this definition is to see that the smaller α is (the more private it is), the more difficult it is to distinguish the distribution of Z conditional on two different X data.\nTheoretical results The case of multinomial estimation In this section, we return back to the problem of the private survey. For the statistician view, estimating a survey is estimating the parameter θ from the Bernouilli distribution $B(θ)$. This problem is a special case of multinomial estimation, where θ is now a multidimensional parameter that is amenable to simplex probability. $∆d := (θ ∈ ℝ+ |∑θ_j = 1)$.\nTheorem : Given α-local-differentially private $Z_i$, there exists some arbitrary constants $C_1$, $C_2$ such that for all $\\alpha\\in [0,1]$: $$C_1 min(1, \\frac{1}{\\sqrt{n\\alpha^2}}, \\frac{d}{n\\alpha^2}) ≤ E[|θ_{hat} - θ|^2] ≤ C_2 min(1, \\frac{d}{n\\alpha^2})$$ and $$C_1 min(1,\\frac{1}{\\sqrt{n\\alpha^2}}) ≤ E[||θ_{hat} - θ||_1] ≤ C_2 min(1,\\frac{d}{\\sqrt{n\\alpha^2}})$$.\nRecall from standard statistics: For non private independant $Z_i$ with finite variance, there exists some arbitrary constants $C_3$ such that: $$E[|θ_{hat} - θ|^2] ≤ \\frac{C_3}{n}$$\nIn others term, providing α-local-differentially privacy causes a reduction in the effective sample size of a factor $\\frac{\\alpha^2}{d}$ for best situations. It thus means that the asymptotically rate of convergences remains unchanged which is a really good news !\nPractical strategies The paper deals with one of the 2 standard methods to implement such a strategy that obtains the minimax rates:\nRandomized responses Laplace Noise (beyond paper) Randomized responses The intuition of this section is the following : to not allow the statistician to retrieve your personnal data in case of Bernouilli distribution, you toss a coin. If it is heads, you say to him your reel answer, if it is tails, you say the opposite. In his point of view, as he doesn\u0026rsquo;t know what was the result of the coin, he can\u0026rsquo;t distinguish if you tell the true or not but in a large scale, he knows that he will have half correct answer, half lies so that he can retrieve information.\nFor the multinomial estimation now, you will generalize this procedure to the multidimensionnal setting. For each coordinate, you will tell to the statistician the reel answer with a certain probability and lies otherwise. More precisely, its leads to :\n$$[Z]_j = x_j \\text{ with probability } \\frac{e^\\frac{\\alpha}{2}} {1 + e^\\frac{\\alpha}{2}}$$ $$[Z]_j = 1 - x_j \\text{ with probability } \\frac{1}{1 + e^\\frac{\\alpha}{2}}$$\nSuch a mechanism achieves α-local-differentially privacy because one can show that :\n$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} = e^\\frac{\\alpha}{2}(||z - x||_1 - ||z - x\u0026rsquo;||_1) \\in [e^{-\\alpha}, e^\\alpha]$$ which is the criteria given above.\nWith the notation as $1_d=[1, 1, 1, \u0026hellip;, 1]$ corresponds to a d-vector with each coordinate equals 1, we can also show that :\n$$E[Z | x] = \\frac{e^\\frac{\\alpha}{2} - 1}{e^\\frac{\\alpha}{2} + 1} * x + \\frac{1}{1 + e^\\frac{\\alpha}{2}}1_d$$\nThis leads to the natural moment-estimator :\n$$θ_{hat} = \\frac{1}{n} ∑_{i=1}^{n} \\frac{Z_i - 1_d}{1 + e^\\frac{\\alpha}{2}} * \\frac{e^\\frac{\\alpha}{2} + 1}{e^\\frac{\\alpha}{2} - 1}$$\nOne can also show that it verifies :\n$$E[ ||θ_{hat}- θ||_2] ≤ \\frac{d}{n} * \\frac{(e^\\frac{\\alpha}{2} + 1)^2}{(e^\\frac{\\alpha}{2} - 1)^2} \u0026lt; \\frac{C_3}{nα^2}$$ which is the announced result.\nLaplace Noise (beyond paper) Instead of saying the truth with some probability, one may think of adding noise to the answer so that the statistician can\u0026rsquo;t retrieve his real answer. This is exactly the mechanism we propose to dive in and which is not covered in the paper.\nDefinition: A noise is said to be a Laplace noise with parameters (μ, b) if it verifies:\n$$f(x|μ, b) = \\frac{1}{2b} * exp(\\frac{-|x - μ|}{b})$$\nA visualisation for differents parameters is given below. We can see that Laplace distribution is a shaper verson of the gaussian distribution : The trick is to use such a noise. Let assume $X_i \\in [-M,M]$ and construct the private mechanism as follow:\n$$Z_i = X_i + \\sigma W_i$$ where $W_i$ is drawn from a Laplace noise (0,1).\nOne can show that :\n$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} \\leq e^{\\frac{1}{\\sigma} * |x - x\u0026rsquo;|} \\leq e^{\\frac{2M}{\\sigma}}$$\nThus, with the choice of $\\sigma = \\frac{2M}{\\alpha}$, it verifies α-local-differentially privacy. The proposed estimator is the following :\n$$\\hat{Z} = \\bar{X} + \\frac{2M}{\\alpha} \\bar{W}$$\nOne can show that it is an unbiaised estimator that achieves the optimal rates:\n$$E[\\hat{Z}] = E[X]$$\n$$V[\\hat{Z}] = \\frac{V(X)}{n} + \\frac{4M^2}{n\\alpha^2} V[\\bar{W}] = \\frac{V(X)}{n} + \\frac{8M^2}{n\\alpha^2}$$ $$E[ |\\hat{Z}- X|^2] \\leq \\frac{C_3}{n\\alpha^2}.$$\nThis is exactly the optimal rates, quite outstanding !\nThe case of density estimation One accurate question that can raise is : what about others distribution ? Is privacy more costly in general cases ? What is the trade-off ?\nTo answer this question, let\u0026rsquo;s precise the problem.\nWe want to estimate in a non-parametric way a 1D-density function f belonging to one of theses classes :\n-Hölder Class (β, L): $\\text{For all }x, y \\in \\mathbb{R} \\text{ and } m \\leq \\beta, \\quad \\left| f^{(m)}(x) - f^{(m)}(y) \\right| \\leq L \\left| x - y \\right|^{\\beta - m}$\n-Sobolev Class: $F_{\\beta}[C] := \\left( f \\in L^2([0, 1]) , \\middle| , f = \\sum_{j=1}^{\\infty} \\theta_j \\phi_j \\text{ such that } \\sum_{j=1}^{\\infty} j^{2\\beta} \\phi_j^2 \\leq C^2 \\right)$\nIn a intuitition way, those two classes express that f is smooth enough to admits Lipschitz constant to its derivative so that it doesn\u0026rsquo;t \u0026ldquo;vary\u0026rdquo; locally too much.\nTheorem Without privacy One can show that without privacy, the minimax rate achievable for estimating a Hölder Class function is:\n$$\\text{MSE}(\\hat{f} - f) \\leq C_1 \\cdot n^{-\\frac{2\\beta}{1+2\\beta}}$$ with the estimator\n$$\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h} K\\left(\\frac{x - X_i}{h}\\right) \\text{with } h = C_2 \\cdot n^{-\\frac{1}{2\\beta+1}}$$\nIn the case of d-multidimensionnal density f, the optimal rate is :\n$$\\text{MSE}(\\hat{f} - f) \\leq C_4 \\cdot n^{-\\frac{2\\beta}{d+ 2\\beta}}$$ with the estimator\n$$\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h^d} K^d\\left(\\frac{x-X_i}{h}\\right) \\quad \\text{with} \\quad h = C_5 \\cdot n^{-\\frac{1}{2\\beta + d}}$$\nThis illustrates once again the curse of dimensionnality.\nWith privacy Let assume that f bellongs to one of the two classes with β as smoothness parameter.\nThen, the optimal α-local-differentially private optimal rate is :\n$$\\text{MSE}(\\hat{f} - f) \\leq C_1 \\cdot (n\\alpha^2)^{-\\frac{2\\beta}{2\\beta+2}}.$$\nOne may observe two pessimistic news:\n-The rate is affected by a factor of $\\alpha^2$ as for the multinomial estimation\n-More damageable: the rate is slower in term of n unlike the previous problem which make privacy in this case more costly.\nPractical strategies Eventhough this rate is pessimistic and proves that privacy comes at a cost, it remains to illustrates how can we achieves this best but not great rate. For this end, once again, two strategies are possible.\nRandomized responses Laplace Noise (beyond paper) Randomized responses This is the strategy illustrated in the paper and consists of sampling for each coordinate according the realisation of a Bernouilli variable with the correct probability as function of α. As it is not the most comprehensive and straightforward method, we prefer to dive in depth into the second one; uncovered in the paper.\nLaplace Noise (beyond paper) Let assume that $X_i \\in [0,M]$ almost surely. We note $G_j = [\\frac{j-1}{K},\\quad \\frac{j}{K}]$ the bin of length $\\frac{1}{K}$.\nWe consider the histogramm estimator: $$\\hat{f}(x) = \\frac{K}{n} \\sum_{j=1}^{K} \\sum_{i=1}^{n} 1_{X_i \\in G_j} \\cdot 1_{x \\in G_j}.$$\nWe now construct the private mechanism as follow:\n$$Z_i = \\left[1_{X_i \\in G_1} + \\frac{2}{\\alpha} W_1, \\ldots, 1_{X_i \\in G_K} + \\frac{2}{\\alpha} W_K\\right]$$\nIn an intuitive way, we add a Laplace noise realisation for each bin.\nThis guarantees α-local-differentially privacy as : $$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} \\leq \\exp\\left(\\frac{\\alpha}{2} \\sum_{j=1}^{K} |1_{x \\in G_j} - 1_{x\u0026rsquo; \\in G_j}| \\right) \\leq \\exp\\left(\\frac{\\alpha}{2} \\cdot 2\\right).$$\nThis leads to the α-local-differentially private estimator :\n$$f_{\\text{private_estimate}} = \\hat{f} + \\frac{2K}{n\\alpha} \\sum_{j=1}^{K} W_j$$\nThe biais is the same as the unprivate case as :\n$$E[f_{\\text{private_estimate}}] = E[\\hat{f}] + 0 .$$\nOne may prove that if f bellongs to the β-Hölder Class:\n$$Biais(f_{\\text{private_estimate}}, f) \\leq C_1 * K^{-\\beta}$$\nMeanwhile, $$V[f_{\\text{private_estimate}}] \\leq \\frac{C_2}{n} + \\frac{4K^2}{\\alpha^2} \\frac{V[W]}{n}$$, such that in total :\n$$\\text{MSE}(f_{\\text{private_estimate}} - f) \\leq C_1 K^{-2\\beta} + \\frac{C_2}{n} + \\frac{C_3 K^2}{n\\alpha^2}.$$ Minimizing over K (hyperparameters) leads to : $K = C_4 \\cdot (n\\alpha^2)^{-\\frac{1}{2\\beta+2}}$ and thus to:\n$$\\text{MSE}(f_{\\text{private_estimate}} - f) \\leq C_5 \\cdot (n\\alpha^2)^{-\\frac{2\\beta}{2\\beta + 2}}$$, which is the expected bound.\nExperiment: Illustration of the Minimax privacy rate Overview The aim of this section is to provide illustrations of the theoretical results set out above. Emphasis is placed on convergence results, with empirical confirmation of the latter.\nFor the sake of reproducibility and transparency, the source code can be found in the notebook at this: Github link.\nMethodology Data Preparation: Rather than working with real datasets, we decide to work with simulated data, as this allows us to maintain control over all aspects. More precisely, we give ourselves $n=1000$ samples of the normal distribution $N(100,1)$ on which we add a Laplace noise $L(0,\\alpha).$\nAs for the different alpha values, we iterate through them: $[0.2, 0.3, 0.5, 0.7]$\nPrivacy Metric Calculation: We will look at the use case of estimating the mean of a distribution.\nEvaluation: The results will be compared in terms of Mean Square Error (MSE).\nResults In terms of the observed distribution (private because subject to Laplace noise) relative to the true data, we obtain the following figure:\nAs expected, the greater the desired privacy (low $\\alpha$), the more spread out the distribution of observed data.\nWhen it comes to estimating the true average from private data, we obtain the following figure:\nThis figure illustrates two major points:\n-The first is that whatever the level of privacy, we have an unbiased estimator of the mean. It\u0026rsquo;s a beautiful property, empirically verified !\n-The second is that, unfortunately, the greater the privacy (low alpha), the greater the variance of this estimator.\nWe recall our main theorem demonstrated above Previous theorem :\nTheorem : Given α-local-differentially private $Z_i$, there exists some arbitrary constants $C_1$, $C_2$ such that for all $\\alpha\\in [0,1]$: $$C_1 min(1, \\frac{1}{\\sqrt{n\\alpha^2}}, \\frac{d}{n\\alpha^2}) ≤ E[|θ_{hat} - θ|^2] ≤ C_2 min(1, \\frac{d}{n\\alpha^2})$$\nWe now want to compare the theoretical optimal rate with empirical results. To do this, we distinguish two situations:\n-The first is with fixed alpha, and determines the MSE as a function of the number of samples n. This leads to these empirical results:\nThe dotted line represents the regime of the theoretical bound of the form $n \\rightarrow \\frac{C1}{n}$ . This is the shape of the empirical curves!\n-The second has a fixed n and determines the MSE as a function of alpha. This leads to these empirical results:\nThe dotted line represents the regime of the theoretical bound of the form $\\alpha \\rightarrow \\frac{C1}{\\alpha^2}$ . This is once again the shape of the empirical curves quite surprisingly!\nConclusion From a problem rooted in an ethical dilemma (privacy versus completeness and transparency), we have looked at the cost of guaranteeing one at the expense of the other, to better sketch out desirable situations.\nThis has enabled us to develop theoretical results in terms of minimax rates. There is indeed a trade-off between these criteria, which is even more costly in the case of non-parametric density estimation.\nFinally, we have compared these theoretical limits with empirical results, which confirm the conformity of the statements.\nThe aim of all this work is to disseminate this important yet under-exploited notion: privacy. To this end, we invite the reader to take the following quiz to ensure his or her understanding.\nQuizz To test yourself abour privacy:\nWhat is privacy?\nAvoid asking questions that can raise private information\rA mechanism that prevents other agent to retrieve personnal information in your answer\rAn ethical-washing trend\rWhich situation is α-local-differentially privacy?\nsup {Q(Z | Xi = x)/Q(Z | Xi = x')} | x, x' ∈ X} \u003e= exp(α)\rYou tell the truth half the time, you lie otherwise.\rZ_i = X_i + (2M/α) W_i with W_i drawn from a Laplace Noise(0,1)\rWhat is the privacy cost in term of optimal rate ?\nMultinomial estimation: A factor α^2/d\rDensity estimation: from n^(-2β/2β+2) (without privacy) to (nα^2)^(-2β/(2β+2))\rWe loose nothing, that's the surprising finding of the paper\rSubmit\rAnnexes References Warner SL. Randomized response: a survey technique for eliminating evasive answer bias. J Am Stat Assoc. 1965 Mar;60(309):63-6. PMID: 12261830. John C. Duchi, Michael I. Jordan, and Martin Wainwright. Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation. Advances in Neural Information Processing Systems (2013) Dwork, C., \u0026amp; Roth, A. (2014). The algorithmic foundations of differential privacy. Foundations and Trends® in Theoretical Computer Science, 9(3-4), 211-407. Narayanan, A., \u0026amp; Shmatikov, V. (2008). Robust de-anonymization of large sparse datasets. In Security and Privacy, 2008. SP 2008. IEEE Symposium on (pp. 111-125). IEEE. ",
      "content_html": "\u003ch1 style=\"font-size: 36px;\"\u003eEstimating Privacy in Data Science: A Comprehensive Guide\u003c/h1\u003e\r\n\u003ch1 style=\"font-size: 24px;\"\u003eAuthor: Antoine Klein \u003ca href=\"https://github.com/AntoineTSP\"\u003eGithub Link\u003c/a\u003e\u003c/h1\u003e\r\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eIncentives\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eDefinition\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eTheory\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eThe case of multinomial estimation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eThe case of density estimation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eExperiment\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-7\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-8\"\u003eQuizz\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0\"\u003eWhy do we care about privacy ?\u003c/h2\u003e\n\u003cp\u003eImagine, you\u0026rsquo;re quietly at home when the doorbell rings. You open the door and a government official appears: population census. Even though he shows you his official badge and you\u0026rsquo;d like to help him in the public interest, you find it hard to answer his questions as you go along. Indeed, the first questions about the date of your move are easy and public. On the other hand, when he asks about the number of children, marital status or your salary and what you do with it, you \u003cem\u003estruggle\u003c/em\u003e. Not because you don\u0026rsquo;t know the answer, but because you\u0026rsquo;re faced with an \u003cstrong\u003eethical dilemma\u003c/strong\u003e: transparency towards the state versus protection of personal data.\u003cbr\u003e\n$$\\text{In short, transparency goes against your privacy. }$$\u003c/p\u003e\n\u003cp\u003eThis stress has major consequences: as you doubt what could happen to you with this data, but you still want to answer it, you \u003cstrong\u003eunderestimate\u003c/strong\u003e your answers. On a wider scale, this leads to a \u003cstrong\u003esuffrage bias\u003c/strong\u003e and therefore a lack of knowledge of the real situation of your population. Warner [1], the first to tackle this problem from a statistical angle talks of an evasive bias and says:\u003cbr\u003e\n\u003cstrong\u003e\u0026ldquo;for reasons of modesty, fear of being thought bigoted, or merely a reluctance to confide secrets to strangers, respondents to surveys might prefer to be able to answer certain questions non-truthfully, or at least without the interviewer knowing their true response\u0026rdquo;\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis situation presented a trusted agent, in that he wasn\u0026rsquo;t trying to harm you directly. Now imagine that you agree to give him your personal data, but that on the way home, this agent of the state is mugged and someone steals his documents. Not only is this an attack on his person, it\u0026rsquo;s also an attack on yours: as the guarantor of your data, it\u0026rsquo;s now at the mercy of the attacker. The problem here is \u003cstrong\u003enot to have protected yourself against a malicious agent\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eAdmittedly, these situations are rare, but with the densification of data, their analogies are omnipresent: cookies on the Internet, cyber-attacks, datacenter crashes\u0026hellip;One area for improvement is quite simply to better \u003cstrong\u003ecertify usage\u003c/strong\u003e by means of cyber protection labels and leads to such a norm to achieve trust:\n\u003cimg\r\n  src=\"/images/Antoine_Klein/Umbrella.png\"\r\n  alt=\"Data Privacy2\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003eIn this blog, we propose to tackle this problem from a completely different angle: \u003cstrong\u003ehow to both enable the agent to take global measures and prevent it and any subsequent malicious agents from being able to re-identify my personal data\u003c/strong\u003e. We\u0026rsquo;ll also use minimax bounds to answer the question: \u003cstrong\u003efor a given privacy criterion, what\u0026rsquo;s the loss in terms of estimation?\u003c/strong\u003e (fundamental trade-offs between privacy and convergence rate)\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eScientific introduction\u003c/h2\u003e\n\u003cp\u003eOur blog will follow the same plan as the article that inspired it (John C. Duchi [2]),i.e. to show that \u003cstrong\u003eresponse randomization achieves optimal convergence\u003c/strong\u003e in the case of multinomial estimation, and then that this process can be generalized to any \u003cem\u003enonparametric distribution estimation\u003c/em\u003e. To this end, we will introduce the notion of \u003cstrong\u003elocal differential privacy\u003c/strong\u003e as well as the \u003cstrong\u003eminimax theory\u003c/strong\u003e for obtaining optimal limits. All this will shed light on the \u003cstrong\u003etrade-off between privacy and estimation rates\u003c/strong\u003e. We will also explain algorithms to implement these optimal strategies. Finally, we will propose some experimental results.\u003c/p\u003e\n\u003ch2 id=\"section-2\"\u003eSome key definitions\u003c/h2\u003e\n\u003cp\u003eLet assume that you want to make private $X_1 , \u0026hellip; , X_n \\in X$ random variable and, as the statistician, you only observe $Z_1, . . . , Z_n ∈ Z$. The paper assumes that there exist a \u003cstrong\u003emarkov kernel\u003c/strong\u003e that links the true ramdom variables and the observed ones as follow: $Q_i(Z_i | X_i = x)$.\u003c/p\u003e\n\u003cp\u003eThe privacy mechanism is to be said \u003cstrong\u003enon interactive\u003c/strong\u003e if each $Z_i$ is obtained only conditionnaly on $X_i$ (and not on the others). This represents the fact that the privacy mechanism is \u003cstrong\u003ememory less\u003c/strong\u003e. If not, the mechnism is said to be interactive.\u003c/p\u003e\n\u003cp\u003eIn the following, we will work only with non-interactive privacy mechanism but in the conlusion we will claim that newer studies showed that it is not enough for some larger problems.\u003c/p\u003e\n\u003cp\u003e$Z_i$ is said to be \u003cstrong\u003eα-local-differentially private\u003c/strong\u003e for the original data $X_i$ if $$sup(\\frac{Q(Z | X_i = x)}{Q(Z | X_i = x\u0026rsquo;)} | x, x\u0026rsquo; ∈ X) ≤ exp(α)$$.\u003c/p\u003e\n\u003cp\u003eAn intuitive way of understanding this definition is to see that the smaller α is (the more private it is), the more \u003cstrong\u003edifficult it is to distinguish\u003c/strong\u003e the distribution of Z conditional on two different X data.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003eTheoretical results\u003c/h2\u003e\n\u003ch3 id=\"section-4\"\u003eThe case of multinomial estimation\u003c/h3\u003e\n\u003cp\u003eIn this section, we return back to the problem of the private survey. For the statistician view, estimating a survey is estimating the parameter θ from the Bernouilli distribution $B(θ)$.\nThis problem is a special case of multinomial estimation, where \u003ccode\u003eθ\u003c/code\u003e is now a multidimensional parameter that is amenable to simplex probability. $∆\u003cem\u003ed := (θ ∈ ℝ\u003c/em\u003e+ |∑θ_j = 1)$.\u003c/p\u003e\n\u003cp\u003e\u003ca name=\"Recall\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTheorem :\u003c/strong\u003e Given α-local-differentially private $Z_i$, there exists some arbitrary constants $C_1$, $C_2$ such that for all $\\alpha\\in [0,1]$:\n$$C_1 min(1, \\frac{1}{\\sqrt{n\\alpha^2}}, \\frac{d}{n\\alpha^2}) ≤ E[|θ_{hat} - θ|^2] ≤ C_2 min(1, \\frac{d}{n\\alpha^2})$$ and\n$$C_1 min(1,\\frac{1}{\\sqrt{n\\alpha^2}}) ≤ E[||θ_{hat} - θ||_1] ≤ C_2 min(1,\\frac{d}{\\sqrt{n\\alpha^2}})$$.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRecall from standard statistics:\u003c/strong\u003e For non private independant $Z_i$ with finite variance, there exists some arbitrary constants $C_3$ such that:\n$$E[|θ_{hat} - θ|^2] ≤ \\frac{C_3}{n}$$\u003c/p\u003e\n\u003cp\u003eIn others term, providing α-local-differentially privacy \u003cstrong\u003ecauses a reduction\u003c/strong\u003e in the effective sample size of a factor $\\frac{\\alpha^2}{d}$ for best situations. It thus means that the \u003cstrong\u003easymptotically rate of convergences remains unchanged\u003c/strong\u003e which is a really good news !\u003c/p\u003e\n\u003ch4 id=\"practical-strategies\"\u003ePractical strategies\u003c/h4\u003e\n\u003cp\u003eThe paper deals with one of the 2 standard methods to implement such a strategy that obtains the minimax rates:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-10\"\u003eRandomized responses\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-11\"\u003eLaplace Noise (beyond paper)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"section-10\"\u003eRandomized responses\u003c/h5\u003e\n\u003cp\u003eThe \u003cem\u003eintuition\u003c/em\u003e of this section is the following : \u003cstrong\u003eto not allow the statistician to retrieve your personnal data\u003c/strong\u003e in case of Bernouilli distribution, you toss a coin. If it is heads, you say to him your reel answer, if it is tails, you say the opposite. In his point of view, as he doesn\u0026rsquo;t know what was the result of the coin, \u003cstrong\u003ehe can\u0026rsquo;t distinguish\u003c/strong\u003e if you tell the true or not but in a large scale, he knows that he will have half correct answer, half lies so that he can retrieve information.\u003c/p\u003e\n\u003cp\u003eFor the multinomial estimation now, you will generalize this procedure to the multidimensionnal setting. For each coordinate, you will tell to the statistician the reel answer with a certain probability and lies otherwise. More precisely, its leads to :\u003c/p\u003e\n\u003cp\u003e$$[Z]_j = x_j \\text{ with probability } \\frac{e^\\frac{\\alpha}{2}} {1 + e^\\frac{\\alpha}{2}}$$\n$$[Z]_j = 1 - x_j \\text{ with probability } \\frac{1}{1 + e^\\frac{\\alpha}{2}}$$\u003c/p\u003e\n\u003cp\u003eSuch a mechanism achieves \u003cem\u003eα-local-differentially privacy\u003c/em\u003e because one can show that :\u003c/p\u003e\n\u003cp\u003e$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} = e^\\frac{\\alpha}{2}(||z - x||_1 - ||z - x\u0026rsquo;||_1) \\in [e^{-\\alpha}, e^\\alpha]$$ which is the criteria given above.\u003c/p\u003e\n\u003cp\u003eWith the notation as $1_d=[1, 1, 1, \u0026hellip;, 1]$ corresponds to a d-vector with each coordinate equals 1, we can also show that :\u003c/p\u003e\n\u003cp\u003e$$E[Z | x] = \\frac{e^\\frac{\\alpha}{2} - 1}{e^\\frac{\\alpha}{2} + 1} * x + \\frac{1}{1 + e^\\frac{\\alpha}{2}}1_d$$\u003c/p\u003e\n\u003cp\u003eThis leads to the natural moment-estimator :\u003c/p\u003e\n\u003cp\u003e$$θ_{hat} = \\frac{1}{n} ∑_{i=1}^{n} \\frac{Z_i - 1_d}{1 + e^\\frac{\\alpha}{2}} * \\frac{e^\\frac{\\alpha}{2} + 1}{e^\\frac{\\alpha}{2} - 1}$$\u003c/p\u003e\n\u003cp\u003eOne can also show that it verifies :\u003c/p\u003e\n\u003cp\u003e$$E[ ||θ_{hat}- θ||_2] ≤  \\frac{d}{n} * \\frac{(e^\\frac{\\alpha}{2} + 1)^2}{(e^\\frac{\\alpha}{2} - 1)^2} \u0026lt; \\frac{C_3}{nα^2}$$ which is the announced result.\u003c/p\u003e\n\u003ch5 id=\"section-11\"\u003eLaplace Noise (beyond paper)\u003c/h5\u003e\n\u003cp\u003eInstead of saying the truth with some probability, one may think of \u003cstrong\u003eadding noise\u003c/strong\u003e to the answer so that the statistician can\u0026rsquo;t retrieve his real answer. This is exactly the mechanism we propose to dive in and which is \u003cstrong\u003enot covered in the paper\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDefinition:\u003c/strong\u003e A noise is said to be a Laplace noise with parameters (μ, b) if it verifies:\u003cbr\u003e\n$$f(x|μ, b) = \\frac{1}{2b} * exp(\\frac{-|x - μ|}{b})$$\u003c/p\u003e\n\u003cp\u003eA visualisation for differents parameters is given below. We can see that Laplace distribution is a \u003cstrong\u003eshaper verson of the gaussian distribution\u003c/strong\u003e :\n\u003cimg\r\n  src=\"/images/Antoine_Klein/Laplace.png\"\r\n  alt=\"Laplace\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003eThe trick is to use such a noise. Let assume $X_i \\in [-M,M]$ and construct the private mechanism as follow:\u003cbr\u003e\n$$Z_i = X_i + \\sigma W_i$$ where $W_i$ is drawn from a Laplace noise (0,1).\u003c/p\u003e\n\u003cp\u003eOne can show that :\u003c/p\u003e\n\u003cp\u003e$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} \\leq e^{\\frac{1}{\\sigma} * |x - x\u0026rsquo;|} \\leq e^{\\frac{2M}{\\sigma}}$$\u003c/p\u003e\n\u003cp\u003eThus, with the choice of $\\sigma = \\frac{2M}{\\alpha}$, \u003cstrong\u003eit verifies α-local-differentially privacy\u003c/strong\u003e. The proposed estimator is the following :\u003cbr\u003e\n$$\\hat{Z} = \\bar{X} + \\frac{2M}{\\alpha} \\bar{W}$$\u003c/p\u003e\n\u003cp\u003eOne can show that it is an unbiaised estimator that achieves the optimal rates:\u003cbr\u003e\n$$E[\\hat{Z}] = E[X]$$\u003cbr\u003e\n$$V[\\hat{Z}] = \\frac{V(X)}{n} + \\frac{4M^2}{n\\alpha^2} V[\\bar{W}] = \\frac{V(X)}{n} + \\frac{8M^2}{n\\alpha^2}$$\n$$E[ |\\hat{Z}- X|^2] \\leq \\frac{C_3}{n\\alpha^2}.$$\u003c/p\u003e\n\u003cp\u003eThis is \u003cstrong\u003eexactly the optimal rates\u003c/strong\u003e, quite outstanding !\u003c/p\u003e\n\u003ch3 id=\"section-5\"\u003eThe case of density estimation\u003c/h3\u003e\n\u003cp\u003eOne accurate question that can raise is : \u003cstrong\u003ewhat about others distribution ?\u003c/strong\u003e Is privacy more costly in general cases ? What is the trade-off ?\u003c/p\u003e\n\u003cp\u003eTo answer this question, let\u0026rsquo;s precise the problem.\u003c/p\u003e\n\u003cp\u003eWe want to estimate in a non-parametric way a 1D-density function \u003ccode\u003ef\u003c/code\u003e belonging to one of theses classes :\u003cbr\u003e\n-\u003cstrong\u003eHölder Class (β, L):\u003c/strong\u003e $\\text{For all }x, y \\in \\mathbb{R} \\text{ and } m \\leq \\beta, \\quad \\left| f^{(m)}(x) - f^{(m)}(y) \\right| \\leq L \\left| x - y \\right|^{\\beta - m}$\u003cbr\u003e\n-\u003cstrong\u003eSobolev Class:\u003c/strong\u003e $F_{\\beta}[C] := \\left( f \\in L^2([0, 1]) , \\middle| , f = \\sum_{j=1}^{\\infty} \\theta_j \\phi_j \\text{ such that } \\sum_{j=1}^{\\infty} j^{2\\beta} \\phi_j^2 \\leq C^2 \\right)$\u003c/p\u003e\n\u003cp\u003eIn a intuitition way, those two classes express that \u003ccode\u003ef\u003c/code\u003e is \u003cstrong\u003esmooth enough\u003c/strong\u003e to admits Lipschitz constant to its derivative so that it doesn\u0026rsquo;t \u0026ldquo;vary\u0026rdquo; locally too much.\u003c/p\u003e\n\u003ch4 id=\"theorem\"\u003eTheorem\u003c/h4\u003e\n\u003ch5 id=\"without-privacy\"\u003eWithout privacy\u003c/h5\u003e\n\u003cp\u003eOne can show that without privacy, the minimax rate achievable for estimating a Hölder Class function is:\u003cbr\u003e\n$$\\text{MSE}(\\hat{f} - f) \\leq C_1 \\cdot n^{-\\frac{2\\beta}{1+2\\beta}}$$ with the estimator\u003cbr\u003e\n$$\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h} K\\left(\\frac{x - X_i}{h}\\right) \\text{with } h = C_2 \\cdot n^{-\\frac{1}{2\\beta+1}}$$\u003c/p\u003e\n\u003cp\u003eIn the case of d-multidimensionnal density \u003ccode\u003ef\u003c/code\u003e, the optimal rate is :\u003cbr\u003e\n$$\\text{MSE}(\\hat{f} - f) \\leq C_4 \\cdot n^{-\\frac{2\\beta}{d+ 2\\beta}}$$ with the estimator\u003cbr\u003e\n$$\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h^d} K^d\\left(\\frac{x-X_i}{h}\\right) \\quad \\text{with} \\quad h = C_5 \\cdot n^{-\\frac{1}{2\\beta + d}}$$\u003c/p\u003e\n\u003cp\u003eThis illustrates once again the \u003cstrong\u003ecurse of dimensionnality\u003c/strong\u003e.\u003c/p\u003e\n\u003ch5 id=\"with-privacy\"\u003eWith privacy\u003c/h5\u003e\n\u003cp\u003eLet assume that \u003ccode\u003ef\u003c/code\u003e bellongs to one of the two classes with  \u003ccode\u003eβ\u003c/code\u003e as smoothness parameter.\u003cbr\u003e\nThen, the optimal α-local-differentially private optimal rate is :\u003cbr\u003e\n$$\\text{MSE}(\\hat{f} - f) \\leq C_1 \\cdot (n\\alpha^2)^{-\\frac{2\\beta}{2\\beta+2}}.$$\u003c/p\u003e\n\u003cp\u003eOne may observe \u003cstrong\u003etwo pessimistic news\u003c/strong\u003e:\u003cbr\u003e\n-The rate is \u003cstrong\u003eaffected by a factor\u003c/strong\u003e of $\\alpha^2$ as for the multinomial estimation\u003cbr\u003e\n-More damageable: the \u003cstrong\u003erate is slower\u003c/strong\u003e in term of \u003ccode\u003en\u003c/code\u003e unlike the previous problem which make privacy in this case \u003cstrong\u003emore costly\u003c/strong\u003e.\u003c/p\u003e\n\u003ch5 id=\"practical-strategies-1\"\u003ePractical strategies\u003c/h5\u003e\n\u003cp\u003eEventhough this rate is pessimistic and proves that \u003cstrong\u003eprivacy comes at a cost\u003c/strong\u003e, it remains to illustrates how can we achieves this best but not great rate.\nFor this end, once again, two strategies are possible.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-12\"\u003eRandomized responses\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-13\"\u003eLaplace Noise (beyond paper)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"section-12\"\u003eRandomized responses\u003c/h5\u003e\n\u003cp\u003eThis is the strategy illustrated in the paper and consists of sampling for each coordinate according the realisation of a Bernouilli variable with the correct probability as function of \u003ccode\u003eα\u003c/code\u003e.\nAs it is not the most comprehensive and straightforward method, \u003cstrong\u003ewe prefer to dive in depth into the second one; uncovered in the paper\u003c/strong\u003e.\u003c/p\u003e\n\u003ch5 id=\"section-13\"\u003eLaplace Noise (beyond paper)\u003c/h5\u003e\n\u003cp\u003eLet assume that $X_i \\in [0,M]$ almost surely. We note $G_j = [\\frac{j-1}{K},\\quad \\frac{j}{K}]$ the bin of length $\\frac{1}{K}$.\u003c/p\u003e\n\u003cp\u003eWe consider the histogramm estimator:\n$$\\hat{f}(x) = \\frac{K}{n} \\sum_{j=1}^{K} \\sum_{i=1}^{n} 1_{X_i \\in G_j} \\cdot 1_{x \\in G_j}.$$\u003c/p\u003e\n\u003cp\u003eWe now construct the private mechanism as follow:\u003cbr\u003e\n$$Z_i = \\left[1_{X_i \\in G_1} + \\frac{2}{\\alpha} W_1, \\ldots, 1_{X_i \\in G_K} + \\frac{2}{\\alpha} W_K\\right]$$\u003c/p\u003e\n\u003cp\u003eIn an intuitive way, we add a Laplace noise realisation for each bin.\u003c/p\u003e\n\u003cp\u003eThis guarantees α-local-differentially privacy as :\n$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} \\leq \\exp\\left(\\frac{\\alpha}{2} \\sum_{j=1}^{K} |1_{x \\in G_j} - 1_{x\u0026rsquo; \\in G_j}| \\right) \\leq \\exp\\left(\\frac{\\alpha}{2} \\cdot 2\\right).$$\u003c/p\u003e\n\u003cp\u003eThis leads to the α-local-differentially private estimator :\u003cbr\u003e\n$$f_{\\text{private_estimate}} = \\hat{f} + \\frac{2K}{n\\alpha} \\sum_{j=1}^{K} W_j$$\u003c/p\u003e\n\u003cp\u003eThe biais is the same as the unprivate case as :\u003cbr\u003e\n$$E[f_{\\text{private_estimate}}] = E[\\hat{f}] + 0 .$$\u003c/p\u003e\n\u003cp\u003eOne may prove that if f bellongs to the β-Hölder Class:\u003cbr\u003e\n$$Biais(f_{\\text{private_estimate}}, f) \\leq C_1 * K^{-\\beta}$$\u003c/p\u003e\n\u003cp\u003eMeanwhile, $$V[f_{\\text{private_estimate}}] \\leq \\frac{C_2}{n} + \\frac{4K^2}{\\alpha^2} \\frac{V[W]}{n}$$, such that in total  :\u003cbr\u003e\n$$\\text{MSE}(f_{\\text{private_estimate}} - f) \\leq C_1 K^{-2\\beta} + \\frac{C_2}{n} + \\frac{C_3 K^2}{n\\alpha^2}.$$\nMinimizing over K (hyperparameters) leads to :  $K = C_4 \\cdot (n\\alpha^2)^{-\\frac{1}{2\\beta+2}}$ and thus to:\u003cbr\u003e\n$$\\text{MSE}(f_{\\text{private_estimate}} - f) \\leq C_5 \\cdot (n\\alpha^2)^{-\\frac{2\\beta}{2\\beta + 2}}$$, which is the expected bound.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"section-6\"\u003eExperiment: Illustration of the Minimax privacy rate\u003c/h2\u003e\n\u003ch3 id=\"section-111\"\u003eOverview\u003c/h3\u003e\n\u003cp\u003eThe aim of this section is to \u003cstrong\u003eprovide illustrations of the theoretical results\u003c/strong\u003e set out above. Emphasis is placed on convergence results, with empirical confirmation of the latter.\u003c/p\u003e\n\u003cp\u003eFor the sake of \u003cstrong\u003ereproducibility and transparency\u003c/strong\u003e, the source code can be found in the notebook at this: \u003ca href=\"https://github.com/AntoineTSP/responsible-ai-datascience-ipParis.github.io.git\"\u003eGithub link\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"methodology\"\u003eMethodology\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Preparation\u003c/strong\u003e: Rather than working with real datasets, we decide to work with simulated data, as this allows us to maintain control over all aspects.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eMore precisely, we give ourselves $n=1000$ samples of the normal distribution $N(100,1)$ on which we add a Laplace noise $L(0,\\alpha).$\u003cbr\u003e\nAs for the different alpha values, we iterate through them: $[0.2, 0.3, 0.5, 0.7]$\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePrivacy Metric Calculation\u003c/strong\u003e: We will look at the use case of estimating the mean of a distribution.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEvaluation\u003c/strong\u003e: The results will be compared in terms of Mean Square Error (MSE).\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"results\"\u003eResults\u003c/h3\u003e\n\u003cp\u003eIn terms of the observed distribution (private because subject to Laplace noise) relative to the true data, we obtain the following figure:\u003c/p\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"/images/Antoine_Klein/Private_distribution.png\"\r\n  alt=\"Data Privacy2\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003eAs expected, the greater the desired privacy (low $\\alpha$), \u003cstrong\u003ethe more spread out\u003c/strong\u003e the distribution of observed data.\u003c/p\u003e\n\u003cp\u003eWhen it comes to estimating the true average from private data, we obtain the following figure:\u003c/p\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"/images/Antoine_Klein/Estimated_mean.png\"\r\n  alt=\"Data Privacy2\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003eThis figure illustrates two major points:\u003cbr\u003e\n-The first is that whatever the level of privacy, we have an \u003cstrong\u003eunbiased estimator\u003c/strong\u003e of the mean. It\u0026rsquo;s a beautiful property, empirically verified !\u003cbr\u003e\n-The second is that, unfortunately, the greater the privacy (low alpha), \u003cstrong\u003ethe greater the variance\u003c/strong\u003e of this estimator.\u003c/p\u003e\n\u003cp\u003eWe recall our main theorem demonstrated above \u003ca href=\"#Recall\" style=\"background-color: yellow; padding: 2px 5px; border-radius: 3px;\"\u003ePrevious theorem\u003c/a\u003e :\u003cbr\u003e\n\u003cstrong\u003eTheorem\u003c/strong\u003e : Given α-local-differentially private $Z_i$, there exists some arbitrary constants $C_1$, $C_2$ such that for all $\\alpha\\in [0,1]$:\n$$C_1 min(1, \\frac{1}{\\sqrt{n\\alpha^2}}, \\frac{d}{n\\alpha^2}) ≤ E[|θ_{hat} - θ|^2] ≤ C_2 min(1, \\frac{d}{n\\alpha^2})$$\u003c/p\u003e\n\u003cp\u003eWe now want to \u003cstrong\u003ecompare the theoretical optimal rate with empirical results\u003c/strong\u003e. To do this, we distinguish two situations:\u003cbr\u003e\n-The first is with \u003cstrong\u003efixed alpha\u003c/strong\u003e, and determines the MSE as a function of the number of samples n. This leads to these empirical results:\u003c/p\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"/images/Antoine_Klein/Minimax_rate_n.png\"\r\n  alt=\"Data Privacy2\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003eThe dotted line represents the regime of the theoretical bound of the form $n \\rightarrow \\frac{C1}{n}$ . This is the shape of the empirical curves!\u003c/p\u003e\n\u003cp\u003e-The second has a \u003cstrong\u003efixed n\u003c/strong\u003e and determines the MSE as a function of alpha. This leads to these empirical results:\u003c/p\u003e\n\u003cp\u003e\u003cimg\r\n  src=\"/images/Antoine_Klein/Minimax_rate_alpha.png\"\r\n  alt=\"Data Privacy2\"\r\n  loading=\"lazy\"\r\n  decoding=\"async\"\r\n  class=\"full-width\"\r\n/\u003e\r\n\r\n\u003c/p\u003e\n\u003cp\u003eThe dotted line represents the regime of the theoretical bound of the form $\\alpha \\rightarrow \\frac{C1}{\\alpha^2}$ . This is once again the shape of the empirical curves quite surprisingly!\u003c/p\u003e\n\u003ch3 id=\"section-7\"\u003eConclusion\u003c/h3\u003e\n\u003cp\u003eFrom a problem rooted in an \u003cstrong\u003eethical dilemma\u003c/strong\u003e (privacy versus completeness and transparency), we have looked at the \u003cstrong\u003ecost of guaranteeing\u003c/strong\u003e one at the expense of the other, to better sketch out desirable situations.\u003cbr\u003e\nThis has enabled us to develop theoretical results in terms of \u003cstrong\u003eminimax rates\u003c/strong\u003e. There is indeed a \u003cstrong\u003etrade-off\u003c/strong\u003e between these criteria, which is even more costly in the case of non-parametric density estimation.\u003cbr\u003e\nFinally, we have compared these theoretical limits with empirical results, which \u003cstrong\u003econfirm the conformity of the statements\u003c/strong\u003e.\u003cbr\u003e\nThe aim of all this work is to disseminate this important yet under-exploited notion: privacy. To this end, we invite the reader to take the following \u003cstrong\u003equiz\u003c/strong\u003e to ensure his or her understanding.\u003c/p\u003e\n\u003ch1 id=\"section-8\"\u003eQuizz\u003c/h1\u003e\n\u003cp\u003eTo test yourself abour privacy:\u003c/p\u003e\n\u003cform id=\"quiz-form\" class=\"quiz-form\"\u003e\r\n    \u003cdiv class=\"quiz-question\"\u003e\r\n        \u003cp\u003eWhat is privacy?\u003c/p\u003e\r\n        \u003cdiv class=\"quiz-options\"\u003e\r\n            \u003clabel\u003e\r\n                \u003cinput type=\"radio\" name=\"question1\" value=\"1\"\u003e\r\n                Avoid asking questions that can raise private information\r\n            \u003c/label\u003e\r\n            \u003clabel\u003e\r\n                \u003cinput type=\"radio\" name=\"question1\" value=\"2\"\u003e\r\n                A mechanism that prevents other agent to retrieve personnal information in your answer\r\n            \u003c/label\u003e\r\n            \u003clabel\u003e\r\n                \u003cinput type=\"radio\" name=\"question1\" value=\"3\"\u003e\r\n                An ethical-washing trend\r\n            \u003c/label\u003e\r\n        \u003c/div\u003e\r\n        \u003cp\u003eWhich situation is α-local-differentially privacy?\u003c/p\u003e\r\n        \u003cdiv class=\"quiz-options\"\u003e\r\n            \u003clabel\u003e\r\n                \u003cinput type=\"radio\" name=\"question2\" value=\"1\"\u003e\r\n                sup {Q(Z | Xi = x)/Q(Z | Xi = x')} | x, x' ∈ X} \u003e= exp(α)\r\n            \u003c/label\u003e\r\n            \u003clabel\u003e\r\n                \u003cinput type=\"radio\" name=\"question2\" value=\"2\"\u003e\r\n                You tell the truth half the time, you lie otherwise.\r\n            \u003c/label\u003e\r\n            \u003clabel\u003e\r\n                \u003cinput type=\"radio\" name=\"question2\" value=\"3\"\u003e\r\n                Z_i = X_i + (2M/α) W_i with W_i drawn from a Laplace Noise(0,1)\r\n            \u003c/label\u003e\r\n        \u003c/div\u003e\r\n        \u003cp\u003eWhat is the privacy cost in term of optimal rate ?\u003c/p\u003e\r\n        \u003cdiv class=\"quiz-options\"\u003e\r\n            \u003clabel\u003e\r\n                \u003cinput type=\"radio\" name=\"question3\" value=\"1\"\u003e\r\n                Multinomial estimation: A factor α^2/d\r\n            \u003c/label\u003e\r\n            \u003clabel\u003e\r\n                \u003cinput type=\"radio\" name=\"question3\" value=\"2\"\u003e\r\n                Density estimation: from n^(-2β/2β+2) (without privacy) to (nα^2)^(-2β/(2β+2))\r\n            \u003c/label\u003e\r\n            \u003clabel\u003e\r\n                \u003cinput type=\"radio\" name=\"question3\" value=\"3\"\u003e\r\n                We loose nothing, that's the surprising finding of the paper\r\n            \u003c/label\u003e\r\n        \u003c/div\u003e\r\n    \u003c/div\u003e\r\n    \u003c!-- Add more quiz questions as needed --\u003e\r\n    \u003cbutton type=\"submit\" class=\"quiz-submit\"\u003eSubmit\u003c/button\u003e\r\n\u003c/form\u003e\r\n\u003cdiv id=\"quiz-results\" class=\"quiz-results\"\u003e\u003c/div\u003e\r\n\u003cscript\u003e\r\n    // Define quiz questions and correct answers\r\n    const quizQuestions = [\r\n        {\r\n            question: \"What is privacy?\",\r\n            answer: \"2\"\r\n        },\r\n        //Add more quiz questions as needed\r\n        {\r\n            question: \"Which situation is α-local-differentially privacy?\",\r\n            answer: \"3\"\r\n        },\r\n        //Add more quiz questions as needed\r\n        {\r\n            question: \"What is the privacy cost in term of optimal rate ?\",\r\n            answer: \"1\"\r\n        }\r\n    ];\r\n\r\n    // Handle form submission\r\n    document.getElementById('quiz-form').addEventListener('submit', function(event) {\r\n        event.preventDefault();\r\n\r\n        // Calculate quiz score\r\n        let score = 0;\r\n        quizQuestions.forEach(question =\u003e {\r\n            const selectedAnswer = document.querySelector(`input[name=\"question${quizQuestions.indexOf(question) + 1}\"]:checked`);\r\n            if (selectedAnswer) {\r\n                if (selectedAnswer.value.toLowerCase() === question.answer) {\r\n                    score++;\r\n                    selectedAnswer.parentElement.classList.add('correct');\r\n                } else {\r\n                    selectedAnswer.parentElement.classList.add('incorrect');\r\n                }\r\n            }\r\n        });\r\n\r\n        // Display quiz results\r\n        const quizResults = document.getElementById('quiz-results');\r\n        quizResults.innerHTML = `\u003cp\u003eYou scored ${score} out of ${quizQuestions.length}.\u003c/p\u003e`;\r\n    });\r\n\u003c/script\u003e\r\n\u003chr\u003e\n\u003chr\u003e\n\u003ch2 id=\"annexes\"\u003eAnnexes\u003c/h2\u003e\n\u003ch3 id=\"references\"\u003eReferences\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eWarner SL. Randomized response: a survey technique for eliminating evasive answer bias. J Am Stat Assoc. 1965 Mar;60(309):63-6. PMID: 12261830.\u003c/li\u003e\n\u003cli\u003eJohn C. Duchi, Michael I. Jordan, and Martin Wainwright. Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation. Advances in Neural Information Processing Systems (2013)\u003c/li\u003e\n\u003cli\u003eDwork, C., \u0026amp; Roth, A. (2014). The algorithmic foundations of differential privacy. Foundations and Trends® in Theoretical Computer Science, 9(3-4), 211-407.\u003c/li\u003e\n\u003cli\u003eNarayanan, A., \u0026amp; Shmatikov, V. (2008). Robust de-anonymization of large sparse datasets. In Security and Privacy, 2008. SP 2008. IEEE Symposium on (pp. 111-125). IEEE.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cscript\u003e\r\nfunction highlight(text) {\r\n  var inputText = document.getElementById(\"markdown-content\");\r\n  var innerHTML = inputText.innerHTML;\r\n  var index = innerHTML.indexOf(text);\r\n  if (index \u003e= 0) {\r\n    innerHTML = innerHTML.substring(0,index) + \"\u003cspan class='highlight'\u003e\" + innerHTML.substring(index,index+text.length) + \"\u003c/span\u003e\" + innerHTML.substring(index + text.length);\r\n    inputText.innerHTML = innerHTML;\r\n  }\r\n}\r\nhighlight(\"Estimating Privacy in Data Science\");\r\n\r\n\u003c/script\u003e\r\n\u003chr\u003e\n\u003cscript\u003e\r\n    function displayInput() {\r\n        var inputValue = document.getElementById(\"inputField\").value;\r\n        document.getElementById(\"output\").innerText = \"You typed: \" + inputValue;\r\n    }\r\n\u003c/script\u003e\r\n\u003cstyle\u003e\r\n.highlight {\r\n  background-color: red;\r\n}\r\n.highlight-on-hover:hover {\r\n        background-color: yellow;\r\n    }\r\n/* Quiz form styles */\r\n.quiz-form {\r\n        max-width: 500px;\r\n        margin: auto;\r\n        padding: 20px;\r\n        border: 1px solid #ccc;\r\n        border-radius: 5px;\r\n        background-color: #f9f9f9;\r\n}\r\n\r\n.quiz-question {\r\n        margin-bottom: 20px;\r\n}\r\n\r\n.quiz-options label {\r\n        display: block;\r\n        margin-bottom: 10px;\r\n}\r\n\r\n.quiz-submit {\r\n        background-color: #4caf50;\r\n        color: white;\r\n        padding: 10px 20px;\r\n        border: none;\r\n        border-radius: 5px;\r\n        cursor: pointer;\r\n}\r\n\r\n.quiz-submit:hover {\r\n        background-color: #45a049;\r\n}\r\n\r\n/* Quiz results styles */\r\n.quiz-results {\r\n        margin-top: 20px;\r\n        font-weight: bold;\r\n}\r\n.quiz-options label {\r\n        display: block;\r\n        margin-bottom: 10px;\r\n    }\r\n.quiz-options label.correct {\r\n        color: green;\r\n}\r\n.quiz-options label.incorrect {\r\n        color: red;\r\n}\r\na[name]:hover {\r\n        background-color: yellow; /* Change to the same color as normal state to maintain yellow highlight */\r\n        text-decoration: none; /* Optionally remove underline on hover */\r\n}\r\n\u003c/style\u003e\r\n\u003cstyle TYPE=\"text/css\"\u003e\r\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\r\n\u003c/style\u003e\r\n\u003cscript type=\"text/x-mathjax-config\"\u003e\r\nMathJax.Hub.Config({\r\n    tex2jax: {\r\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\r\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\r\n    }\r\n});\r\nMathJax.Hub.Queue(function() {\r\n    var all = MathJax.Hub.getAllJax(), i;\r\n    for(i = 0; i \u003c all.length; i += 1) {\r\n        all[i].SourceElement().parentNode.className += ' has-jax';\r\n    }\r\n});\r\n\u003c/script\u003e\r\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\r\n",
      "url": "http://localhost:1313/posts/statistical_minimax_rates_under_privacy/",
      "date_published": "31016-31-09T122:3131:00+01:00",
      "date_modified": "31016-31-09T122:3131:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "aba6ac7516897c4ee7afcdf83296daebaa43622b",
      "title": "Another article",
      "summary": "",
      "content_text": "Authors : John Smith and John Smith\nDo not forget to add the script posted on moodle to enable latex in your blogpost! What a beauty! $y=\\theta_0 + \\theta_1x_1$\n",
      "content_html": "\u003cp\u003e\u003cstrong\u003eAuthors\u003c/strong\u003e : John Smith and John Smith\u003c/p\u003e\n\u003chr\u003e\u003c/hr\u003e\r\n\u003cstyle\r\nTYPE=\"text/css\"\u003e\r\n\u003cp\u003ecode.has-jax {font:\ninherit;\nfont-size:\n100%;\nbackground:\ninherit;\nborder:\ninherit;}\u003c/p\u003e\n\u003cp\u003e\u003c/style\u003e\u003c/p\u003e\n\u003cscript\r\ntype=\"text/x-mathjax-config\"\u003e\r\n\r\nMathJax.Hub.Config({\r\n\r\n    tex2jax: {\r\n\r\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\r\n\r\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\r\n\r\n    }\r\n\r\n});\r\n\r\nMathJax.Hub.Queue(function() {\r\n\r\n    var all = MathJax.Hub.getAllJax(), i;\r\n\r\n    for(i = 0; i \u003c all.length; i += 1) {\r\n\r\n        all[i].SourceElement().parentNode.className += ' has-jax';\r\n\r\n    }\r\n\r\n});\r\n\r\n\u003c/script\u003e\r\n\u003cscript\r\ntype=\"text/javascript\"\r\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\r\n\u003cp\u003eDo not forget to add the script posted on moodle to enable latex in your blogpost!\nWhat a beauty! $y=\\theta_0 + \\theta_1x_1$\u003c/p\u003e\n",
      "url": "http://localhost:1313/posts/my-first-blog/",
      "date_published": "8016-08-09T126:88:00+01:00",
      "date_modified": "8016-08-09T126:88:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    },
    
    {
      "id": "94409f4b19cf119747df5e05d9507c6fc3ae2286",
      "title": "Title of the article",
      "summary": "",
      "content_text": "Authors : John Smith and John Smith\nStart writing here !\n",
      "content_html": "\u003cp\u003e\u003cstrong\u003eAuthors\u003c/strong\u003e : John Smith and John Smith\u003c/p\u003e\n\u003chr\u003e\u003c/hr\u003e\r\n\u003cp\u003eStart writing here !\u003c/p\u003e\n",
      "url": "http://localhost:1313/posts/my-second-blog/",
      "date_published": "8016-08-09T126:88:00+01:00",
      "date_modified": "8016-08-09T126:88:00+01:00",
      "author": {
        "name": "Students from M2 Data Science IP Paris",
        "url": "http://localhost:1313/"
      }
    }
    
  ]
}