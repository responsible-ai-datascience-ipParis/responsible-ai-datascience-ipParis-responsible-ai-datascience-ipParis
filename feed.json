{"version":"https://jsonfeed.org/version/1","title":"Bloggin on Responsible AI","home_page_url":"https://responsible-ai-datascience-ipParis.github.io/","feed_url":"https://responsible-ai-datascience-ipParis.github.io/feed.json","description":"Bloggin on Responsible AI","favicon":"https://responsible-ai-datascience-ipParis.github.io//assets/favicon.ico","expired":false,"author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"},"items":[{"id":"37b39cffb06a5944d96062052d7f779421cde6be","title":"RobustAI_RegMixup","summary":"","content_text":" \u003c!DOCTYPE html\u003e Styled Table RegMixup : Regularizer for robust AI Improve accuracy and Out-of-Distribution Robustness Authors: Marius Ortega, Ly An CHHAY Paper : RegMixup by Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania Table of Contents Abstract Introduction Prerequisites Empirical Risk Minimization Vicinal Risk Minimization Mixup RegMixup in theory RegMixup in practice Conclusion Abstract In this blog post, we will present the paper \u0026ldquo;RegMixup: Regularizer for robust AI\u0026rdquo; by Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania. This paper introduces a new regularizer called RegMixup, which is designed to improve the accuracy and out-of-distribution robustness of deep neural networks. The authors show that RegMixup can be used to improve the performance of state-of-the-art models on various datasets, including CIFAR-10, CIFAR-100, and ImageNet. The paper also provides an extensive empirical evaluation of RegMixup, demonstrating its effectiveness in improving the robustness of deep neural networks to out-of-distribution samples.\nIn this blong post, we will provide an overview of the paper, explain the theoretical background of RegMixup, and finally, perform a toy example to demonstrate how to use RegMixup with the torch-uncertainty library.\nIntroduction Most real-world machine algorithm applications are good when it comes to predicting new data following the train distribution. However, they are not robust to out-of-distribution (OOD) samples (i.e. when the test data distribution is different from the train data distribution). This is a major problem in machine learning as it can lead to catastrophic predictions.\nThe question is how to improve the robustness of machine learning algorithms to OOD samples ? Many researchers have tried such as Liu et al. (2020a, 2020b), Wen et al. (2021), Lakshminarayanan et al. (2017). Even though they have shown some improvements, their approaches use expensive ensemble methods or propose non-trivial modifications of the neural network architecture. What if we could improve the robustness of deep neural networks with respect to OOD samples while utilizing much simpler and cost-effective methods?\nThe first step toward the method presented in this blog is Mixup, proposed by Zang and al (2018). This method is quite good when it comes to dealing with slight perturbations in the data distribution. However, Mixup has the tendency to emphasize difference in labels from very similar samples (high predictive entropy). This is not ideal for OOD samples as the model do not differentiate ID (In-distribution) and OOD samples very well.\nRegMixup adds a new layer to Mixup by using it as a regularizer. From there, we will present the theoretical background of the paper, the implementation so as to easily use it in practice.\n1. Prerequisites In order to understand the paper, we need to understand what is Empirical and Vicinal Risk Minimization (ERM and VRM) as well as Mixup.\n1.1. Empirical Risk Minimization (ERM) Empirical Risk Minimization is an inference principle which consists in finding the model $\\hat{f}$ that minimizes the empirical risk $R_{emp}(\\hat{f})$ on the training set. The empirical risk is defined as the average loss over the training set :\n$$ R_{emp}(\\hat{f}) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{f}(x_i), y_i) \\tag{1} $$\nwhere $L$ is the loss function, $x_i$ is the input, $y_i$ is the label and $n$ is the number of samples in the training set. However, ERM contains a very strong assumption which is that $\\hat{f} \\approx f$ where $f$ is the true (and unknown) distribution for all points of the dataset. Thereby, if the testing set distribution differs even slighly from the training set one, ERM is unable to explain or provide generalization. Vicinal Risk is a way to relax this assumption.\n1.2. Vicinal Risk Minimization (VRM) Vicinal Risk Minimization (VRM) is a generalization of ERM. Instead of having a single distribution estimate $\\hat{f}$, VRM uses a set of distributions $\\hat{f}_{x_i, y_i}$ for each training sample $(x_i, y_i)$. The goal is to minimize the average loss over the training set, but with respect to the vicinal distribution of each sample.\n$$ R_{vrm}(\\hat{f}) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{f}_{x_i, y_i}(x_i), y_i) \\tag{2} $$\nConsequently, each training point has its own distribution estimate. This is a way to relax the strong assumption of ERM explained above.\n1.3. Mixup Mixup is a data augmentation technique that generates new samples by mixing pairs of training samples. By doing so, Mixup regularizes models to favor simple linear behavior in-between training examples. Experimentally speaking, Mixup has been shown to improve the generalization of deep neural networks, increase their robustness to adversarial attacks, reduce the memorization of corrupt labels as well as stabilize the training of generative adversarial networks.\nIn essence, Mixup can be thought as a learning objective designed for robustness and accountability of the model. Now, let\u0026rsquo;s see how Mixup works.\nFirst, we take two samples $(x_i, y_i)$ and $(x_j, y_j)$ from the training set. Then, we generate a new sample $(\\tilde{x}, \\tilde{y})$ by taking a convex combination of the two samples with a mixup coefficient $\\lambda \\sim \\text{Beta}(\\alpha, \\alpha)$ :\n$$ \\tilde{x} = \\lambda x_i + (1 - \\lambda) x_j \\hspace{1cm} \\tilde{y} = \\lambda y_i + (1 - \\lambda) y_j $$\nWe can then define the vicinal distribution of the mixed sample $(\\tilde{x}, \\tilde{y})$ as :\n$$ P_{x_i, y_i} = \\mathbb{E}_\\lambda[( \\delta {\\tilde{x}_i}(x), \\delta{\\tilde{y}_i}(y))] \\tag{3} $$\nMixup is an interesting method to consider but it possesses some limitations :\nSmall $\\alpha$ issues : With our setup, $\\alpha \\approx 1$ encourages $\\tilde{x}$ to be perceptually different from $x$. Consequently, training and testing distribution will also grow appart from each other. When $\\alpha \\ll 1$, the mixup convex interpolation factor λ leads to a sharp peaks of 0 and 1. Therefore, Mixup will produce samples close to the initial ones (in case λ close to 1) or in the direction of another sample (in case of λ close to 0). Look at the figure below, one of the two interpolating images dominates the interpolated one. What is noticed after cross-validation of alpha is that the best values are $\\alpha \\approx 0.2$ which is very small. Consequently, the final sample effectively presents only a small perturbation in comparison to the original one while the vicinal distribution exploration space is much larger. We could say that Mixup does not allow to use the full potential of the vicinal distributions of the data. Model underconfidence : When a neural network is trained with Mixup, it is only exposed to interpolated samples. Consequently, the model learns to predict smoothed labels which is the very root cause of its underconfidence. This results in a high predictive entropy for both ID and OOD samples. Mixup vs RegMixup, underconfidence and space exploration.\n2. RegMixup in theory Now that we have understood the path that led to RegMixup, we will explore its theoretical background and see how and why it is a good regularizer for robust AI.\nWhile Mixup utilizes data points\u0026rsquo; vicinal distribution only, RegMixup uses both the vicinal and the empirical one (refering respectively to VRM and ERM). This can seem far-fetched or even counter-intuitive but produces very interesting properties.\n$$ P(x, y) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\gamma \\delta_{x_i}(x) \\delta_{y_i}(y) + (1-\\gamma) P_{x_i, y_i}(x, y) \\right) \\tag{4} $$\nHere, $\\gamma$ is the hyperparameter controlling the mixup between the empirical and vicinal distribution. In fact, we see that the distribution $P(x, y)$ for RegMixup is a convex combination of the empirical distribution (left term of the addition in equation 4) and the vicinal distribution defined with equations (2) and (3).\nFrom there, we can define a new loss function $\\mathcal{L}$ based on the Cross Entropy Loss ($\\text{CE}$)\n$$ \\mathcal{L}(\\hat{y}, y) = \\text{CE}(p_\\theta(\\hat{y} \\vert x), y) + \\eta \\text{CE}(p_\\theta(\\hat{y} \\vert \\tilde{x}), \\tilde{y}) \\tag{5} $$\nWith $ \\eta \\in R_{+}^{\\ast} $ being the hyperparameter controlling the importance of the vicinal cross entropy sub-loss and $p_\\theta$ the activation function of the model parameterized by $\\theta$. In the paper, the value of $\\eta$ is set to 1 and its variation seem negligible. Consequently, we will not focus on it in this blog post.\nSuch a model (equation 4) exhibits properties that lacked in Mixup :\nValues of $\\alpha$ and underconfidence : As we explicitly add the empirical distribution to the vicinal one, the ERM term will encourage the model to predict the true labels of the training set while the VRM term, motivated by the interpolation factor $\\lambda$, will explore the vicinal distribution space in a much more thorough way than what was possible with Mixup. For instance, if λ $\\approx$ 0.5, a wide variety of images containing features from both the images in the pair are obtained (look at the figure). Consequently, the ERM term allows to better predict in-distribution samples while the VRM term, with a larger $\\alpha$, will allow to better predict OOD samples. This is a very interesting property as it allows to have a model that is both confident and accurate. Prediction entropy : Through their experiments and observations, researchers found that a cross-validated value of $\\alpha$ leads to a maximum likelihood estimation having high entropy for ODD samples only. While Mixup demonstrated high entropy for both ID and OOD samples, RegMixup is able to differentiate between the two. This is an highly desirable properties indicating us that RegMixup acts as a regularizer in essense. As a preliminary conclusion, RegMixup is a very powerful, cost-efficient and simple-to-implement regularizer that allows to improve the robustness and accuracy of deep neural networks for both in-distribution and out-of-distribution samples. In the next section, we will see how to use RegMixup in practice trough a toy example.\n3. RegMixup in practice (implementation) Now, our objective will be to demonstrate the effectiveness of RegMixup through a very simple example. We will use the CIFAR-10-C dataset (corrupted version of CIFAR-10) and a standard ResNet-18 model. We will compare performances of 3 models :\nA baseline model trained with ERM A model trained with Mixup A model trained with RegMixup To do so, we have two possibilities :\nUse the official implementation of RegMixup available on Francesco Pinto's GitHub. Use the torch-uncertainty library which provides a simple and efficient way to use RegMixup. Note, the library is developed by researchers from ENSTA Paris and is available on GitHub. In this blog post, we will use the torch-uncertainty library as it is very simple to use and provides a very well-implemented version of RegMixup.\n3.1. Installation First, we need to install the torch-uncertainty library. To do so, we can use pip :\npip install torch-uncertainty Note: If you use a gpu, torch-uncertainty will automatically install a cpu version of torch and torchvision, you can compile the following lines to install the gpu version of torch and torchvision (took from PyTorch website) :\npip unistall torch torchvision pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118 To check if the installation was successful, you can run the following code, it should return True if you have a gpu and False if you don\u0026rsquo;t have one :\nimport torch print(torch.cuda.is_available()) 3.2. Training the models with torch-uncertainty Now that we have installed torch-uncertainty, we can train the models. First, we need to import the necessary libraries :\nfrom torch_uncertainty import cli_main, init_args from torch_uncertainty.baselines.classification import ResNet from torch_uncertainty.optimization_procedures import optim_cifar10_resnet18 from torch_uncertainty.datamodules import CIFAR10DataModule from torchvision.datasets import CIFAR10 from torchvision import transforms from torch.nn import CrossEntropyLoss import torch import os from pathlib import Path from cli_test_helpers import ArgvContext Then, we can define the 3 models we discussed earlier :\nbaseline = ResNet(num_classes=10, loss=CrossEntropyLoss, optimization_procedure=optim_cifar10_resnet18, version=\u0026#34;std\u0026#34;, in_channels=3, arch=18).cuda() mixup = ResNet(num_classes=10, loss=CrossEntropyLoss, optimization_procedure=optim_cifar10_resnet18, version=\u0026#34;std\u0026#34;, in_channels=3, arch=18, mixup=True, mixup_alpha=0.2).cuda() regmixup = ResNet(num_classes=10, loss=CrossEntropyLoss, optimization_procedure=optim_cifar10_resnet18, version=\u0026#34;std\u0026#34;, in_channels=3, arch=18, reg_mixup=True, mixup_alpha=15).cuda() Before training the models, we need to define important arguments such as training parameters (epochs, estimators, etc.) and the datamodule. We can do so with the following code:\nroot = Path(os.path.abspath(\u0026#34;\u0026#34;)) # We mock the arguments for the trainer with ArgvContext( \u0026#34;file.py\u0026#34;, \u0026#34;--max_epochs\u0026#34;, \u0026#34;20\u0026#34;, \u0026#34;--enable_progress_bar\u0026#34;, \u0026#34;False\u0026#34;, \u0026#34;--num_estimators\u0026#34;, \u0026#34;8\u0026#34; ): args = init_args(network=ResNet, datamodule=CIFAR10DataModule) net_name = \u0026#34;logs/reset18-cifar10\u0026#34; # datamodule args.root = str(root / \u0026#34;data\u0026#34;) dm = CIFAR10DataModule(**vars(args)) Finally, we can train the models using the cli_main function from torch-uncertainty :\nresults_baseline = cli_main(baseline, dm, root, net_name, args=args) results_mixup = cli_main(mixup, dm, root, net_name, args=args) results_regmixup = cli_main(regmixup, dm, root, net_name, args=args) Note: If you have a gpu, you can make a slight modification to the code to use it :\nClick on cli_main and press F12 to go to the function definition. Go to line 222 and replace the trainer definition by the following one : # trainer trainer = pl.Trainer.from_argparse_args( args, accelerator=\u0026#34;gpu\u0026#34;, devices=1, callbacks=callbacks, logger=tb_logger, deterministic=(args.seed is not None), inference_mode=not (args.opt_temp_scaling or args.val_temp_scaling), ) Save the file and you are all set. 3.3. Results So as to compare the performances of the 3 models, we use two corrupted versions of Cifar-10-C. The first version has a corruption severity factor of 5 (slight data corruption) and the second one has a corruption severity factor of 15 (more severe data corruption). Our study contains 5 metrics : entropy, accuracy, brier score, expected calibration error (ECE) and negative log-likelihood (NLL). In our explanation, we will focus on the accuracy and entropy to keep it simple.\nWith corruption severity factor of 5, we obtain the following results :\nentropy accuracy brier ece nll baseline 0.656294 0.7480 0.349862 0.032466 0.729336 mixup 0.640811 0.7578 0.335403 0.024429 0.703844 regmixup 0.676174 0.7564 0.340233 0.023135 0.711405 First of all, we can see that the accuracy is quite similar for the 3 models. This makes sense as the corruption severity factor is quite low, thus cifar-10-c is not very different from the original cifar-10. However, we can see that the entropy of the RegMixup model is higher than the one of the Mixup model. This is symptomatic of Mixup\u0026rsquo;s underconfidence. As stated previously, given the low corruption severity factor of cifar-10-c, the underconfidence of Mixup does not impact its performances in a visible manner.\nWith corruption severity factor of 15, we obtain the following results :\nentropy accuracy brier ece nll baseline 0.615607 0.7402 0.358522 0.048414 0.750933 mixup 0.698558 0.7558 0.338540 0.014760 0.709190 regmixup 0.702599 0.7614 0.327945 0.008439 0.687550 Here the results are much more unequivocal. As the severity factor increases, the baseline model drops in accuracy and entropy, Mixup also drops in accuracy but increases in entropy and RegMixup increases in accuracy and entropy. Here, RegMixup has the higher entropy as the model has higher entropy for OOD samples which are more frequent at this corruption level. Mixup shows a greater delta increase in entropy due to its higher predictive entropy tendency whether or not samples are OOD or ID. Consequently, RegMixup is more confident and accurate than the Mixup model eventhough Mixup is not fully underperforming.\n4. Conclusion As a conclusion, we have seen that RegMixup is a powerful method to regularize deep neural networks. Despite being very simple and cost-effective, it is important to specify that the paper does not provide a theoretical explanation of the method. These experimental grounds are very promising but it appears important to stay cautious while utilizing RegMixup.\n","content_html":"\u003cstyle\nTYPE=\"text/css\"\u003e\n\ncode.has-jax {font:\ninherit;\nfont-size:\n100%; \nbackground: \ninherit; \nborder: \ninherit;}\n\n\u003c/style\u003e\n\u003cscript\ntype=\"text/x-mathjax-config\"\u003e\n\nMathJax.Hub.Config({\n\n    tex2jax: {\n\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n\n    }\n\n});\n\nMathJax.Hub.Queue(function() {\n\n    var all = MathJax.Hub.getAllJax(), i;\n\n    for(i = 0; i \u003c all.length; i += 1) {\n\n        all[i].SourceElement().parentNode.className += ' has-jax';\n\n    }\n\n});\n\n\u003c/script\u003e\n\u003cscript\ntype=\"text/javascript\"\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"UTF-8\"\u003e\n\u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e\n\u003ctitle\u003eStyled Table\u003c/title\u003e\n\u003cstyle\u003e\n    table {\n        border-collapse: collapse;\n        width: 100%;\n    }\n    th, td {\n        padding: 8px;\n        text-align: center;\n        border-bottom: 1px solid #ddd;\n    }\n    th {\n        background-color: #f2f2f2;\n    }\n    tr:hover {\n        background-color: #f5f5f5;\n    }\n\u003c/style\u003e\n\u003c/head\u003e\n\u003c/html\u003e\n\u003ch1 style=\"font-size: 36px;\"\u003eRegMixup : Regularizer for robust AI\u003c/h1\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eImprove accuracy and Out-of-Distribution Robustness\u003ch1\u003e\n\u003ch1 style=\"font-size: 18px;\"\u003eAuthors: Marius Ortega, Ly An CHHAY \u003cbr /\u003e\nPaper : \u003ca href=\"https://arxiv.org/abs/2206.14502\"\u003eRegMixup\u003c/a\u003e  by Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania\u003c/h1\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0.0\"\u003eAbstract\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-0.1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003ePrerequisites\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-1.1\"\u003eEmpirical Risk Minimization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1.2\"\u003eVicinal Risk Minimization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1.3\"\u003eMixup\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eRegMixup in theory\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eRegMixup in practice \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0.0\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eIn this blog post, we will present the paper \u0026ldquo;RegMixup: Regularizer for robust AI\u0026rdquo; by Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania. This paper introduces a new regularizer called RegMixup, which is designed to improve the accuracy and out-of-distribution robustness of deep neural networks. The authors show that RegMixup can be used to improve the performance of state-of-the-art models on various datasets, including CIFAR-10, CIFAR-100, and ImageNet. The paper also provides an extensive empirical evaluation of RegMixup, demonstrating its effectiveness in improving the robustness of deep neural networks to out-of-distribution samples.\u003c/p\u003e\n\u003cp\u003eIn this blong post, we will provide an overview of the paper, explain the theoretical background of RegMixup, and finally, perform a toy example to demonstrate how to use RegMixup with the torch-uncertainty library.\u003c/p\u003e\n\u003ch2 id=\"section-0.1\"\u003eIntroduction \u003c/h2\u003e\n\u003cp\u003eMost real-world machine algorithm applications are good when it comes to predicting new data following the train distribution. However, they are not robust to out-of-distribution (OOD) samples (i.e. when the test data distribution is different from the train data distribution). This is a major problem in machine learning as it can lead to catastrophic predictions.\u003c/p\u003e\n\u003cp\u003eThe question is how to improve the robustness of machine learning algorithms to OOD samples ?\nMany researchers have tried such as Liu et al. (2020a, 2020b), Wen et al. (2021), Lakshminarayanan et al. (2017). Even though they have shown some improvements, their approaches use expensive ensemble methods or propose non-trivial modifications of the neural network architecture. What if we could improve the robustness of deep neural networks with respect to OOD samples while utilizing much simpler and cost-effective methods?\u003c/p\u003e\n\u003cp\u003eThe first step toward the method presented in this blog is Mixup, proposed by Zang and al (2018). This method is quite good when it comes to dealing with slight perturbations in the data distribution. However, Mixup has the tendency to emphasize difference in labels from very similar samples (high predictive entropy). This is not ideal for OOD samples as the model do not differentiate ID (In-distribution) and OOD samples very well.\u003c/p\u003e\n\u003cp\u003eRegMixup adds a new layer to Mixup by using it as a regularizer. From there, we will present the theoretical background of the paper, the implementation so as to easily use it in practice.\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003e1. Prerequisites \u003c/h2\u003e\n\u003cp\u003eIn order to understand the paper, we need to understand what is Empirical and Vicinal Risk Minimization (ERM and VRM) as well as Mixup.\u003c/p\u003e\n\u003ch3 id=\"section-1.1\"\u003e1.1. Empirical Risk Minimization (ERM)\u003c/h3\u003e\n\u003cp\u003eEmpirical Risk Minimization is an inference principle which consists in finding the model $\\hat{f}$ that minimizes the empirical risk $R_{emp}(\\hat{f})$ on the training set. The empirical risk is defined as the average loss over the training set :\u003c/p\u003e\n\u003cp\u003e$$\nR_{emp}(\\hat{f}) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{f}(x_i), y_i) \\tag{1}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $L$ is the loss function, $x_i$ is the input, $y_i$ is the label and $n$ is the number of samples in the training set. However, ERM contains a very strong assumption which is that $\\hat{f} \\approx f$ where $f$ is the true (and unknown) distribution for all points of the dataset. Thereby, if the testing set distribution differs even slighly from the training set one, ERM is unable to explain or provide generalization. Vicinal Risk is a way to relax this assumption.\u003c/p\u003e\n\u003ch3 id=\"section-1.2\"\u003e1.2. Vicinal Risk Minimization (VRM)\u003c/h3\u003e\n\u003cp\u003eVicinal Risk Minimization (VRM) is a generalization of ERM. Instead of having a single distribution estimate $\\hat{f}$, VRM uses a set of distributions $\\hat{f}_{x_i, y_i}$ for each training sample $(x_i, y_i)$. The goal is to minimize the average loss over the training set, but with respect to the vicinal distribution of each sample.\u003c/p\u003e\n\u003cp\u003e$$\nR_{vrm}(\\hat{f}) = \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{f}_{x_i, y_i}(x_i), y_i) \\tag{2}\n$$\u003c/p\u003e\n\u003cp\u003eConsequently, each training point has its own distribution estimate. This is a way to relax the strong assumption of ERM explained above.\u003c/p\u003e\n\u003ch3 id=\"section-1.3\"\u003e1.3. Mixup\u003c/h3\u003e\n\u003cp\u003eMixup is a data augmentation technique that generates new samples by mixing pairs of training samples. By doing so, Mixup regularizes models to favor simple linear behavior in-between training examples. Experimentally speaking, Mixup has been shown to improve the generalization of deep neural networks, increase their robustness to adversarial attacks, reduce the memorization of corrupt labels as well as stabilize the training of generative adversarial networks.\u003c/p\u003e\n\u003cp\u003eIn essence, Mixup can be thought as a learning objective designed for robustness and accountability of the model. Now, let\u0026rsquo;s see how Mixup works.\u003c/p\u003e\n\u003cp\u003eFirst, we take two samples $(x_i, y_i)$ and $(x_j, y_j)$ from the training set. Then, we generate a new sample $(\\tilde{x}, \\tilde{y})$ by taking a convex combination of the two samples with a mixup coefficient $\\lambda \\sim \\text{Beta}(\\alpha, \\alpha)$ :\u003c/p\u003e\n\u003cp\u003e$$\n\\tilde{x} = \\lambda x_i + (1 - \\lambda) x_j \\hspace{1cm}\n\\tilde{y} = \\lambda y_i + (1 - \\lambda) y_j\n$$\u003c/p\u003e\n\u003cp\u003eWe can then define the vicinal distribution of the mixed sample $(\\tilde{x}, \\tilde{y})$ as :\u003c/p\u003e\n\u003cp\u003e$$\nP_{x_i, y_i} = \\mathbb{E}_\\lambda[( \\delta {\\tilde{x}_i}(x), \\delta{\\tilde{y}_i}(y))] \\tag{3}\n$$\u003c/p\u003e\n\u003cp\u003eMixup is an interesting method to consider but it possesses some limitations :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSmall $\\alpha$ issues :\u003c/strong\u003e With our setup, $\\alpha \\approx 1$ encourages $\\tilde{x}$ to be perceptually different from $x$. Consequently, training and testing distribution will also grow appart from each other. When $\\alpha \\ll 1$, the mixup convex interpolation factor λ leads to a sharp peaks of 0 and 1. Therefore, Mixup will produce samples close to the initial ones (in case λ close to 1) or in the direction of another sample (in case of λ close to 0). Look at the \u003cstrong\u003e\u003ca href=\"#my-fig\"\u003efigure\u003c/a\u003e\u003c/strong\u003e below, one of the two interpolating images dominates the interpolated one. What is noticed after cross-validation of alpha is that the best values are $\\alpha \\approx 0.2$ which is very small. Consequently, the final sample effectively presents only a small perturbation in comparison to the original one while the vicinal distribution exploration space is much larger. We could say that Mixup does not allow to use the full potential of the vicinal distributions of the data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel underconfidence :\u003c/strong\u003e When a neural network is trained with Mixup, it is only exposed to interpolated samples. Consequently, the model learns to predict smoothed labels which is the very root cause of its underconfidence. This results in a high predictive entropy for both ID and OOD samples.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure id=\"my-fig\" class=\"numbered\"\u003e\n    \u003cimg src=\"/images/regmixup/fig.png\" class=\"align-center\"\u003e\n    \u003cp style=\"text-align: center;\"\u003eMixup vs RegMixup, underconfidence and space exploration.\u003c/p\u003e\n\u003c/figure\u003e\n\u003ch2 id=\"section-2\"\u003e2. RegMixup in theory\u003c/h2\u003e\n\u003cp\u003eNow that we have understood the path that led to RegMixup, we will explore its theoretical background and see how and why it is a good regularizer for robust AI.\u003c/p\u003e\n\u003cp\u003eWhile Mixup utilizes data points\u0026rsquo; vicinal distribution only, RegMixup uses both the vicinal and the empirical one (refering respectively to VRM and ERM). This can seem far-fetched or even counter-intuitive but produces very interesting properties.\u003c/p\u003e\n\u003cp\u003e$$\nP(x, y) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\gamma \\delta_{x_i}(x) \\delta_{y_i}(y) + (1-\\gamma) P_{x_i, y_i}(x, y) \\right) \\tag{4}\n$$\u003c/p\u003e\n\u003cp\u003eHere, $\\gamma$ is the hyperparameter controlling the mixup between the empirical and vicinal distribution. In fact, we see that the distribution $P(x, y)$ for RegMixup is a convex combination of the empirical distribution (left term of the addition in equation 4) and the vicinal distribution defined with equations (2) and (3).\u003c/p\u003e\n\u003cp\u003eFrom there, we can define a new loss function $\\mathcal{L}$ based on the Cross Entropy Loss ($\\text{CE}$)\u003c/p\u003e\n\u003cp\u003e$$\n\\mathcal{L}(\\hat{y}, y) = \\text{CE}(p_\\theta(\\hat{y} \\vert x), y) + \\eta \\text{CE}(p_\\theta(\\hat{y} \\vert \\tilde{x}), \\tilde{y}) \\tag{5}\n$$\u003c/p\u003e\n\u003cp\u003eWith $ \\eta \\in R_{+}^{\\ast} $ being the hyperparameter controlling the importance of the vicinal cross entropy sub-loss and $p_\\theta$ the activation function of the model parameterized by $\\theta$. In the paper, the value of $\\eta$ is set to 1 and its variation seem negligible. Consequently, we will not focus on it in this blog post.\u003c/p\u003e\n\u003cp\u003eSuch a model (equation 4) exhibits properties that lacked in Mixup :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eValues of $\\alpha$ and underconfidence :\u003c/strong\u003e As we explicitly add the empirical distribution to the vicinal one, the ERM term will encourage the model to predict the true labels of the training set while the VRM term, motivated by the interpolation factor $\\lambda$, will explore the vicinal distribution space in a much more thorough way than what was possible with Mixup. For instance, if λ $\\approx$ 0.5, a wide variety of images containing features from both the images in the pair are obtained (look at the \u003cstrong\u003e\u003ca href=\"#my-fig\"\u003efigure\u003c/a\u003e\u003c/strong\u003e). Consequently, the ERM term allows to better predict in-distribution samples while the VRM term, with a larger $\\alpha$, will allow to better predict OOD samples. This is a very interesting property as it allows to have a model that is both confident and accurate.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePrediction entropy :\u003c/strong\u003e Through their experiments and observations, researchers found that a cross-validated value of $\\alpha$ leads to a maximum likelihood estimation having high entropy for ODD samples only. While Mixup demonstrated high entropy for both ID and OOD samples, RegMixup is able to differentiate between the two. This is an highly desirable properties indicating us that RegMixup acts as a \u003cstrong\u003eregularizer\u003c/strong\u003e in essense.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs a preliminary conclusion, RegMixup is a very powerful, cost-efficient and simple-to-implement regularizer that allows to improve the robustness and accuracy of deep neural networks for both in-distribution and out-of-distribution samples. In the next section, we will see how to use RegMixup in practice trough a toy example.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003e3. RegMixup in practice (implementation)\u003c/h2\u003e\n\u003cp\u003eNow, our objective will be to demonstrate the effectiveness of RegMixup through a very simple example. We will use the CIFAR-10-C dataset (corrupted version of CIFAR-10) and a standard ResNet-18 model. We will compare performances of 3 models :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA baseline model trained with ERM\u003c/li\u003e\n\u003cli\u003eA model trained with Mixup\u003c/li\u003e\n\u003cli\u003eA model trained with RegMixup\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo do so, we have two possibilities :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUse the official implementation of RegMixup available on \u003ca href=\"https://github.com/FrancescoPinto/RegMixup\"\u003eFrancesco Pinto's GitHub\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eUse the torch-uncertainty library which provides a simple and efficient way to use RegMixup. Note, the library is developed by researchers from ENSTA Paris and is available on \u003ca href=\"https://github.com/ENSTA-U2IS-AI/torch-uncertainty\"\u003eGitHub\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn this blog post, we will use the torch-uncertainty library as it is very simple to use and provides a very well-implemented version of RegMixup.\u003c/p\u003e\n\u003ch3 id=\"31-installation\"\u003e3.1. Installation\u003c/h3\u003e\n\u003cp\u003eFirst, we need to install the torch-uncertainty library. To do so, we can use pip :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip install torch-uncertainty\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNote: If you use a gpu, torch-uncertainty will automatically install a cpu version of torch and torchvision, you can compile the following lines to install the gpu version of torch and torchvision (took from \u003ca href=\"https://pytorch.org/get-started/locally/\"\u003ePyTorch website\u003c/a\u003e) :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip unistall torch torchvision\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTo check if the installation was successful, you can run the following code, it should return True if you have a gpu and False if you don\u0026rsquo;t have one :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eprint\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecuda\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eis_available\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"32-training-the-models-with-torch-uncertainty\"\u003e3.2. Training the models with torch-uncertainty\u003c/h3\u003e\n\u003cp\u003eNow that we have installed torch-uncertainty, we can train the models. First, we need to import the necessary libraries :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch_uncertainty\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_main\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003einit_args\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch_uncertainty.baselines.classification\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch_uncertainty.optimization_procedures\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eoptim_cifar10_resnet18\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch_uncertainty.datamodules\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eCIFAR10DataModule\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorchvision.datasets\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eCIFAR10\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorchvision\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etransforms\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch.nn\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eCrossEntropyLoss\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003etorch\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eos\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003epathlib\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ePath\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_test_helpers\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eArgvContext\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThen, we can define the 3 models we discussed earlier :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003ebaseline\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enum_classes\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eloss\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eCrossEntropyLoss\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eoptimization_procedure\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eoptim_cifar10_resnet18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eversion\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;std\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ein_channels\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003earch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecuda\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003emixup\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enum_classes\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eloss\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eCrossEntropyLoss\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eoptimization_procedure\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eoptim_cifar10_resnet18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eversion\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;std\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ein_channels\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003earch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003emixup\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#00a8c8\"\u003eTrue\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003emixup_alpha\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.2\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecuda\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eregmixup\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enum_classes\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eloss\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eCrossEntropyLoss\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eoptimization_procedure\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eoptim_cifar10_resnet18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003eversion\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;std\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ein_channels\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003earch\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e18\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003ereg_mixup\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#00a8c8\"\u003eTrue\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#111\"\u003emixup_alpha\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e15\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecuda\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eBefore training the models, we need to define important arguments such as training parameters (epochs, estimators, etc.) and the datamodule. We can do so with the following code:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ePath\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eos\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003epath\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eabspath\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# We mock the arguments for the trainer\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#00a8c8\"\u003ewith\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eArgvContext\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;file.py\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;--max_epochs\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;20\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;--enable_progress_bar\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;False\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;--num_estimators\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#d88200\"\u003e\u0026#34;8\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003einit_args\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003enetwork\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eResNet\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edatamodule\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eCIFAR10DataModule\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003enet_name\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#d88200\"\u003e\u0026#34;logs/reset18-cifar10\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# datamodule\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003estr\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e \u003cspan style=\"color:#d88200\"\u003e\u0026#34;data\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003edm\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eCIFAR10DataModule\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003e\u003cspan style=\"color:#111\"\u003evars\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eFinally, we can train the models using the \u003ccode\u003ecli_main\u003c/code\u003e function from torch-uncertainty :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eresults_baseline\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_main\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ebaseline\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edm\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enet_name\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eresults_mixup\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_main\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003emixup\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edm\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enet_name\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#111\"\u003eresults_regmixup\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003ecli_main\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eregmixup\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003edm\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eroot\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003enet_name\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNote: If you have a gpu, you can make a slight modification to the code to use it :\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eClick on \u003ccode\u003ecli_main\u003c/code\u003e and press \u003ccode\u003eF12\u003c/code\u003e to go to the function definition.\u003c/li\u003e\n\u003cli\u003eGo to line 222 and replace the trainer definition by the following one :\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# trainer\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003etrainer\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#111\"\u003epl\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eTrainer\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003efrom_argparse_args\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003eaccelerator\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#d88200\"\u003e\u0026#34;gpu\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003edevices\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003ecallbacks\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003ecallbacks\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003elogger\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003etb_logger\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003edeterministic\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eseed\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eis\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e \u003cspan style=\"color:#00a8c8\"\u003eNone\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#111\"\u003einference_mode\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e \u003cspan style=\"color:#111\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eopt_temp_scaling\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eor\u003c/span\u003e \u003cspan style=\"color:#111\"\u003eargs\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#111\"\u003eval_temp_scaling\u003c/span\u003e\u003cspan style=\"color:#111\"\u003e),\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#111\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"3\"\u003e\n\u003cli\u003eSave the file and you are all set.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"33-results\"\u003e3.3. Results\u003c/h3\u003e\n\u003cp\u003eSo as to compare the performances of the 3 models, we use two corrupted versions of Cifar-10-C. The first version has a corruption severity factor of 5 (slight data corruption) and the second one has a corruption severity factor of 15 (more severe data corruption). Our study contains 5 metrics : entropy, accuracy, brier score, expected calibration error (ECE) and negative log-likelihood (NLL). In our explanation, we will focus on the accuracy and entropy to keep it simple.\u003c/p\u003e\n\u003cp\u003eWith corruption severity factor of 5, we obtain the following results :\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eentropy\u003c/th\u003e\n\u003cth\u003eaccuracy\u003c/th\u003e\n\u003cth\u003ebrier\u003c/th\u003e\n\u003cth\u003eece\u003c/th\u003e\n\u003cth\u003enll\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003ebaseline\u003c/td\u003e\n\u003ctd\u003e0.656294\u003c/td\u003e\n\u003ctd\u003e0.7480\u003c/td\u003e\n\u003ctd\u003e0.349862\u003c/td\u003e\n\u003ctd\u003e0.032466\u003c/td\u003e\n\u003ctd\u003e0.729336\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003emixup\u003c/td\u003e\n\u003ctd\u003e0.640811\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003e0.7578\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e0.335403\u003c/td\u003e\n\u003ctd\u003e0.024429\u003c/td\u003e\n\u003ctd\u003e0.703844\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eregmixup\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003e0.676174\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e0.7564\u003c/td\u003e\n\u003ctd\u003e0.340233\u003c/td\u003e\n\u003ctd\u003e0.023135\u003c/td\u003e\n\u003ctd\u003e0.711405\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eFirst of all, we can see that the accuracy is quite similar for the 3 models. This makes sense as the corruption severity factor is quite low, thus cifar-10-c is not very different from the original cifar-10. However, we can see that the entropy of the RegMixup model is higher than the one of the Mixup model. This is symptomatic of Mixup\u0026rsquo;s underconfidence. As stated previously, given the low corruption severity factor of cifar-10-c, the underconfidence of Mixup does not impact its performances in a visible manner.\u003c/p\u003e\n\u003cp\u003eWith corruption severity factor of 15, we obtain the following results :\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eentropy\u003c/th\u003e\n\u003cth\u003eaccuracy\u003c/th\u003e\n\u003cth\u003ebrier\u003c/th\u003e\n\u003cth\u003eece\u003c/th\u003e\n\u003cth\u003enll\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003ebaseline\u003c/td\u003e\n\u003ctd\u003e0.615607\u003c/td\u003e\n\u003ctd\u003e0.7402\u003c/td\u003e\n\u003ctd\u003e0.358522\u003c/td\u003e\n\u003ctd\u003e0.048414\u003c/td\u003e\n\u003ctd\u003e0.750933\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003emixup\u003c/td\u003e\n\u003ctd\u003e0.698558\u003c/td\u003e\n\u003ctd\u003e0.7558\u003c/td\u003e\n\u003ctd\u003e0.338540\u003c/td\u003e\n\u003ctd\u003e0.014760\u003c/td\u003e\n\u003ctd\u003e0.709190\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eregmixup\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003e0.702599\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003e0.7614\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e0.327945\u003c/td\u003e\n\u003ctd\u003e0.008439\u003c/td\u003e\n\u003ctd\u003e0.687550\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eHere the results are much more unequivocal. As the severity factor increases, the baseline model drops in accuracy and entropy, Mixup also drops in accuracy but increases in entropy and RegMixup increases in accuracy and entropy. Here, RegMixup has the higher entropy as the model has higher entropy for OOD samples which are more frequent at this corruption level. Mixup shows a greater delta increase in entropy due to its higher predictive entropy tendency whether or not samples are OOD or ID. Consequently, RegMixup is more confident and accurate than the Mixup model eventhough Mixup is not fully underperforming.\u003c/p\u003e\n\u003ch2 id=\"section-4\"\u003e4. Conclusion\u003c/h2\u003e\n\u003cp\u003eAs a conclusion, we have seen that RegMixup is a powerful method to regularize deep neural networks. Despite being very simple and cost-effective, it is important to specify that the paper does not provide a theoretical explanation of the method. These experimental grounds are very promising but it appears important to stay cautious while utilizing RegMixup.\u003c/p\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/robustai_regmixup/","date_published":"24036-24-09T338:2424:00+01:00","date_modified":"24036-24-09T338:2424:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"9bec8a0b2d12c702729b80a3b910f16d3d73250f","title":"Adversarially Reweighted Learning","summary":"","content_text":"Fairness without Demographics through Adversarially Reweighted Learning Authors: Pierre Fihey \u0026 Guerlain Messin Table of Contents Fairness issues in ML and AI The privacy of demographic’s data The Adversarial Reweighted Learning Model An Hypothesis: Protected Groups are Correlated with Both Features and Labels Computational identifiability of protected groups The Rawlsian Max-Min Fairness principle The ARL objective The Model Architecture Results analysis Conclusion Fairness issues in ML and AI As Machine Learning and Artificial Intelligence algorithms are increasingly developed to aid and automate decision-making, it is crucial that they provide ethical, fair and discrimination-free results. However, discriminative biases are now found in many facets of AI and ML and affect many possible applications.\nSuch biases can be found in NLP applications, where we can see that generative AIs often associate certain genders or ethnic groups with professions. In computer vision, the lack of diversity in the training data also induces numerous discriminatory biases, since we can see that the algorithms\u0026rsquo; performances differ according to age, gender and ethnic group, which can lead to unfair treatments. Machine Learning models, used in decision-making processes from loan approvals to job applications, can inherit historical biases present in their training data, resulting in unfair outcomes.\nThe root of these biases lies in the historical prejudices and inequalities that are inadvertently encoded into the datasets used to train AI and ML models. These datasets often reflect the societal, cultural, and institutional biases that have existed over time. As a result, when AI and ML technologies are trained on such data, they risk mirroring and amplifying these biases instead of offering neutral, objective outputs. It is therefore vital to focus on AI fairness to enable the development of technologies that will benefit everyone fairly and equitably.\nThe privacy of demographic’s data Strict regulations established by laws such as the General Data Protection Regulation (GDPR) severely restrict the collection of demographic data, including age, gender, religion and other personal attributes. This legal framework, designed to protect individual privacy and data rights, poses a problem for the study of discriminatory bias in algorithms, since it becomes almost impossible to measure. This situation creates a real paradox, since protecting personal data conflicts with limiting discrimination and promoting fairness for ML and iA algorithms.\nIn this blog, we\u0026rsquo;ll look at the paper Fairness without Demographics through Adversarially Reweighted Learning, published by Google\u0026rsquo;s 2020 research team to propose a method for improving the fairness of AI models despite the lack of demographic data. Indeed, while much previous works have focused on improving fairness in AI and ML, most of these works assume that models have access to this protected data. Given the observations made above, the problem this paper attempts to address is as follows: How can we train a ML model to improve fairness when we do not have access to protected features neither at training nor inference time, i.e., we do not know protected group memberships?\nThe Adversarial Reweighted Learning Model An Hypothesis: Protected Groups are Correlated with Both Features and Labels While access to the protected features is often impossible, the authors of this paper assume that there is a strong correlation between these variables and the observable features X as well as the class labels Y. Although these correlations are the cause of the fairness problems faced by ML algorithms, they represent a real advantage here, as they can help to identify these protected groups and thus to evaluate and correct possible discrimination biases.\nThe authors have shown that this hypothesis is frequently verified. For example, they were able to predict the race and gender of individuals in the Adults and LSAC Datasets with high accuracy from unprotected features and labels.\nThis assumption therefore implies that protected groups can be computationally identifiable. It is on this notion of computational identifiability that the model proposed by Google\u0026rsquo;s research team is based to outperform previous work.\nComputational identifiability of protected groups Computational identifiability refers to the ability to algorithmically identify specific subgroups or patterns within a dataset based on certain criteria, using computable functions. Mathematically, this notion is defined as follows:\nFor a family of binary functions $F$, we say that a subgroup $S$ is computationally-identifiable if there is a function $f : X \\times Y \\rightarrow \\text{{0, 1}}$ in $F$ such that $f(x, y) = 1$ if and only if $(x, y) \\in S$.\nThis function typically maps input data to a binary outcome, indicating protected subgroup membership. While many previous works have used this principle of computational identifiability, the model presented in this article differs in that it does not require these subgroups to be present in the input space, but also in its objective. While most work has focused on reducing the efficiency gap between each subgroup, the ARL model aims to increase efficiency for these subgroups, while considering that this should not be at the expense of the other groups. Indeed, the authors have decided to follow the Rawlsian Max Min fairness principle, which we present below.\nThe Rawlsian Max-Min Fairness principle In philosophy, the Rawlsian Max Min principle of distributive justice is defined by John Rawls as maximizing the welfare of the most disadvantaged member of society. In a mathematical context, this can be translated as maximizing the minimum utility U a model has across all groups s ∈ S. We adopt the following definition:\nDefinition (Rawslan Max-Min Fairness): Suppose $H$ is a set of hypotheses, and $U_{D_s}(h)$ is the expected utility of the hypothesis $h$ for the individuals in group $s$, then a hypothesis $h^* $ is said to satisfy Rawlsian Max-Min fairness principle if it maximizes the utility of the worst-off group, i.e., the group with the lowest utility. $$h^* = argmax_{h \\in H} min_{s \\in S} U_{D_s}(h)$$\nThe Maxmin Rawlsian principle inherently accepts the existence of inequalities, as its core aim is not to ensure uniform outcomes across all groups but rather to maximize the overall utility, particularly focusing on enhancing the welfare of the least advantaged. This is what will enable our model to obtain truly relevant results, and we\u0026rsquo;ll now see how it adapts this principle to define a loss function to be minimized during training.\nThe ARL objective To adapt this Rawlsian principle to a Machine Learning task, the authors decided to set up a MinMax Problem. A minmax algorithm is a mathematical problem defined in game theory. Its aim is to optimize the worst possible scenario for a player, assuming that the opponent plays optimally. The aim is now to minimize the highest loss, i.e. the loss of the most disadvantaged protected group. This new objective function is defined as follows:\n$$J(\\theta, \\lambda) := min_{\\theta} max_{\\lambda} \\sum_{s \\in S} \\lambda_s L_{D_s}(h)$$ $$= min_{\\theta} max_{\\lambda} \\sum_{i=0}^{n} \\lambda_{s_i} l(h(x_i), y_i)$$\nWith $l(.,.)$ the cross-entropy loss and lambda the weights that maximize the weighted loss of protected groups. To solve this minmax problem, the authors set up a special architecture consisting of two neural networks, a learner and an adversary.\nThe Model Architecture As previously announced, the authors therefore decided to implement the Adversarial Reweighted Learning (ARL) approach, training two models alternately.\nThe learner optimizes for the main classification task, and aims to learn the best parameters θ that minimizes expected loss.\nThe adversary learns a function mapping $f_\\phi : X \\times Y \\rightarrow [0, 1]$ to computationally-identifiable regions with high loss, and makes an adversarial assignment of weight vector $\\lambda_\\phi : f_\\phi \\rightarrow \\mathbb{R}$ so as to maximize the expected loss.\nThe learner then adjusts itself to minimize the adversarial loss: $$J(\\theta, \\phi) = min_{\\theta} max_{\\phi} \\sum_{i=1}^{n} \\lambda_{\\phi}(x_i, y_i) \\cdot l_{ce}(h_\\theta(x_i), y_i)$$\nTo ensure that the loss function is well defined, it\u0026rsquo;s crucial to introduce specific constraints on the weights used in the loss function. Ensuring these weights are non-negative, prevent zero values to include all training examples, and are normalized, addresses potential instability and promotes uniform contribution across the dataset.\n$$\\lambda_{\\phi}(x_i, y_i) = 1 + n \\cdot \\frac{f_{\\phi}(x_i, y_i)}{\\sum_{i=1}^{n} f_{\\phi}(x_i, y_i)}$$\nThe authors have implemented these two networks using standard feed-forward network. The learner is a fully connected two-layer feed-forward network with 64 and 32 hidden units in the hidden layers, with ReLU activation function. For small datasets, the adversary which performs the best is a linear model.\nResults analysis This section provides a detailed examination of the results obtained from our implementation of the Adversarial Reweighted Learning (ARL) model. We replicate the experiments conducted by Lahoti et al. and present the outcomes of our implementation. Furthermore, we analyze the significance of the results through a comprehensive evaluation.\nReproducibility We first reproduce the results reported by Lahoti et al. using their TensorFlow implementation. However, due to the absence of optimal hyperparameters, we utilize default parameters for our runs. As a result, our AUC scores are lower than those reported in the original paper. For instance, the average AUC for the Adult dataset in Lahoti et al.\u0026rsquo;s work is 0.907, whereas our run yields an AUC of 0.497. Similarly, for the LSAC dataset, Lahoti et al. report an AUC of 0.823, whereas we obtain 0.518. The COMPAS dataset also exhibits a similar trend, with Lahoti et al. reporting an AUC of 0.748, compared to our result of 0.536. Subsequent experimentation with optimal parameters from TensorFlow implementation demonstrates improved performance, although AUC scores remain lower than those presented in the original paper.\nReplicability We replicate the experiments using our PyTorch implementation of the ARL model with optimal hyperparameters obtained through grid-search. Comparing the AUC scores with Lahoti et al.\u0026rsquo;s results reveals close alignment for the Adult and LSAC datasets. However, a slightly larger difference is observed for the COMPAS dataset. Notably, all AUC metrics for the COMPAS dataset are lower than the baseline model presented by Lahoti et al. This discrepancy suggests potential challenges with dataset size, leading to increased variance in results. Nonetheless, our PyTorch implementation demonstrates consistency with Lahoti et al.\u0026rsquo;s findings, highlighting the robustness of the ARL model across different implementations.\nSignificance Evaluation We conduct significance tests to evaluate the performance improvement of our PyTorch-implemented ARL model compared to a simple baseline model. Despite observing notable improvements in fairness metrics, none of the p-values obtained are less than 0.05. Consequently, according to established significance criteria, the performance enhancement achieved by our ARL model is not statistically significant. This finding underscores the need for further investigation into the efficacy of adversarial learning methods in enhancing fairness without demographic information.\nConclusion In this study, we critically examined the paper \u0026ldquo;Fairness without Demographics through Adversarially Reweighted Learning\u0026rdquo; by Lahoti et al., focusing on reproducibility, replicability, and the significance of reported results. While encountering challenges in reproducing Lahoti et al.\u0026rsquo;s results due to parameter settings and dataset characteristics, we successfully replicated the experiments using our PyTorch implementation. Despite demonstrating consistency with the original findings, our significance tests indicate a lack of statistical significance in the performance improvement achieved by the ARL model. This prompts further inquiry into the suitability of adversarial learning approaches for addressing fairness concerns in machine learning without relying on demographic data.\nAnnexes References [1] Lahoti, P., Beutel, A., Chen, J., Lee, K., Prost, F., Thain, N., Wang, X., \u0026amp; Chi, E. H. (2020). Fairness without demographics through adversarially reweighted learning. arXiv preprint arXiv:2006.13114.\n[2] Veale, M., \u0026amp; Binns, R. (2017). Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data. Big Data \u0026amp; Society, 4(2), 2053951717743530.\n[3] Hanley, J. A., \u0026amp; McNeil, B. J. (1982). The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology, 143(1), 29-36.\n[4] Hanley, J. A., \u0026amp; McNeil, B. J. (1983). A method of comparing the areas under receiver operating characteristic curves derived from the same cases. Radiology, 148(3), 839-843.\n[5] Dua, D., \u0026amp; Graff, C. (2019). UCI machine learning repository.\n[6] Kim, M. P., Ghorbani, A., \u0026amp; Zou, J. (2019). Multiaccuracy: Black-box post-processing for fairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (pp. 247-254).\n[7] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., \u0026hellip; \u0026amp; Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27, 2672-2680.\n[8] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., \u0026hellip; \u0026amp; Chintala, S. (2019). PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32 (pp. 8024-8035).\n[9] Kamishima, T., Akaho, S., \u0026amp; Sakuma, J. (2011). Fairness-aware learning through regularization approach. In 2011 IEEE 11th International Conference on Data Mining Workshops (pp. 643-650). IEEE.\n","content_html":"\u003ch1 style=\"font-size: 36px;\"\u003eFairness without Demographics through Adversarially Reweighted Learning\u003c/h1\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eAuthors: Pierre Fihey \u0026 Guerlain Messin\u003c/h1\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eFairness issues in ML and AI\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eThe privacy of demographic’s data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eThe Adversarial Reweighted Learning Model\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eAn Hypothesis: Protected Groups are Correlated with Both Features and Labels\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eComputational identifiability of protected groups\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eThe Rawlsian Max-Min Fairness principle\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eThe ARL objective\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-7\"\u003eThe Model Architecture\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-8\"\u003eResults analysis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-9\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0\"\u003eFairness issues in ML and AI\u003c/h2\u003e\n\u003cp\u003eAs Machine Learning and Artificial Intelligence algorithms are increasingly developed to aid and automate decision-making, it is crucial that they provide ethical, fair and discrimination-free results. However, discriminative biases are now found in many facets of AI and ML and affect many possible applications.\u003c/p\u003e\n\u003cp\u003eSuch biases can be found in NLP applications, where we can see that generative AIs often associate certain genders or ethnic groups with professions. In computer vision, the lack of diversity in the training data also induces numerous discriminatory biases, since we can see that the algorithms\u0026rsquo; performances differ according to age, gender and ethnic group, which can lead to unfair treatments.\nMachine Learning models, used in decision-making processes from loan approvals to job applications, can inherit historical biases present in their training data, resulting in unfair outcomes.\u003c/p\u003e\n\u003cp\u003eThe root of these biases lies in the historical prejudices and inequalities that are inadvertently encoded into the datasets used to train AI and ML models. These datasets often reflect the societal, cultural, and institutional biases that have existed over time. As a result, when AI and ML technologies are trained on such data, they risk mirroring and amplifying these biases instead of offering neutral, objective outputs. It is therefore vital to focus on AI fairness to enable the development of technologies that will benefit everyone fairly and equitably.\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eThe privacy of demographic’s data\u003c/h2\u003e\n\u003cp\u003eStrict regulations established by laws such as the General Data Protection Regulation (GDPR) severely restrict the collection of demographic data, including age, gender, religion and other personal attributes. This legal framework, designed to protect individual privacy and data rights, poses a problem for the study of discriminatory bias in algorithms, since it becomes almost impossible to measure. This situation creates a real paradox, since protecting personal data conflicts with limiting discrimination and promoting fairness for ML and iA algorithms.\u003c/p\u003e\n\u003cp\u003eIn this blog, we\u0026rsquo;ll look at the paper Fairness without Demographics through Adversarially Reweighted Learning, published by Google\u0026rsquo;s 2020 research team to propose a method for improving the fairness of AI models despite the lack of demographic data. Indeed, while much previous works have focused on improving fairness in AI and ML, most of these works assume that models have access to this protected data. Given the observations made above, the problem this paper attempts to address is as follows: How can we train a ML model to improve fairness when we do not have access to protected features neither at training nor inference time, i.e., we do not know protected group memberships?\u003c/p\u003e\n\u003ch2 id=\"section-2\"\u003eThe Adversarial Reweighted Learning Model\u003c/h2\u003e\n\u003ch3 id=\"section-3\"\u003eAn Hypothesis: Protected Groups are Correlated with Both Features and Labels\u003c/h3\u003e\n\u003cp\u003eWhile access to the protected features is often impossible, the authors of this paper assume that there is a strong correlation between these variables and the observable features X as well as the class labels Y. Although these correlations are the cause of the fairness problems faced by ML algorithms, they represent a real advantage here, as they can help to identify these protected groups and thus to evaluate and correct possible discrimination biases.\u003c/p\u003e\n\u003cp\u003eThe authors have shown that this hypothesis is frequently verified. For example, they were able to predict the race and gender of individuals in the Adults and LSAC Datasets with high accuracy from unprotected features and labels.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Fihey_Messin/Identifying_Groups.png\"\n  alt=\"Identifying Groups\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThis assumption therefore implies that protected groups can be computationally identifiable. It is on this notion of computational identifiability that the model proposed by Google\u0026rsquo;s research team is based to outperform previous work.\u003c/p\u003e\n\u003ch3 id=\"section-4\"\u003eComputational identifiability of protected groups\u003c/h3\u003e\n\u003cp\u003eComputational identifiability refers to the ability to algorithmically identify specific subgroups or patterns within a dataset based on certain criteria, using computable functions. Mathematically, this notion is defined as follows:\u003c/p\u003e\n\u003cp\u003eFor a family of binary functions $F$, we say that a subgroup $S$ is computationally-identifiable if there is a function $f : X \\times Y \\rightarrow \\text{{0, 1}}$ in $F$ such that $f(x, y) = 1$ if and only if $(x, y) \\in S$.\u003c/p\u003e\n\u003cp\u003eThis function typically maps input data to a binary outcome, indicating protected subgroup membership. While many previous works have used this principle of computational identifiability, the model presented in this article differs in that it does not require these subgroups to be present in the input space, but also in its objective. While most work has focused on reducing the efficiency gap between each subgroup, the ARL model aims to increase efficiency for these subgroups, while considering that this should not be at the expense of the other groups. Indeed, the authors have decided to follow the Rawlsian Max Min fairness principle, which we present below.\u003c/p\u003e\n\u003ch3 id=\"section-5\"\u003eThe Rawlsian Max-Min Fairness principle\u003c/h3\u003e\n\u003cp\u003eIn philosophy, the Rawlsian Max Min principle of distributive justice is defined by John Rawls as maximizing the welfare of the most disadvantaged member of society. In a mathematical context, this can be translated as maximizing the minimum utility U a model has across all groups s ∈ S. We adopt the following definition:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDefinition (Rawslan Max-Min Fairness):\u003c/strong\u003e Suppose $H$ is a set of hypotheses, and $U_{D_s}(h)$ is the expected utility of the hypothesis $h$ for the individuals in group $s$, then a hypothesis $h^* $ is said to satisfy Rawlsian Max-Min fairness principle if it maximizes the utility of the worst-off group, i.e., the group with the lowest utility.\n$$h^* = argmax_{h \\in H} min_{s \\in S} U_{D_s}(h)$$\u003c/p\u003e\n\u003cp\u003eThe Maxmin Rawlsian principle inherently accepts the existence of inequalities, as its core aim is not to ensure uniform outcomes across all groups but rather to maximize the overall utility, particularly focusing on enhancing the welfare of the least advantaged.  This is what will enable our model to obtain truly relevant results, and we\u0026rsquo;ll now see how it adapts this principle to define a loss function to be minimized during training.\u003c/p\u003e\n\u003ch3 id=\"section-6\"\u003eThe ARL objective\u003c/h3\u003e\n\u003cp\u003eTo adapt this Rawlsian principle to a Machine Learning task, the authors decided to set up a MinMax Problem. A minmax algorithm is a mathematical problem defined in game theory. Its aim is to optimize the worst possible scenario for a player, assuming that the opponent plays optimally.\nThe aim is now to minimize the highest loss, i.e. the loss of the most disadvantaged protected group. This new objective function is defined as follows:\u003c/p\u003e\n\u003cp\u003e$$J(\\theta, \\lambda) := min_{\\theta} max_{\\lambda} \\sum_{s \\in S} \\lambda_s L_{D_s}(h)$$\n$$= min_{\\theta} max_{\\lambda} \\sum_{i=0}^{n} \\lambda_{s_i} l(h(x_i), y_i)$$\u003c/p\u003e\n\u003cp\u003eWith $l(.,.)$ the cross-entropy loss and lambda the weights that maximize the weighted loss of protected groups. To solve this minmax problem, the authors set up a special architecture consisting of two neural networks, a learner and an adversary.\u003c/p\u003e\n\u003ch3 id=\"section-7\"\u003eThe Model Architecture\u003c/h3\u003e\n\u003cp\u003eAs previously announced, the authors therefore decided to implement the Adversarial Reweighted Learning (ARL) approach, training two models alternately.\u003c/p\u003e\n\u003cp\u003eThe learner optimizes for the main classification task, and aims to learn the best parameters θ that minimizes expected loss.\u003c/p\u003e\n\u003cp\u003eThe adversary learns a function mapping $f_\\phi : X \\times Y \\rightarrow [0, 1]$ to computationally-identifiable regions with high loss, and makes an adversarial assignment of weight vector $\\lambda_\\phi : f_\\phi \\rightarrow \\mathbb{R}$ so as to maximize the expected loss.\u003c/p\u003e\n\u003cp\u003eThe learner then adjusts itself to minimize the adversarial loss:\n$$J(\\theta, \\phi) = min_{\\theta} max_{\\phi} \\sum_{i=1}^{n} \\lambda_{\\phi}(x_i, y_i) \\cdot l_{ce}(h_\\theta(x_i), y_i)$$\u003c/p\u003e\n\u003cp\u003eTo ensure that the loss function is well defined, it\u0026rsquo;s crucial to introduce specific constraints on the weights used in the loss function. Ensuring these weights are non-negative, prevent zero values to include all training examples, and are normalized, addresses potential instability and promotes uniform contribution across the dataset.\u003c/p\u003e\n\u003cp\u003e$$\\lambda_{\\phi}(x_i, y_i) = 1 + n \\cdot \\frac{f_{\\phi}(x_i, y_i)}{\\sum_{i=1}^{n} f_{\\phi}(x_i, y_i)}$$\u003c/p\u003e\n\u003cp\u003eThe authors have implemented these two networks using standard feed-forward network. The learner is a fully connected two-layer feed-forward network with 64 and 32 hidden units in the hidden layers, with ReLU activation function. For small datasets, the adversary which performs the best is a linear model.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Fihey_Messin/ARL_Computational_Graph.png\"\n  alt=\"ARL Computational Graph\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003ch2 id=\"section-8\"\u003eResults analysis\u003c/h2\u003e\n\u003cp\u003eThis section provides a detailed examination of the results obtained from our implementation of the Adversarial Reweighted Learning (ARL) model. We replicate the experiments conducted by Lahoti et al. and present the outcomes of our implementation. Furthermore, we analyze the significance of the results through a comprehensive evaluation.\u003c/p\u003e\n\u003ch3 id=\"reproducibility\"\u003eReproducibility\u003c/h3\u003e\n\u003cp\u003eWe first reproduce the results reported by Lahoti et al. using their TensorFlow implementation. However, due to the absence of optimal hyperparameters, we utilize default parameters for our runs. As a result, our AUC scores are lower than those reported in the original paper. For instance, the average AUC for the Adult dataset in Lahoti et al.\u0026rsquo;s work is 0.907, whereas our run yields an AUC of 0.497. Similarly, for the LSAC dataset, Lahoti et al. report an AUC of 0.823, whereas we obtain 0.518. The COMPAS dataset also exhibits a similar trend, with Lahoti et al. reporting an AUC of 0.748, compared to our result of 0.536. Subsequent experimentation with optimal parameters from TensorFlow implementation demonstrates improved performance, although AUC scores remain lower than those presented in the original paper.\u003c/p\u003e\n\u003ch3 id=\"replicability\"\u003eReplicability\u003c/h3\u003e\n\u003cp\u003eWe replicate the experiments using our PyTorch implementation of the ARL model with optimal hyperparameters obtained through grid-search. Comparing the AUC scores with Lahoti et al.\u0026rsquo;s results reveals close alignment for the Adult and LSAC datasets. However, a slightly larger difference is observed for the COMPAS dataset. Notably, all AUC metrics for the COMPAS dataset are lower than the baseline model presented by Lahoti et al. This discrepancy suggests potential challenges with dataset size, leading to increased variance in results. Nonetheless, our PyTorch implementation demonstrates consistency with Lahoti et al.\u0026rsquo;s findings, highlighting the robustness of the ARL model across different implementations.\u003c/p\u003e\n\u003ch3 id=\"significance-evaluation\"\u003eSignificance Evaluation\u003c/h3\u003e\n\u003cp\u003eWe conduct significance tests to evaluate the performance improvement of our PyTorch-implemented ARL model compared to a simple baseline model. Despite observing notable improvements in fairness metrics, none of the p-values obtained are less than 0.05. Consequently, according to established significance criteria, the performance enhancement achieved by our ARL model is not statistically significant. This finding underscores the need for further investigation into the efficacy of adversarial learning methods in enhancing fairness without demographic information.\u003c/p\u003e\n\u003ch2 id=\"section-9\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIn this study, we critically examined the paper \u0026ldquo;Fairness without Demographics through Adversarially Reweighted Learning\u0026rdquo; by Lahoti et al., focusing on reproducibility, replicability, and the significance of reported results. While encountering challenges in reproducing Lahoti et al.\u0026rsquo;s results due to parameter settings and dataset characteristics, we successfully replicated the experiments using our PyTorch implementation. Despite demonstrating consistency with the original findings, our significance tests indicate a lack of statistical significance in the performance improvement achieved by the ARL model. This prompts further inquiry into the suitability of adversarial learning approaches for addressing fairness concerns in machine learning without relying on demographic data.\u003c/p\u003e\n\u003chr\u003e\n\u003chr\u003e\n\u003ch2 id=\"annexes\"\u003eAnnexes\u003c/h2\u003e\n\u003ch3 id=\"references\"\u003eReferences\u003c/h3\u003e\n\u003cp\u003e[1] Lahoti, P., Beutel, A., Chen, J., Lee, K., Prost, F., Thain, N., Wang, X., \u0026amp; Chi, E. H. (2020). Fairness without demographics through adversarially reweighted learning. arXiv preprint arXiv:2006.13114.\u003c/p\u003e\n\u003cp\u003e[2] Veale, M., \u0026amp; Binns, R. (2017). Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data. Big Data \u0026amp; Society, 4(2), 2053951717743530.\u003c/p\u003e\n\u003cp\u003e[3] Hanley, J. A., \u0026amp; McNeil, B. J. (1982). The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology, 143(1), 29-36.\u003c/p\u003e\n\u003cp\u003e[4] Hanley, J. A., \u0026amp; McNeil, B. J. (1983). A method of comparing the areas under receiver operating characteristic curves derived from the same cases. Radiology, 148(3), 839-843.\u003c/p\u003e\n\u003cp\u003e[5] Dua, D., \u0026amp; Graff, C. (2019). UCI machine learning repository.\u003c/p\u003e\n\u003cp\u003e[6] Kim, M. P., Ghorbani, A., \u0026amp; Zou, J. (2019). Multiaccuracy: Black-box post-processing for fairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (pp. 247-254).\u003c/p\u003e\n\u003cp\u003e[7] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., \u0026hellip; \u0026amp; Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27, 2672-2680.\u003c/p\u003e\n\u003cp\u003e[8] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., \u0026hellip; \u0026amp; Chintala, S. (2019). PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32 (pp. 8024-8035).\u003c/p\u003e\n\u003cp\u003e[9] Kamishima, T., Akaho, S., \u0026amp; Sakuma, J. (2011). Fairness-aware learning through regularization approach. In 2011 IEEE 11th International Conference on Data Mining Workshops (pp. 643-650). IEEE.\u003c/p\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/adversarially_reweighted_learning/","date_published":"4036-04-09T335:44:00+01:00","date_modified":"4036-04-09T335:44:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"d7ac2f8435ec113c47081837ac2b76e60dcb8690","title":"Packed Ensembles","summary":"","content_text":" Introduction The document \"Packed-Ensembles for Efficient Uncertainty Estimation\" introduces a novel framework for designing and training compact, structured ensembles of neural networks, termed Packed-Ensembles (PE). It addresses the limitations of Deep Ensembles (DE) in terms of computational efficiency and hardware constraints by leveraging grouped convolutions. This technique allows for parallelizing the ensemble into a single shared backbone, improving training and inference speeds within the memory limits of standard neural networks. The paper demonstrates through extensive experiments that PEs maintain the beneficial properties of DEs, such as diversity and robustness to distribution shift, while achieving comparable accuracy, calibration, and out-of-distribution detection capabilities. The work includes implementation details, experimental results on CIFAR-10/100 and ImageNet datasets and comparisons with existing approaches. It concludes with insights on the reproducibility of results and the potential ethical considerations of deploying such models in safety-critical systems. Presentation of the model Packed-Ensembles\nThe base network and Packed-Ensembles\nPacked-Ensembles (PE) is a technique for designing and training lightweight ensembles of neural networks. It is based on the idea of using grouped convolutions to create multiple subnetworks within a single network. These subnetworks are trained independently, which helps to improve the efficiency of the ensemble.\nBenefits of Packed-Ensembles\nPacked-Ensembles offer several benefits over traditional ensemble methods, including:\nEfficiency: Packed-Ensembles are more efficient than traditional ensembles in terms of memory usage and training time. This is because they use grouped convolutions to share parameters between the subnetworks.\nAccuracy: Packed-Ensembles can achieve accuracy levels that are comparable to traditional ensembles.\nCalibration: Packed-Ensembles are well-calibrated, meaning that their predicted probabilities are accurate reflections of the true probabilities.\nOut-of-distribution (OOD) detection: Packed-Ensembles are good at detecting out-of-distribution data, which is data that comes from a different distribution than the data that the model was trained on.\nComparison to other ensemble methods\nThe paper compares Packed-Ensembles to several other ensemble methods, including Deep Ensembles, BatchEnsemble, MIMO, and Masksembles. The paper found that Packed-Ensembles are more efficient than all of these methods, and they achieve comparable accuracy on most tasks.\nPacked-Ensembles: A Technique for Efficient Neural Network Ensembles Packed-Ensembles (PE) is a method for designing and training lightweight ensembles of neural networks. It aims to improve efficiency while maintaining accuracy and other desirable properties. This technique achieves this by leveraging grouped convolutions to create multiple subnetworks within a single network, enabling them to be trained independently.\nUnderstanding Convolutional Layers and Grouped Convolutions:\nConvolutional Layers: These are the backbone of Convolutional Neural Networks (CNNs), performing filtering operations on input data using learnable filters (kernels). Mathematically, the output of a convolutional layer, denoted by $z_{j+1}$, is calculated as follows: $z^{(j+1)}(c,:,:) = (h^j \\otimes \\omega^j)(c,:,:) = \\sum_{k=0}^{C_{j}-1} \\omega^j(c, k,:,:) \\star h^j(k,:,:)$\nwhere:\n$c$ represents the channel index\n$h^j$ denotes the input feature map\n$ω^j$ represents the weight tensor (kernel)\n$⋆$ denotes the 2D cross-correlation operator\nGrouped Convolutions: This technique allows training multiple subnetworks within a single network by dividing the channels of feature maps and weight tensors into groups. Each group is processed by a separate set of filters, essentially creating independent subnetworks. The mathematical formulation for grouped convolutions is given by:\n$$ z^{(j+1)}(c,:,:) = \\left( h^j \\otimes \\omega^j_{\\gamma} \\right) (c,:,:) = \\sum_{k=0}^{\\frac{C_{j}}{\\gamma}-1} \\omega^j_{\\gamma} (c, k,:,:) \\star h^j \\left( k + \\left\\lfloor \\frac{c}{C_{j+1}/\\gamma} \\right\\rfloor \\frac{C_{j}}{\\gamma}, :,:\\right) $$\nwhere:\n$γ$ represents the number of groups $C_{j+1}$ and $C_j$ denote the number of output and input channels, respectively. The formula states that a grouped convolution layer is mathematically equivalent to a standard convolution where the weights are selectively applied using a binary mask $\\text{mask}_{m}^j$ $\\in \\{{ 0, 1 \\}}^{C_{j+1} \\times C_j \\times s_j^2}$ with $s_j^2$ the kernel size squared of the layer $j$. Each element in $\\text{mask}_{m}^j$ is either 0 or 1.\nThe condition $\\text{mask}_{m}^j(k, l, :, :) = 1$ happens only if $\\left\\lfloor \\frac{l}{C_{j}/\\gamma} \\right\\rfloor = \\left\\lfloor \\frac{k}{C_{j+1}/\\gamma} \\right\\rfloor$ for each group $m \\in [|0, \\gamma - 1 |]$\nComplete Mask and Convolution: $\\text{mask}^j = \\sum_{m=0}^{{\\gamma}-1}\\text{mask}_{m}^j$ : This combines the masks for all groups ($m$) into a single $\\text{mask}^j$ for layer $j$. $z^{j+1} = h^j \\otimes (ω^j ◦ \\text{mask}^j)$: This rewrites the grouped convolution operation. Here: $z^{j+1}$: Output feature map of the layer. $h^j$: Input feature map. $ω^j$: Convolution weights for layer j. $\\otimes$: Denotes convolution operation. $◦$: Denotes Hadamard product (element-wise multiplication). In simpler terms:\nGrouped convolution divides the input channels and weights into groups. A separate mask is created for each group, ensuring elements within a group are aligned. These masks effectively turn specific weights to zero during the convolution, essentially selecting which weights contribute to the output for each group. The final convolution is equivalent to applying the original weights element-wise multiplied by the combined mask. Background on Deep Ensembles This section delves into Deep Ensembles (DE), a technique for image classification tasks.\nDeep Ensembles\nSetting the Scene\nWe have a dataset $D$ containing pairs of images and their corresponding labels:\n$x_i$: Represents an image sample with dimensions $C0 \\times H0 \\times W0$ (likely referring to color channels, height, and width). $y_i$ : One-hot encoded label representing the class of the image ($NC$ total classes). The dataset is assumed to be drawn from a joint distribution $P(X, Y)$.\nA neural network $f_\\theta$ processes the images and predicts their class labels. This network has learnable parameters denoted by $\\theta$.\n$\\hat{y}_i = f_θ(xi)$: The predicted class label for image $x_i$ based on the network with parameters $θ$. Traditional Approach:\nThe model predicts probabilities for each class using a Multinoulli distribution. These probabilities are treated as point estimates, meaning they represent the most likely class without considering uncertainty.\nIntroducing Deep Ensembles\nDE works by training multiple Deep Neural Networks (DNNs) $M$ with random initializations. These DNNs are denoted by $θ_m$ for the $m-th$ network ($0$ to $M-1$).\nThe ensemble prediction is obtained by averaging the predictions of all $M$ DNNs as shown in the equation below:\n$$ P(y_i|x_i, D) = M^{-1} \\sum_{m=0}^{M-1} P(y_i|x_i, \\theta_m) $$\nThis essentially combines the outputs of multiple networks to create a more robust prediction.\nIn simpler terms, DE trains multiple neural networks with slight variations and combines their predictions to get a more reliable estimate, including the level of uncertainty in the prediction.\nBuilding Packed-Ensembles:\nPacked-Ensembles combine the concepts of Deep Ensembles (ensembles of multiple independent DNNs) and grouped convolutions. Here\u0026rsquo;s how it works:\nSubnetworks: The ensemble is formed by creating $M$ smaller subnetworks within the main network architecture. These subnetworks share the same structure but have independent parameters due to the use of grouped convolutions. Hyperparameters: Packed-Ensembles are defined by three hyperparameters: $α$ (alpha): expansion factor that scales the width of each subnetwork (compensates for the decrease in capacity due to using fewer parameters). $M$: number of subnetworks in the ensemble (represents the ensemble size). $γ$ (gamma): number of groups for grouped convolutions within each subnetwork (introduces another level of sparsity). Mathematical Implementation:\nThe output of a Packed-Ensemble layer is calculated by averaging the predictions from each subnetwork, as shown in the following equation:\n$$ \\hat{y} = M^{-1} \\sum_{m=0}^{M-1} P(y|\\theta_a^m, x) \\quad \\text{with} \\quad \\theta_a^m = ({\\omega_j^{\\alpha} \\circ \\text{mask}_{m}^j})_j $$\nwhere:\n$\\hat{y}$ represents the ensemble\u0026rsquo;s predicted label $P(y|θ_a^m, x)$ denotes the probability of class $y$ given the input $x$ and the parameters $θ_a^m$ of the $m-th$ subnetwork $\\theta_a^m = ({\\omega_j^{\\alpha} \\circ \\text{mask}_{m}^j})_j$ represents the parameters of the $m-th$ subnetwork, obtained by applying element-wise multiplication ($∘$) between the expanded weights ($\\omega_j^{\\alpha}$) and the group mask ($\\text{mask}_{m}$) for each layer $j$ Implementation\nEquivalent architectures for Packed-Ensembles\nThe authors proposed a method for designing efficient ensemble convolutional layers using grouped convolutions. This approach exploits the parallelization capabilities of GPUs to accelerate training and inference. The sequential training architecture is replaced with parallel implementations, as shown in the part b and c of the figure above. This figure summarizes equivalent architectures for a simple ensemble of M=3 neural networks with three convolutional layers and a final dense layer. In these implementations, feature maps are stacked on the channel dimension (denoted as rearrange operation). This results in a feature map of size M × Cj × Hj × Wj, regrouped by batches of size B × M, where B is the batch size of the ensemble. To maintain the original batch size, the batch is repeated M times after rearrangement. Grouped convolutions with M groups and γ subgroups per subnetwork are employed. Each feature map is processed independently by each subnetwork, resulting in separate outputs. Grouped convolutions are used throughout to ensure gradients remain independent between subnetworks. Other operations, like Batch Normalization, can be applied if they are groupable or act independently on each channel. The figure below illustrates the masks used to encode Packed Ensembles for M=2 and M=2 with γ=2. Finally, implementations (b) and (c) of the figure above are equivalent. A standard convolution can replace the initial steps (rearrangement and first grouped convolution) if all subnetworks receive the same images simultaneously.\nDiagram representation of a subnetwork mask: maskj, with M = 2, j an integer corresponding to a fully connected layer\nExperiments The experiment section evaluates the Packed-Ensembles (PE) method on classification tasks. Here are the key points:\nDatasets: CIFAR-10, CIFAR-100, and ImageNet are used for various complexity levels. Architectures: PE is compared on ResNet-18, ResNet-50, Wide ResNet-28-10 against Deep Ensembles, BatchEnsemble, MIMO, and Masksembles. Metrics: Accuracy (%), Negative Log-Likelihood (NLL), Expected Calibration Error (ECE) for calibration, and Areas Under Precision-Recall (AUPR) and ROC (AUC) curves for Out-of-Distribution (OOD) detection are used. Implementation Details: Softmax probabilities from all subnetworks are averaged for prediction. Maximum value of the output vector is considered the class. SVHN dataset is used for OOD detection on CIFAR-10/100. Mutual Information (MI) is used as a criterion for ensemble techniques on ImageNet-O and Texture datasets. ImageNet-R is used to evaluate robustness under distribution shift. Code: PyTorch-Lightning framework is used for implementation. Results The experiment results show that Packed-Ensembles (PE) achieves similar performance to Deep Ensembles (DE) on classification tasks, but with lower memory usage. Here are the key findings:\nCIFAR-10/100: PE performs similarly or slightly better than DE on OOD detection and classification (especially with larger architectures like ResNet-50 and Wide ResNet). Smaller architectures (ResNet-18) might not have enough capacity for PE to perform as well on CIFAR-100. ImageNet: PE improves uncertainty quantification for OOD detection and distribution shift compared to DE and single models. PE achieves better accuracy with a reasonable increase in training and inference cost. These results suggest that PE is a memory-efficient alternative to DE for tasks requiring good uncertainty estimation.\nPacked-Ensembles of ResNet50 performance on CIFAR-10 and CIFAR-100\nEthics This section emphasizes the ethical considerations of the research. Here are the key points:\nGoal: This research proposes a method to improve uncertainty estimation in deep learning models. Limitations: The authors acknowledge limitations, particularly for safety-critical systems (systems where failure can have severe consequences). Even though the method aims to improve reliability, it\u0026rsquo;s not ready for such applications. Concerns: The text mentions limitations explored in the experiments. These limitations highlight the need for further validation and verification before real-world use, especially concerning robustness in various scenarios like: Unknown situations Corner cases (uncommon but important situations) Adversarial attacks (attempts to intentionally mislead the model) Potential biases in the model Overall: The authors advocate for responsible use of the method and emphasize the importance of further research before deploying it in safety-critical systems. Reproducibility: Packed-Ensemble on CIFAR-10 We attempted to reproduce the experiment outlined in the tutorial available at https://torch-uncertainty.github.io/auto_tutorials/tutorial_pe_cifar10.html which trains a Packed-Ensemble classifier on the CIFAR-10 dataset. The tutorial details a step-by-step approach, including:\nData Loading and Preprocessing: Utilizing torchvision to load the CIFAR-10 dataset and performing normalization on the images. Packed-Ensemble Definition: Defining a Packed-Ensemble model with M=4 subnetworks, alpha=2, and gamma=1, built upon a standard convolutional neural network architecture. Loss Function and Optimizer: Employing Classification Cross-Entropy loss and SGD with momentum for optimization during training. Training: Training the Packed-Ensemble model on the CIFAR-10 training data. Testing and Evaluation: Evaluating the trained Packed-Ensemble on the CIFAR-10 test data, with a focus on uncertainty quantification and OOD (Out-of-Distribution) detection performance. Experimental Runs and Observations:\nTest 1:\nGroundTruth: cat ship ship plane\nThe predicted labels are: cat ship ship ship\nTest 2:\nGroundTruth: dog bird horse bird\nThe predicted labels are: dog frog car dog\nTest 3:\nGroundTruth: dog truck plane car The predicted labels are: dog horse ship truck\nChallenges and Limitations:\nA significant limitation of the tutorial is the lack of guidance on evaluating the model\u0026rsquo;s performance. Without a defined evaluation metric (e.g., accuracy, precision, recall), it\u0026rsquo;s challenging to determine the overall effectiveness of the trained Packed-Ensemble. While the provided test results show inconsistencies between ground truth labels and predictions, a quantitative evaluation metric is necessary to draw more concrete conclusions.\n","content_html":"\u003cscript\ntype=\"text/x-mathjax-config\"\u003e\n\nMathJax.Hub.Config({\n\n    tex2jax: {\n\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n\n    }\n\n});\n\nMathJax.Hub.Queue(function() {\n\n    var all = MathJax.Hub.getAllJax(), i;\n\n    for(i = 0; i \u003c all.length; i += 1) {\n\n        all[i].SourceElement().parentNode.className += ' has-jax';\n\n    }\n\n});\n\n\u003c/script\u003e\n\u003cscript\ntype=\"text/javascript\"\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003eIntroduction\u003c/h1\u003e\n\u003c/div\u003e\nThe document \"Packed-Ensembles for Efficient Uncertainty Estimation\" introduces a novel framework for designing and training compact, structured ensembles of neural networks, termed Packed-Ensembles (PE). It addresses the limitations of Deep Ensembles (DE) in terms of computational efficiency and hardware constraints by leveraging grouped convolutions. This technique allows for parallelizing the ensemble into a single shared backbone, improving training and inference speeds within the memory limits of standard neural networks. The paper demonstrates through extensive experiments that PEs maintain the beneficial properties of DEs, such as diversity and robustness to distribution shift, while achieving comparable accuracy, calibration, and out-of-distribution detection capabilities. The work includes implementation details, experimental results on CIFAR-10/100 and ImageNet datasets and comparisons with existing approaches. It concludes with insights on the reproducibility of results and the potential ethical considerations of deploying such models in safety-critical systems.\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003ePresentation of the model\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003ePacked-Ensembles\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/fig1.jpg\" alt=\"The base network and Packed-Ensembles\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eThe base network and Packed-Ensembles\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003ePacked-Ensembles (PE) is a technique for designing and training lightweight ensembles of neural networks. It is based on the idea of using grouped convolutions to create multiple subnetworks within a single network. These subnetworks are trained independently, which helps to improve the efficiency of the ensemble.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBenefits of Packed-Ensembles\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePacked-Ensembles offer several benefits over traditional ensemble methods, including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEfficiency:\u003c/strong\u003e Packed-Ensembles are more efficient than traditional ensembles in terms of memory usage and training time. This is because they use grouped convolutions to share parameters between the subnetworks.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAccuracy:\u003c/strong\u003e Packed-Ensembles can achieve accuracy levels that are comparable to traditional ensembles.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCalibration:\u003c/strong\u003e Packed-Ensembles are well-calibrated, meaning that their predicted probabilities are accurate reflections of the true probabilities.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eOut-of-distribution (OOD) detection:\u003c/strong\u003e Packed-Ensembles are good at detecting out-of-distribution data, which is data that comes from a different distribution than the data that the model was trained on.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eComparison to other ensemble methods\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe paper compares Packed-Ensembles to several other ensemble methods, including Deep Ensembles, BatchEnsemble, MIMO, and Masksembles. The paper found that Packed-Ensembles are more efficient than all of these methods, and they achieve comparable accuracy on most tasks.\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003ePacked-Ensembles: A Technique for Efficient Neural Network Ensembles\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003ePacked-Ensembles (PE) is a method for designing and training lightweight ensembles of neural networks. It aims to improve efficiency while maintaining accuracy and other desirable properties. This technique achieves this by leveraging grouped convolutions to create multiple subnetworks within a single network, enabling them to be trained independently.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUnderstanding Convolutional Layers and Grouped Convolutions:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eConvolutional Layers:\u003c/strong\u003e These are the backbone of Convolutional Neural Networks (CNNs), performing filtering operations on input data using learnable filters (kernels). Mathematically, the output of a convolutional layer, denoted by $z_{j+1}$, is calculated as follows:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$z^{(j+1)}(c,:,:) = (h^j \\otimes \\omega^j)(c,:,:) = \\sum_{k=0}^{C_{j}-1} \\omega^j(c, k,:,:) \\star h^j(k,:,:)$\u003c/p\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e$c$\u003c/strong\u003e represents the channel index\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e$h^j$\u003c/strong\u003e denotes the input feature map\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e$ω^j$\u003c/strong\u003e represents the weight tensor (kernel)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e$⋆$\u003c/strong\u003e denotes the 2D cross-correlation operator\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eGrouped Convolutions:\u003c/strong\u003e This technique allows training multiple subnetworks within a single network by dividing the channels of feature maps and weight tensors into groups. Each group is processed by a separate set of filters, essentially creating \u003cstrong\u003eindependent subnetworks\u003c/strong\u003e. The mathematical formulation for grouped convolutions is given by:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$\nz^{(j+1)}(c,:,:) = \\left( h^j \\otimes \\omega^j_{\\gamma} \\right) (c,:,:) = \\sum_{k=0}^{\\frac{C_{j}}{\\gamma}-1} \\omega^j_{\\gamma} (c, k,:,:) \\star h^j \\left( k + \\left\\lfloor \\frac{c}{C_{j+1}/\\gamma} \\right\\rfloor \\frac{C_{j}}{\\gamma}, :,:\\right)\n$$\u003c/p\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e$γ$\u003c/strong\u003e represents the number of groups\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$C_{j+1}$\u003c/strong\u003e and \u003cstrong\u003e$C_j$\u003c/strong\u003e denote the number of output and input channels, respectively.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe formula states that a grouped convolution layer is mathematically equivalent to a standard convolution where the weights are selectively applied using a binary mask \u003cstrong\u003e$\\text{mask}_{m}^j$\u003c/strong\u003e\n\u003cstrong\u003e$\\in \\{{ 0, 1 \\}}^{C_{j+1} \\times C_j \\times s_j^2}$\u003c/strong\u003e with $s_j^2$ the kernel size squared of the layer $j$. Each element in $\\text{mask}_{m}^j$ is either 0 or 1.\u003c/p\u003e\n\u003cp\u003eThe condition \u003cstrong\u003e$\\text{mask}_{m}^j(k, l, :, :) = 1$\u003c/strong\u003e happens only if $\\left\\lfloor \\frac{l}{C_{j}/\\gamma} \\right\\rfloor = \\left\\lfloor \\frac{k}{C_{j+1}/\\gamma} \\right\\rfloor$ for each group $m \\in [|0, \\gamma - 1 |]$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eComplete Mask and Convolution:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e$\\text{mask}^j = \\sum_{m=0}^{{\\gamma}-1}\\text{mask}_{m}^j$ : This combines the masks for all groups ($m$) into a single $\\text{mask}^j$ for layer $j$.\u003c/li\u003e\n\u003cli\u003e$z^{j+1} = h^j \\otimes (ω^j ◦ \\text{mask}^j)$: This rewrites the grouped convolution operation. Here:\n\u003cul\u003e\n\u003cli\u003e$z^{j+1}$: Output feature map of the layer.\u003c/li\u003e\n\u003cli\u003e$h^j$: Input feature map.\u003c/li\u003e\n\u003cli\u003e$ω^j$: Convolution weights for layer \u003ccode\u003ej\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e$\\otimes$: Denotes convolution operation.\u003c/li\u003e\n\u003cli\u003e$◦$: Denotes Hadamard product (element-wise multiplication).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eIn simpler terms:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGrouped convolution divides the input channels and weights into groups.\u003c/li\u003e\n\u003cli\u003eA separate mask is created for each group, ensuring elements within a group are aligned.\u003c/li\u003e\n\u003cli\u003eThese masks effectively turn specific weights to zero during the convolution, essentially selecting which weights contribute to the output for each group.\u003c/li\u003e\n\u003cli\u003eThe final convolution is equivalent to applying the original weights element-wise multiplied by the combined mask.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003eBackground on Deep Ensembles\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003eThis section delves into Deep Ensembles (DE), a technique for image classification tasks.\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/fig2.png\" alt=\"Deep Ensembles\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eDeep Ensembles\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSetting the Scene\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWe have a dataset $D$ containing pairs of images and their corresponding labels:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$x_i$: Represents an image sample with dimensions $C0 \\times H0 \\times W0$ (likely referring to color channels, height, and width).\u003c/li\u003e\n\u003cli\u003e$y_i$ : One-hot encoded label representing the class of the image ($NC$ total classes).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe dataset is assumed to be drawn from a joint distribution $P(X, Y)$.\u003c/p\u003e\n\u003cp\u003eA neural network $f_\\theta$ processes the images and predicts their class labels. This network has learnable parameters denoted by $\\theta$.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\hat{y}_i = f_θ(xi)$: The predicted class label for image $x_i$ based on the network with parameters $θ$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eTraditional Approach:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe model predicts probabilities for each class using a Multinoulli distribution. These probabilities are treated as point estimates, meaning they represent the most likely class without considering uncertainty.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIntroducing Deep Ensembles\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDE works by training multiple Deep Neural Networks (DNNs) $M$ with random initializations. These DNNs are denoted by $θ_m$ for the $m-th$ network ($0$ to $M-1$).\u003c/p\u003e\n\u003cp\u003eThe ensemble prediction is obtained by averaging the predictions of all $M$ DNNs as shown in the equation below:\u003c/p\u003e\n\u003cp\u003e$$\nP(y_i|x_i, D) = M^{-1} \\sum_{m=0}^{M-1} P(y_i|x_i, \\theta_m)\n$$\u003c/p\u003e\n\u003cp\u003eThis essentially combines the outputs of multiple networks to create a more robust prediction.\u003c/p\u003e\n\u003cp\u003eIn simpler terms, DE trains multiple neural networks with slight variations and combines their predictions to get a more reliable estimate, including the level of uncertainty in the prediction.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBuilding Packed-Ensembles:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePacked-Ensembles combine the concepts of Deep Ensembles (ensembles of multiple independent DNNs) and grouped convolutions. Here\u0026rsquo;s how it works:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSubnetworks:\u003c/strong\u003e The ensemble is formed by creating \u003cstrong\u003e$M$\u003c/strong\u003e smaller subnetworks within the main network architecture. These subnetworks share the same structure but have \u003cstrong\u003eindependent parameters\u003c/strong\u003e due to the use of grouped convolutions.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHyperparameters:\u003c/strong\u003e Packed-Ensembles are defined by three hyperparameters:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e$α$ (alpha):\u003c/strong\u003e expansion factor that scales the width of each subnetwork (compensates for the decrease in capacity due to using fewer parameters).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$M$:\u003c/strong\u003e number of subnetworks in the ensemble (represents the ensemble size).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$γ$ (gamma):\u003c/strong\u003e number of groups for grouped convolutions within each subnetwork (introduces another level of sparsity).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eMathematical Implementation:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe output of a Packed-Ensemble layer is calculated by averaging the predictions from each subnetwork, as shown in the following equation:\u003c/p\u003e\n\u003cp\u003e$$\n\\hat{y} = M^{-1} \\sum_{m=0}^{M-1} P(y|\\theta_a^m, x) \\quad \\text{with} \\quad \\theta_a^m = ({\\omega_j^{\\alpha} \\circ \\text{mask}_{m}^j})_j\n$$\u003c/p\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e$\\hat{y}$\u003c/strong\u003e represents the ensemble\u0026rsquo;s predicted label\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$P(y|θ_a^m, x)$\u003c/strong\u003e denotes the probability of class \u003cstrong\u003e$y$\u003c/strong\u003e given the input \u003cstrong\u003e$x$\u003c/strong\u003e and the parameters \u003cstrong\u003e$θ_a^m$\u003c/strong\u003e of the \u003cstrong\u003e$m-th$\u003c/strong\u003e subnetwork\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$\\theta_a^m = ({\\omega_j^{\\alpha} \\circ \\text{mask}_{m}^j})_j$\u003c/strong\u003e represents the parameters of the \u003cstrong\u003e$m-th$\u003c/strong\u003e subnetwork, obtained by applying element-wise multiplication (\u003cstrong\u003e$∘$\u003c/strong\u003e) between the expanded weights (\u003cstrong\u003e$\\omega_j^{\\alpha}$\u003c/strong\u003e) and the group mask (\u003cstrong\u003e$\\text{mask}_{m}$\u003c/strong\u003e) for each layer \u003cstrong\u003e$j$\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/fig4.png\" alt=\"Equivalent architectures for Packed-Ensembles\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eEquivalent architectures for Packed-Ensembles\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003eThe authors proposed a method for designing efficient ensemble convolutional layers using grouped convolutions. This approach exploits the parallelization capabilities of GPUs to accelerate training and inference. The sequential training architecture is replaced with parallel implementations, as shown in the part b and c of the figure above. This figure summarizes equivalent architectures for a simple ensemble of M=3 neural networks with three convolutional layers and a final dense layer. In these implementations, feature maps are stacked on the channel dimension (denoted as rearrange operation). This results in a feature map of size M × Cj × Hj × Wj, regrouped by batches of size B × M, where B is the batch size of the ensemble. To maintain the original batch size, the batch is repeated M times after rearrangement. Grouped convolutions with M groups and γ subgroups per subnetwork are employed. Each feature map is processed independently by each subnetwork, resulting in separate outputs. Grouped convolutions are used throughout to ensure gradients remain independent between subnetworks. Other operations, like Batch Normalization, can be applied if they are groupable or act independently on each channel. The figure below illustrates the masks used to encode Packed Ensembles for M=2 and M=2 with γ=2. Finally, implementations (b) and (c) of the figure above are equivalent. A standard convolution can replace the initial steps (rearrangement and first grouped convolution) if all subnetworks receive the same images simultaneously.\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/fig5.png\" alt=\"subnetwork mask\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eDiagram representation of a subnetwork mask: maskj, with M = 2, j an integer corresponding to a fully connected layer\u003c/i\u003e\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003eExperiments\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003eThe experiment section evaluates the Packed-Ensembles (PE) method on classification tasks. Here are the key points:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDatasets:\u003c/strong\u003e CIFAR-10, CIFAR-100, and ImageNet are used for various complexity levels.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eArchitectures:\u003c/strong\u003e PE is compared on ResNet-18, ResNet-50, Wide ResNet-28-10 against Deep Ensembles, BatchEnsemble, MIMO, and Masksembles.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMetrics:\u003c/strong\u003e Accuracy (%), Negative Log-Likelihood (NLL), Expected Calibration Error (ECE) for calibration, and Areas Under Precision-Recall (AUPR) and ROC (AUC) curves for Out-of-Distribution (OOD) detection are used.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplementation Details:\u003c/strong\u003e Softmax probabilities from all subnetworks are averaged for prediction. Maximum value of the output vector is considered the class. SVHN dataset is used for OOD detection on CIFAR-10/100. Mutual Information (MI) is used as a criterion for ensemble techniques on ImageNet-O and Texture datasets. ImageNet-R is used to evaluate robustness under distribution shift.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCode:\u003c/strong\u003e PyTorch-Lightning framework is used for implementation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003eResults\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003eThe experiment results show that Packed-Ensembles (PE) achieves similar performance to Deep Ensembles (DE) on classification tasks, but with lower memory usage. Here are the key findings:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCIFAR-10/100:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003ePE performs similarly or slightly better than DE on OOD detection and classification (especially with larger architectures like ResNet-50 and Wide ResNet).\u003c/li\u003e\n\u003cli\u003eSmaller architectures (ResNet-18) might not have enough capacity for PE to perform as well on CIFAR-100.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImageNet:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003ePE improves uncertainty quantification for OOD detection and distribution shift compared to DE and single models.\u003c/li\u003e\n\u003cli\u003ePE achieves better accuracy with a reasonable increase in training and inference cost.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese results suggest that PE is a memory-efficient alternative to DE for tasks requiring good uncertainty estimation.\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/fig3.png\" alt=\"ResNet50 performance\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003ePacked-Ensembles of ResNet50 performance on CIFAR-10 and CIFAR-100\u003c/i\u003e\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003eEthics\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003eThis section emphasizes the ethical considerations of the research. Here are the key points:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e This research proposes a method to improve uncertainty estimation in deep learning models.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLimitations:\u003c/strong\u003e The authors acknowledge limitations, particularly for safety-critical systems (systems where failure can have severe consequences). Even though the method aims to improve reliability, it\u0026rsquo;s not ready for such applications.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConcerns:\u003c/strong\u003e The text mentions limitations explored in the experiments. These limitations highlight the need for further validation and verification before real-world use, especially concerning robustness in various scenarios like:\n\u003cul\u003e\n\u003cli\u003eUnknown situations\u003c/li\u003e\n\u003cli\u003eCorner cases (uncommon but important situations)\u003c/li\u003e\n\u003cli\u003eAdversarial attacks (attempts to intentionally mislead the model)\u003c/li\u003e\n\u003cli\u003ePotential biases in the model\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOverall:\u003c/strong\u003e The authors advocate for responsible use of the method and emphasize the importance of further research before deploying it in safety-critical systems.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n\u003ch1\u003eReproducibility: Packed-Ensemble on CIFAR-10\u003c/h1\u003e\n\u003c/div\u003e\n\u003cp\u003eWe attempted to reproduce the experiment outlined in the tutorial available at \u003ca href=\"https://torch-uncertainty.github.io/auto_tutorials/tutorial_pe_cifar10.html\"\u003ehttps://torch-uncertainty.github.io/auto_tutorials/tutorial_pe_cifar10.html\u003c/a\u003e which trains a Packed-Ensemble classifier on the CIFAR-10 dataset. The tutorial details a step-by-step approach, including:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Loading and Preprocessing:\u003c/strong\u003e Utilizing torchvision to load the CIFAR-10 dataset and performing normalization on the images.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePacked-Ensemble Definition:\u003c/strong\u003e Defining a Packed-Ensemble model with M=4 subnetworks, alpha=2, and gamma=1, built upon a standard convolutional neural network architecture.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLoss Function and Optimizer:\u003c/strong\u003e Employing Classification Cross-Entropy loss and SGD with momentum for optimization during training.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraining:\u003c/strong\u003e Training the Packed-Ensemble model on the CIFAR-10 training data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTesting and Evaluation:\u003c/strong\u003e Evaluating the trained Packed-Ensemble on the CIFAR-10 test data, with a focus on uncertainty quantification and OOD (Out-of-Distribution) detection performance.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eExperimental Runs and Observations:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTest 1:\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/Result1.png\" alt=\"First result\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eGroundTruth:  cat   ship  ship  plane\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003eThe predicted labels are: cat   ship  ship  ship\u003c/p\u003e\n\u003cp\u003eTest 2:\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/Result2.png\" alt=\"Second result\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eGroundTruth: dog bird horse bird\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003eThe predicted labels are: dog  frog  car  dog\u003c/p\u003e\n\u003cp\u003eTest 3:\u003c/p\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/Result3.png\" alt=\"Third result\" style=\"display:block; margin:auto;\"\u003e\n\u003c/div\u003e\n\u003cp style=\"text-align:center;\"\u003e\u003ci\u003eGroundTruth:  dog truck plane car \u003c/i\u003e\u003c/p\u003e\n\u003cp\u003eThe predicted labels are: dog  horse ship  truck\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eChallenges and Limitations:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eA significant limitation of the tutorial is the lack of guidance on evaluating the model\u0026rsquo;s performance. Without a defined evaluation metric (e.g., accuracy, precision, recall), it\u0026rsquo;s challenging to determine the overall effectiveness of the trained Packed-Ensemble. While the provided test results show inconsistencies between ground truth labels and predictions, a quantitative evaluation metric is necessary to draw more concrete conclusions.\u003c/p\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/packed-ensembles/","date_published":"27026-27-09T25:2727:00+01:00","date_modified":"27026-27-09T25:2727:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"fc0f8a815b8318f3a6944187159cad98a8481ca8","title":"A Framework to Learn with Interpretation","summary":"","content_text":" A Framework to Learn with Interpretation Authors: Maroun ABOU BOUTROS, Mohamad EL OSMAN\nArticle: A Framework to Learn with Interpretation by Jayneel Parekh, Pavlo Mozharovskyi and Florence d’Alché-Buc\nTable of Contents Introduction Learning a classifier and an interpreter Design of FLINT Interpretation in FLINT Learning by imposing interpretability properties Understanding encoded concepts in FLINT Reproducing the experiments Global interpretation Local interpretation Subjective evaluation Specialization of FLINT to post-hoc interpretability Conclusion 1 Introduction In this blog post, we’ll explore FLINT, a framework introduced in the paper titled “A Framework to Learn with Interpretation” by Jayneel Parekh, Pavlo Mozharovskyi and Florence d’Alché-Buc, available on the following link, addressing the crucial need for interpretability in machine learning as complex predictive models become more prevalent in fields like law, healthcare, and defense. Interpretability, synonymous with explainability, provides insights into a model’s decision-making process. Two main approaches, post-hoc methods and “interpretable by design” methods, tackle the challenge of interpreting models, each with its pros and cons. A new approach, Supervised Learning with Interpretation (SLI), jointly learns a predictive model and an interpreter model. FLINT, specifically designed for deep neural network classifiers, introduces a novel interpreter network architecture promoting local and global interpretability. It also proposes a criterion for concise and diverse attribute functions, enhancing interpretability. We’ll delve into the architecture of FLINT and how it works to give explainable predictions, and we will reproduce some experiments done in the experimental section of the article and evaluate their outputs to study FLINT\u0026rsquo;s performance. And finally, we will present a specialization of FLINT for post-hoc interpretability.\n2 Learning a classifier and its interpreter with FLINT The paper introduces Supervised Learning with Interpretation (SLI), a new task aimed at incorporating interpretability alongside prediction in machine learning models. In SLI, a separate model, called an interpreter, is employed to interpret the predictions made by the primary predictive model. The task involves minimizing a combined loss function consisting of prediction error and interpretability objectives. The paper focuses on addressing SLI within the context of deep neural networks for multi-class classification tasks. It proposes a framework called Framework to Learn with INTerpretation (FLINT), which utilizes a specialized architecture for the interpreter model, distinguishes between local and global interpretations, and introduces corresponding penalties in the loss function to achieve the desired interpretability.\nSo for a dataset $S$ and a given model $f \\in F$ where $F$ is a class of classifiers (here neural networks) and an interpreter model $g \\in G_f$ where $G_f$ is a family of models, the SLI problem is presented by: $$ \\arg{\\min_{f \\in F, g \\in G_f}{L_{pred}(f, S) + L_{int}(f, g, S)}} $$ Where $L_{pred}(f, S)$ denotes a loss term related to prediction error and $L_{int}(f, g, S)$ measures the ability of $g$ to provide interpretations of predictions by $f$.\n2.1 Design of FLINT In FLINT, depicted in the image above, both a prediction model ($f$) and an interpreter model ($g$) are used. The input to FLINT is a vector $x \\in X$, where $X = \\mathbb{R}^d$, and the output is a vector $y \\in Y$, where $Y$ is defined as the set of one-hot encoding vectors with binary components of size $C$ (the number of classes to predict). The prediction model $f$ is structured as a deep neural network with $l$ hidden layers, represented as $f = f_{l+1} \\circ f_l \\circ \\ldots \\circ f_1$. Each $f_k$ represents a hidden layer mapping from $R^{d_{k-1}}$ to $R^{d_k}$. To interpret the outputs of $f$, we randomly select a subset of $T$ hidden layers, indexed by $I=\\{i_1, i_2, \\ldots, i_T\\}$, and concatenate their outputs to form a new vector $f_I(x) \\in \\mathbb{R}^D$, where $D = \\sum_{t=1}^T d_{i_t}$. This vector is then fed into a neural network $\\Psi$ to produce an output vector $\\Phi(x) = \\Psi(f_I(x)) = (\\phi_1(x), \u0026hellip;, \\phi_J (x)) \\in \\mathbb{R}^J$, representing an attribute dictionary comprising functions $\\phi_j: X \\rightarrow \\mathbb{R}^+$, where $\\phi_j(x)$ captures the activation of a high-level attribute or a \u0026ldquo;concept\u0026rdquo; over $X$. Finally, $g$ computes the composition of the attribute dictionnary with an interpretable function $h: R^J \\rightarrow Y$. $$ \\forall x \\in X, g(x) = h(\\Phi(x)) $$ For now we take $h(x) = softmax(W^T \\Phi(x))$ but $h$ can be any interpretable function (like a decision tree for example).\nNote that $d$ in the image is a decoder network that takes $\\Phi(x)$ and reconstructs the input $x$. This decoder is used for training and its purpose will be detailed later on in section 2.3.\n2.2 Interpretation in FLINT With the interpreter defined, let\u0026rsquo;s clarify its role and interpretability objectives within FLINT. Interpretation serves as an additional task alongside prediction. We\u0026rsquo;re interested in two types: global interpretation, which aids in understanding which attribute functions contribute to predicting a class, and local interpretation, which pinpoints the attribute functions involved in predicting a specific sample.\nTo interpret a local prediction $f(x)$, it\u0026rsquo;s crucial that the interpreter\u0026rsquo;s output $g(x)$ aligns with $f(x)$. Any discrepancy prompts analysis of conflicting data, potentially raising concerns about the prediction\u0026rsquo;s confidence.\nTo establish local and global interpretation, we rely on attribute relevance. Given an interpreter with parameters $\\Theta_g = (\\theta_\\Psi, \\theta_h)$ and an input $x$, an attribute $\\phi_j$\u0026rsquo;s relevance is defined concerning the prediction $g(x) = f(x) = \\hat{y}$. The attribute\u0026rsquo;s contribution to the unnormalized score of class $\\hat{y}$ is $\\alpha_{j, \\hat{y}, x} = \\phi_j(x) \\cdot w_{j, \\hat{y}}$, where $w_{j, \\hat{y}}$ is the coefficient associated with this class. Relevance score $r_{j, x}$ is computed by normalizing $\\alpha$ as $r_{j, x} = \\frac{\\alpha_{j, \\hat{y}, x}}{\\max_i |\\alpha_{i, \\hat{y}, x}|}$. An attribute $\\phi_j$ is considered relevant for a local prediction if it\u0026rsquo;s both activated and effectively used in the linear model.\nAttribute relevance extends to its overall importance in predicting any class $c$. This is achieved by averaging relevance scores from local interpretations over a random subset or the entirety of the training set $S$ where the predicted class is $c$. Thus, $r_{j, c} = \\frac{1}{|S_c|} \\sum_{x \\in S_c} r_{j, x}$, where $S_c = \\{x \\in S \\mid \\hat{y} = c\\}$.\nNow, let\u0026rsquo;s introduce the local and global interpretations the interpreter will provide:\nGlobal interpretation ($G(g, f)$) identifies class-attribute pairs $(c, \\phi_j)$ where the global relevance $r_{j, c}$ exceeds a threshold $\\frac{1}{\\tau}$.\nLocal interpretation ($L(x, g, f)$) for a sample $x$ includes attribute functions $\\phi_j$ with local relevance $r_{j, x}$ surpassing $\\frac{1}{\\tau}$. These definitions don\u0026rsquo;t assess interpretation quality directly.\n2.3 Learning by imposing interpretability properties For learning, the paper defines certain penalties to minimize, where each one aims to enforce a certain desirable property:\nFidelity to output: The output of $g(x)=h(\\Psi(f_I(x)))$ should be close to $f(x)$ for any x. This can be imposed through a cross-entropy loss: $$ L_{of}(f, g, S) = - \\sum_{x \\in S} h(\\Psi(f_I(x)))^T \\log(f(x)) $$\nConciseness and Diversity of Interpretations: We aim for concise local interpretations, containing only essential attributes per sample, promoting clearer understanding and capturing high-level concepts. Simultaneously, we seek diverse interpretations across samples to prevent attribute functions from being class-exclusive. To achieve this, the paper proposes that we leverage entropy (defined for a vector as $\\mathcal{E}(v) = - \\sum_i p_i \\log(p_i)$), which quantifies uncertainty in real vectors. Conciseness is fostered by minimizing the entropy of the interpreter\u0026rsquo;s output, $\\Phi(x) = \\Psi(f_I(x))$, while diversity is encouraged by maximizing the entropy of the average $\\Psi(f_I(x))$ over a mini-batch. This approach promotes sparse and varied coding of $f_I(x)$, enhancing interpretability. However, as entropy-based losses lack attribute activation constraints, leading to suboptimal optimization, we also minimize the $l_1$ norm of $\\Psi(f_I(x))$ with hyperparameter $\\eta$. Although $l_1$-regularization commonly encourages sparsity, the experiments done in the paper show that entropy-based methods are more effective. $$ L_{cd}(f, g, S) = -\\mathcal{E}(\\frac{1}{\\lvert S \\lvert} \\sum_{x \\in S} \\Psi(f_I(x))) + \\sum_{x \\in S} \\mathcal{E}(\\Psi(f_I(x))) + \\sum_{x \\in S} \\eta \\lVert \\Psi(f_I(x)) \\lVert_1 $$\nFidelity to input: In order to promote the representation of intricate patterns associated with the input within $\\Phi(x)$, a decoder network $d : \\mathbb{R}^J \\rightarrow X$ is employed. This network is designed to take the attribute dictionary $\\Phi(x)=\\Psi(f_I(x))$ as input and reconstruct the original input $x$. $$ L_{if}(f, g, d, S) = \\sum_{x \\in S} (d(\\Psi(f_I(x))) - x)^2 $$\nGiven the proposed loss terms, the loss for the interpretability model writes as follows: $$ L_{int}(f, g, d, S) = \\beta L_{of}(f, g, S) + \\gamma L_{if}(f, g, d, S) + \\delta L_{cd}(f, g, S) $$ Where $\\beta, \\gamma, \\delta$ are non-negative hyperparameters. the total loss to be minimized $L = L_{pred} + L_{int}$, where the prediction loss, $L_{pred}$, is the well-know cross entropy loss (since this a classification problem).\n3 Understanding encoded concepts in FLINT Once the predictor and interpreter networks are jointly learned, interpretation can be conducted at both global and local levels . A critical aspect highlighted by the authors is understanding the concepts encoded by each individual attribute function ​$\\phi_j$ . Focusing on image classification, the authors propose representing an encoded concept as a collection of visual patterns in the input space that strongly activate $\\phi_j$ . They present a pipeline for generating visualizations for both global and local interpretation, adapting various existing tools .\nFor global interpretation visualization, the authors propose starting by selecting a small subset of training samples from a given class c that maximally activate ​$\\phi_j$ . This subset, referred to as Maximum Activating Samples (MAS), is denoted as $MAS(c , ​\\phi_j , l)$ where $l$ is the subset size (set as 3 in their experiments). However, while MAS provides some insight into the encoded concept, further analysis is required to understand the specific aspects of these samples that cause ​$\\phi_j$ activation. To achieve this, the authors propose utilizing a modified version of activation maximization called Activation Maximization with Partial Initialization (AM+PI). This technique aims to synthesize input that maximally activates ​$\\phi_j$ by optimizing a common activation maximization objective, initialized with a low-intensity version of the sample from MAS.\nFor local analysis, given any test sample $x_{0}$ , its local interpretation $L(x_{0},f,g)$ can be determined, representing the relevant attribute functions . To visualize a relevant attribute ​$\\phi_j$, the authors suggest repeating the AM+PI procedure with initialization using a low-intensity version of $x_{0}$ to enhance the concept detected by ​$\\phi_j$ in $x_{0}$ .\n4 Reproducing the experiments In the experimental section of the article, several experiments were conducted to do a quantitative evaluation of FLINT\u0026rsquo;s performance compared to other state-of-the-art models designed for interpretability, such as SENN and PrototypeDNN. Additionally, FLINT was compared to LIME and VIBI to evaluate the fidelity of its interpretations, measuring the proportion of samples where the predictions of a model and its interpreter agree. Across these tests, FLINT consistently outperformed the other models, demonstrating its reliability and effectiveness.\nHowever, in this blog post we will specifically focus on reproducing the experiments in the article related to FLINT\u0026rsquo;s explainability, that aim to do a qualitative analysis of it. To achieve a thorough understanding of the model and its operational dynamics across prevalent datasets, we replicated the study by cloning the project from the GitHub repository referenced in the article (repo link). Our experimentation involved the CIFAR10 and QuickDRAW datasets, employing a ResNet18-based network for both. For the QuickDRAW dataset, we utilized J=24 attributes, while for the CIFAR10 dataset, we used J=36 attributes.\nThe instructions provided in the GitHub repository for executing the model are clear, and the model runs flawlessly. We have the option to either train the model ourselves or download the pre-trained models. Furthermore, there is a well-detailed Python notebook named \u0026ldquo;FLINT demo.ipynb\u0026rdquo;, which contains code for visualizing data, such as attribute relevance scores for each class and local interpretations for data samples. We will execute FLINT on test images and take a look at how interpretability is done with FLINT in this section.\n4.1 Global interpretation In the article, the authors explore global interpretation using a figure similar to the one provided below which was reproduced from the notebook, and which illustrates the generated global relevances $r_{j,c}$ for all class-attribute pairs in the QuickDraw dataset.\nAdditionally, by running the model on the CIFAR10 and QuickDRAW dataset we got visual outputs representative of class-attribute pair analyses for both datasets. These outputs served as pivotal tools in elucidating interrelations and facilitating comparative assessments between attributes and classes. We present below two figures derived from the resultant class-attribute pair analyses for each of the 2 datasets. The class-attribute pairs shown are different from the examples shown in the paper.\nCaption: Class-attribute pair analysis on dataset CIFAR10\nCaption: Class-attribute pair analysis on dataset QuickDraw\nWe focus on class-attribute pairs with high relevance, showcasing examples in the provided figure above . For each pair, we examine Maximum Activating Samples (MAS) alongside their corresponding Activation Maximization with Partial Initialization (AM+PI) outputs.\nMAS analysis alone provides valuable insights into the encoded concept. For instance, on QuickDRAW dataset, attribute $\\phi_{16}$ relevant for class \u0026lsquo;Banana\u0026rsquo; activates the curve shape of the banana. However, AM+PI outputs offer deeper insights by elucidating which parts of the input activate an attribute function more clearly. And on CIFAR10 dataset , attribute $\\phi_{12}$ activates for \u0026lsquo;Deer\u0026rsquo; class , but the specific focus of the attribute remains ambiguous. The outputs of the AM+PI method indicate that attribute $\\phi_{12}$ predominantly highlights the area encompassing the legs and the horns of the deer, characterized as the most prominently enhanced regions.\n4.2 Local interpretation Similarly to the article, we explored local interpretation through the figure provided below which was generated in the notebook, which showcases visualizations for 4 test samples of the QuickDRAW dataset. Both predictor $f$ and interpreter $g$ accurately predict the true class in all cases, for the first 2 it\u0026rsquo;s \u0026ldquo;Cat\u0026rdquo; and the last 2 it\u0026rsquo;s \u0026ldquo;Banana\u0026rdquo;. For each case, they highlighted the top 3 relevant attributes to the prediction along with their relevances and corresponding AM+PI outputs.\nAnalysis of the AM+PI outputs reveals that attribute functions generally activate for patterns corresponding to the same concept inferred during global analysis. This consistency is evident for attribute functions present in the previous figures. Additionaly, by looking at the figure showing the relevance of class-attribute pairs in section 4.1 for the QuickDRAW dataset we observe that the 3 most important features for each class in the local interpretations are also those having the highest relevence for these classes. For example for the \u0026ldquo;Banana\u0026rdquo; class, $\\phi_{16}$, which activates the curve shape, is by far the most important feature for identifying this class by looking at both the local interpretations and the class-attribute relevences. While for the \u0026ldquo;Cat\u0026rdquo; class, it seems that the most important features are in order $\\phi_{23}$, $\\phi_1$ and $\\phi_{19}$ when looking at both the local interpretations and the class-attribute relevences.\n5 Subjective evaluation In the article, a subjective evaluation survey with 20 respondents using the QuickDraw dataset to assess FLINT\u0026rsquo;s interpretability is conducted. The authors selected 10 attributes covering 17 class-attribute pairs and presented visualizations (3 MAS and AM+PI outputs) along with textual descriptions for each attribute to the respondents. They were asked to indicate their level of agreement with the association between the descriptions and the patterns in the visualizations using predefined choices.\nDescriptions were manually generated, including 40% incorrect ones to ensure informed responses. Results showed that for correct descriptions, 77.5% of respondents agreed, 10.0% were unsure, and 12.5% disagreed. For incorrect descriptions, 83.7% disagreed, 7.5% were unsure, and 8.8% agreed. These results affirm that the concepts encoded in FLINT\u0026rsquo;s learned attributes are understandable to humans.\n6 Specialization of FLINT to post-hoc interpretability FLINT primarily aims for interpretability by design, but the authors of the article propose that it can also be adapted to provide post-hoc interpretations when a classifier $\\hat{f}$ is already available. Post-hoc interpretation learning, a special case of SLI, involves building an interpreter for $\\hat{f}$ by minimizing a certain objective function. Specifically, Given a classifier $\\hat{f} \\in F$ and a training set $S$, the goal is to build an interpreter of $\\hat{f}$ by solving: $$ \\text{arg} \\min_{g \\in G_{f}} L_{int}(\\hat{f}, g, S) $$ Where $g(x)=h(\\Phi(\\hat{f_I} (x)))$ for a given set of $I$ hidden layers and an attribute dictionnary of size $J$. The learning is performed the same as before but we only keep the parameters $\\theta_\\Psi$, $\\theta_h$ and $\\theta_d$. We fix $\\theta_\\hat{f}$ and remove $L_{pred}$ from the training loss $L$.\nThere are experimental results in the article and in the supplements that are not mentionned here that demonstrate the effectiveness of post-hoc interpretation within FLINT, showing that even without fine-tuning the internal layers of the classifier, meaningful interpretations can be generated with high fidelity.\n7 Conclusion In conclusion, FLINT offers a robust framework for enhancing the interpretability of machine learning models, particularly deep neural networks, in critical domains like healthcare, law, and defense. By jointly learning predictor and interpreter models, FLINT addresses the challenge of providing both global and local interpretations of model predictions. Through carefully designed loss functions, FLINT ensures fidelity to input and output, promotes concise and diverse interpretations, and facilitates the representation of intricate patterns associated with input data. Reproducing experiments on datasets such as CIFAR10 and QuickDRAW showcases FLINT\u0026rsquo;s effectiveness in providing interpretable insights into model predictions. Subjective evaluations affirm the understandability of FLINT\u0026rsquo;s learned attributes, reinforcing its potential for real-world applications. Moreover, FLINT\u0026rsquo;s adaptability for post-hoc interpretability underscores its versatility, enabling meaningful interpretations without extensive modification of the underlying classifier. Overall, FLINT emerges as a valuable tool for fostering transparency and trust in complex machine learning models, contributing to the development of interpretable AI systems across various domains.\n","content_html":"\u003chr\u003e\u003c/hr\u003e\n\u003cstyle\nTYPE=\"text/css\"\u003e\n\u003cp\u003ecode.has-jax {font:\ninherit;\nfont-size:\n100%;\nbackground:\ninherit;\nborder:\ninherit;}\u003c/p\u003e\n\u003cp\u003e\u003c/style\u003e\u003c/p\u003e\n\u003cscript\ntype=\"text/x-mathjax-config\"\u003e\n\nMathJax.Hub.Config({\n\n    tex2jax: {\n\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n\n    }\n\n});\n\nMathJax.Hub.Queue(function() {\n\n    var all = MathJax.Hub.getAllJax(), i;\n\n    for(i = 0; i \u003c all.length; i += 1) {\n\n        all[i].SourceElement().parentNode.className += ' has-jax';\n\n    }\n\n});\n\n\u003c/script\u003e\n\u003cscript\ntype=\"text/javascript\"\nsrc=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003ch1 style=\"font-size: 36px;\"\u003eA Framework to Learn with Interpretation\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eAuthors: Maroun ABOU BOUTROS, Mohamad EL OSMAN\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eArticle: \u003ca href=\"https://arxiv.org/abs/2010.09345\"\u003eA Framework to Learn with Interpretation\u003c/a\u003e by Jayneel Parekh, Pavlo Mozharovskyi and Florence d’Alché-Buc\u003c/strong\u003e\u003c/p\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eLearning a classifier and an interpreter\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-2.1\"\u003eDesign of FLINT\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2.2\"\u003eInterpretation in FLINT\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2.3\"\u003eLearning by imposing interpretability properties\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eUnderstanding encoded concepts in FLINT\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eReproducing the experiments\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-4.1\"\u003eGlobal interpretation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4.2\"\u003eLocal interpretation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eSubjective evaluation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eSpecialization of FLINT to post-hoc interpretability\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-7\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"section-1\"\u003e1 Introduction\u003c/h1\u003e\n\u003cp\u003eIn this blog post, we’ll explore FLINT, a framework introduced in the paper titled “A Framework to Learn with Interpretation” by Jayneel Parekh, Pavlo Mozharovskyi and Florence d’Alché-Buc, available on the following \u003ca href=\"https://arxiv.org/abs/2010.09345\"\u003elink\u003c/a\u003e, addressing the crucial need for interpretability in machine learning as complex predictive models become more prevalent in fields like law, healthcare, and defense. Interpretability, synonymous with explainability, provides insights into a model’s decision-making process. Two main approaches, post-hoc methods and “interpretable by design” methods, tackle the challenge of interpreting models, each with its pros and cons. A new approach, Supervised Learning with Interpretation (SLI), jointly learns a predictive model and an interpreter model. FLINT, specifically designed for deep neural network classifiers, introduces a novel interpreter network architecture promoting local and global interpretability. It also proposes a criterion for concise and diverse attribute functions, enhancing interpretability. We’ll delve into the architecture of FLINT and how it works to give explainable predictions, and we will reproduce some experiments done in the experimental section of the article and evaluate their outputs to study FLINT\u0026rsquo;s performance. And finally, we will present a specialization of FLINT for post-hoc interpretability.\u003c/p\u003e\n\u003ch1 id=\"section-2\"\u003e2 Learning a classifier and its interpreter with FLINT\u003c/h1\u003e\n\u003cp\u003eThe paper introduces Supervised Learning with Interpretation (SLI), a new task aimed at incorporating interpretability alongside prediction in machine learning models. In SLI, a separate model, called an interpreter, is employed to interpret the predictions made by the primary predictive model. The task involves minimizing a combined loss function consisting of prediction error and interpretability objectives. The paper focuses on addressing SLI within the context of deep neural networks for multi-class classification tasks. It proposes a framework called Framework to Learn with INTerpretation (FLINT), which utilizes a specialized architecture for the interpreter model, distinguishes between local and global interpretations, and introduces corresponding penalties in the loss function to achieve the desired interpretability.\u003cbr\u003e\nSo for a dataset $S$ and a given model $f \\in F$ where $F$ is a class of classifiers (here neural networks) and an interpreter model $g \\in G_f$ where $G_f$ is a family of models, the SLI problem is presented by:\n$$\n\\arg{\\min_{f \\in F, g \\in G_f}{L_{pred}(f, S) + L_{int}(f, g, S)}}\n$$\nWhere $L_{pred}(f, S)$ denotes a loss term related to prediction error and $L_{int}(f, g, S)$ measures the ability of $g$ to provide interpretations of predictions by $f$.\u003c/p\u003e\n\u003ch2 id=\"section-2.1\"\u003e2.1 Design of FLINT\u003c/h2\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/FLINT/FLINT_design.png\"\n  alt=\"design of FLINT\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eIn FLINT, depicted in the image above, both a prediction model ($f$) and an interpreter model ($g$) are used. The input to FLINT is a vector $x \\in X$, where $X = \\mathbb{R}^d$, and the output is a vector $y \\in Y$, where $Y$ is defined as the set of one-hot encoding vectors with binary components of size $C$ (the number of classes to predict). The prediction model $f$ is structured as a deep neural network with $l$ hidden layers, represented as $f = f_{l+1} \\circ f_l \\circ \\ldots \\circ f_1$. Each $f_k$ represents a hidden layer mapping from $R^{d_{k-1}}$ to $R^{d_k}$. To interpret the outputs of $f$, we randomly select a subset of $T$ hidden layers, indexed by $I=\\{i_1, i_2, \\ldots, i_T\\}$, and concatenate their outputs to form a new vector $f_I(x) \\in \\mathbb{R}^D$, where $D = \\sum_{t=1}^T d_{i_t}$. This vector is then fed into a neural network $\\Psi$ to produce an output vector $\\Phi(x) = \\Psi(f_I(x)) = (\\phi_1(x), \u0026hellip;, \\phi_J (x)) \\in \\mathbb{R}^J$, representing an attribute dictionary comprising functions $\\phi_j: X \\rightarrow \\mathbb{R}^+$, where $\\phi_j(x)$ captures the activation of a high-level attribute or a \u0026ldquo;concept\u0026rdquo; over $X$. Finally, $g$ computes the composition of the attribute dictionnary with an interpretable function $h: R^J \\rightarrow Y$.\n$$\n\\forall x \\in X, g(x) = h(\\Phi(x))\n$$\nFor now we take $h(x) = softmax(W^T \\Phi(x))$ but $h$ can be any interpretable function (like a decision tree for example).\u003c/p\u003e\n\u003cp\u003eNote that $d$ in the image is a decoder network that takes $\\Phi(x)$ and reconstructs the input $x$. This decoder is used for training and its purpose will be detailed later on in section 2.3.\u003c/p\u003e\n\u003ch2 id=\"section-2.2\"\u003e2.2 Interpretation in FLINT\u003c/h2\u003e\n\u003cp\u003eWith the interpreter defined, let\u0026rsquo;s clarify its role and interpretability objectives within FLINT. Interpretation serves as an additional task alongside prediction. We\u0026rsquo;re interested in two types: global interpretation, which aids in understanding which attribute functions contribute to predicting a class, and local interpretation, which pinpoints the attribute functions involved in predicting a specific sample.\u003c/p\u003e\n\u003cp\u003eTo interpret a local prediction $f(x)$, it\u0026rsquo;s crucial that the interpreter\u0026rsquo;s output $g(x)$ aligns with $f(x)$. Any discrepancy prompts analysis of conflicting data, potentially raising concerns about the prediction\u0026rsquo;s confidence.\u003c/p\u003e\n\u003cp\u003eTo establish local and global interpretation, we rely on attribute relevance. Given an interpreter with parameters $\\Theta_g = (\\theta_\\Psi, \\theta_h)$ and an input $x$, an attribute $\\phi_j$\u0026rsquo;s relevance is defined concerning the prediction $g(x) = f(x) = \\hat{y}$. The attribute\u0026rsquo;s contribution to the unnormalized score of class $\\hat{y}$ is $\\alpha_{j, \\hat{y}, x} = \\phi_j(x) \\cdot w_{j, \\hat{y}}$, where $w_{j, \\hat{y}}$ is the coefficient associated with this class. Relevance score $r_{j, x}$ is computed by normalizing $\\alpha$ as $r_{j, x} = \\frac{\\alpha_{j, \\hat{y}, x}}{\\max_i |\\alpha_{i, \\hat{y}, x}|}$. An attribute $\\phi_j$ is considered relevant for a local prediction if it\u0026rsquo;s both activated and effectively used in the linear model.\u003c/p\u003e\n\u003cp\u003eAttribute relevance extends to its overall importance in predicting any class $c$. This is achieved by averaging relevance scores from local interpretations over a random subset or the entirety of the training set $S$ where the predicted class is $c$. Thus, $r_{j, c} = \\frac{1}{|S_c|} \\sum_{x \\in S_c} r_{j, x}$, where $S_c = \\{x \\in S \\mid \\hat{y} = c\\}$.\u003c/p\u003e\n\u003cp\u003eNow, let\u0026rsquo;s introduce the local and global interpretations the interpreter will provide:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGlobal interpretation ($G(g, f)$) identifies class-attribute pairs $(c, \\phi_j)$ where the global relevance $r_{j, c}$ exceeds a threshold $\\frac{1}{\\tau}$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLocal interpretation ($L(x, g, f)$) for a sample $x$ includes attribute functions $\\phi_j$ with local relevance $r_{j, x}$ surpassing $\\frac{1}{\\tau}$. These definitions don\u0026rsquo;t assess interpretation quality directly.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-2.3\"\u003e2.3 Learning by imposing interpretability properties\u003c/h2\u003e\n\u003cp\u003eFor learning, the paper defines certain penalties to minimize, where each one aims to enforce a certain desirable property:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eFidelity to output:\u003c/strong\u003e\u003c/em\u003e The output of $g(x)=h(\\Psi(f_I(x)))$ should be close to $f(x)$ for any x. This can be imposed through a cross-entropy loss:\n$$\nL_{of}(f, g, S) = - \\sum_{x \\in S} h(\\Psi(f_I(x)))^T \\log(f(x))\n$$\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eConciseness and Diversity of Interpretations:\u003c/strong\u003e\u003c/em\u003e We aim for concise local interpretations, containing only essential attributes per sample, promoting clearer understanding and capturing high-level concepts. Simultaneously, we seek diverse interpretations across samples to prevent attribute functions from being class-exclusive. To achieve this, the paper proposes that we leverage entropy (defined for a vector as $\\mathcal{E}(v) = - \\sum_i p_i \\log(p_i)$), which quantifies uncertainty in real vectors. Conciseness is fostered by minimizing the entropy of the interpreter\u0026rsquo;s output, $\\Phi(x) = \\Psi(f_I(x))$, while diversity is encouraged by maximizing the entropy of the average $\\Psi(f_I(x))$ over a mini-batch. This approach promotes sparse and varied coding of $f_I(x)$, enhancing interpretability. However, as entropy-based losses lack attribute activation constraints, leading to suboptimal optimization, we also minimize the $l_1$ norm of $\\Psi(f_I(x))$ with hyperparameter $\\eta$. Although $l_1$-regularization commonly encourages sparsity, the experiments done in the paper show that entropy-based methods are more effective.\n$$\nL_{cd}(f, g, S) = -\\mathcal{E}(\\frac{1}{\\lvert S \\lvert} \\sum_{x \\in S} \\Psi(f_I(x))) + \\sum_{x \\in S} \\mathcal{E}(\\Psi(f_I(x))) + \\sum_{x \\in S} \\eta \\lVert \\Psi(f_I(x)) \\lVert_1\n$$\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eFidelity to input:\u003c/strong\u003e\u003c/em\u003e In order to promote the representation of intricate patterns associated with the input within $\\Phi(x)$, a decoder network $d : \\mathbb{R}^J \\rightarrow X$ is employed. This network is designed to take the attribute dictionary $\\Phi(x)=\\Psi(f_I(x))$ as input and reconstruct the original input $x$.\n$$\nL_{if}(f, g, d, S) = \\sum_{x \\in S} (d(\\Psi(f_I(x))) - x)^2\n$$\u003c/p\u003e\n\u003cp\u003eGiven the proposed loss terms, the loss for the interpretability model writes as follows:\n$$\nL_{int}(f, g, d, S) = \\beta L_{of}(f, g, S) + \\gamma L_{if}(f, g, d, S) + \\delta L_{cd}(f, g, S)\n$$\nWhere $\\beta, \\gamma, \\delta$ are non-negative hyperparameters. the total loss to be minimized $L = L_{pred} + L_{int}$, where the prediction loss, $L_{pred}$, is the well-know cross entropy loss (since this a classification problem).\u003c/p\u003e\n\u003ch1 id=\"section-3\"\u003e3 Understanding encoded concepts in FLINT\u003c/h1\u003e\n\u003cp\u003eOnce the predictor and interpreter networks are jointly learned, interpretation can be conducted at both global and local levels . A critical aspect highlighted by the authors is understanding the concepts encoded by each individual attribute function ​$\\phi_j$ . Focusing on image classification, the authors propose representing an encoded concept as a collection of visual patterns in the input space that strongly activate $\\phi_j$ . They present a pipeline for generating visualizations for both global and local interpretation, adapting various existing tools .\u003c/p\u003e\n\u003cp\u003eFor global interpretation visualization, the authors propose starting by selecting a small subset of training samples from a given class c that maximally activate ​$\\phi_j$ . This subset, referred to as Maximum Activating Samples (MAS), is denoted as $MAS(c , ​\\phi_j , l)$ where $l$ is the subset size (set as 3 in their experiments). However, while MAS provides some insight into the encoded concept, further analysis is required to understand the specific aspects of these samples that cause ​$\\phi_j$ activation. To achieve this, the authors propose utilizing a modified version of activation maximization called Activation Maximization with Partial Initialization (AM+PI). This technique aims to synthesize input that maximally activates ​$\\phi_j$ by optimizing a common activation maximization objective, initialized with a low-intensity version of the sample from MAS.\u003c/p\u003e\n\u003cp\u003eFor local analysis, given any test sample $x_{0}$ , its local interpretation $L(x_{0},f,g)$ can be determined, representing the relevant attribute functions . To visualize a relevant attribute ​$\\phi_j$, the authors suggest repeating the AM+PI procedure with initialization using a low-intensity version of $x_{0}$ to enhance the concept detected by ​$\\phi_j$ in $x_{0}$ .\u003c/p\u003e\n\u003ch2 id=\"section-4\"\u003e4 Reproducing the experiments\u003c/h2\u003e\n\u003cp\u003eIn the experimental section of the article, several experiments were conducted to do a quantitative evaluation of FLINT\u0026rsquo;s performance compared to other state-of-the-art models designed for interpretability, such as SENN and PrototypeDNN. Additionally, FLINT was compared to LIME and VIBI to evaluate the fidelity of its interpretations, measuring the proportion of samples where the predictions of a model and its interpreter agree. Across these tests, FLINT consistently outperformed the other models, demonstrating its reliability and effectiveness.\u003c/p\u003e\n\u003cp\u003eHowever, in this blog post we will specifically focus on reproducing the experiments in the article related to FLINT\u0026rsquo;s explainability, that aim to do a qualitative analysis of it. To achieve a thorough understanding of the model and its operational dynamics across prevalent datasets, we replicated the study by cloning the project from the GitHub repository referenced in the article (\u003ca href=\"https://github.com/jayneelparekh/FLINT\"\u003erepo link\u003c/a\u003e). Our experimentation involved the CIFAR10 and QuickDRAW datasets, employing a ResNet18-based network for both. For the QuickDRAW dataset, we utilized J=24 attributes, while for the CIFAR10 dataset, we used J=36 attributes.\u003c/p\u003e\n\u003cp\u003eThe instructions provided in the GitHub repository for executing the model are clear, and the model runs flawlessly. We have the option to either train the model ourselves or download the pre-trained models. Furthermore, there is a well-detailed Python notebook named \u0026ldquo;FLINT demo.ipynb\u0026rdquo;, which contains code for visualizing data, such as attribute relevance scores for each class and local interpretations for data samples. We will execute FLINT on test images and take a look at how interpretability is done with FLINT in this section.\u003c/p\u003e\n\u003ch3 id=\"section-4.1\"\u003e4.1 Global interpretation\u003c/h3\u003e\n\u003cp\u003eIn the article, the authors explore global interpretation using a figure similar to the one provided below which was reproduced from the notebook, and which illustrates the generated global relevances $r_{j,c}$ for all class-attribute pairs in the QuickDraw dataset.\u003c/p\u003e\n\u003c!-- ![Global class-attribute relevances](/images/FLINT/Global_class_attribute_QuickDRAW.png) --\u003e\n\u003cdiv style=\"text-align:center;\"\u003e\n    \u003cimg src=\"/images/FLINT/Global_class_attribute_QuickDRAW.png\" alt=\"Image\" width=\"300\" height=\"200\"\u003e\n\u003c/div\u003e\n\u003cp\u003eAdditionally, by running the model on the CIFAR10 and QuickDRAW dataset we got visual outputs representative of class-attribute pair analyses for both datasets. These outputs served as pivotal tools in elucidating interrelations and facilitating comparative assessments between attributes and classes. We present below two figures derived from the resultant class-attribute pair analyses for each of the 2 datasets. The class-attribute pairs shown are different from the examples shown in the paper.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/FLINT/Class_attribute_pair_CIFAR10.png\"\n  alt=\"Class-attribute pair analysis on dataset CIFAR10\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\n\u003cem\u003eCaption: Class-attribute pair analysis on dataset CIFAR10\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/FLINT/Class_attribute_pair_QuickDraw.png\"\n  alt=\"Class-attribute pair analysis on dataset QuickDraw\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\n\u003cem\u003eCaption: Class-attribute pair analysis on dataset QuickDraw\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWe focus on class-attribute pairs with high relevance, showcasing examples in the provided figure above . For each pair, we examine Maximum Activating Samples (MAS) alongside their corresponding Activation Maximization with Partial Initialization (AM+PI) outputs.\u003c/p\u003e\n\u003cp\u003eMAS analysis alone provides valuable insights into the encoded concept. For instance, on QuickDRAW dataset, attribute $\\phi_{16}$  relevant for class \u0026lsquo;Banana\u0026rsquo; activates the curve shape of the banana. However, AM+PI outputs offer deeper insights by elucidating which parts of the input activate an attribute function more clearly. And on CIFAR10 dataset , attribute $\\phi_{12}$ activates for \u0026lsquo;Deer\u0026rsquo; class , but the specific focus of the attribute remains ambiguous. The outputs of the AM+PI method indicate that attribute $\\phi_{12}$ predominantly highlights the area encompassing the legs and the horns of the deer, characterized as the most prominently enhanced regions.\u003c/p\u003e\n\u003ch3 id=\"section-4.2\"\u003e4.2 Local interpretation\u003c/h3\u003e\n\u003cp\u003eSimilarly to the article, we explored local interpretation through the figure provided below which was generated in the notebook, which showcases visualizations for 4 test samples of the QuickDRAW dataset. Both predictor $f$ and interpreter $g$ accurately predict the true class in all cases, for the first 2 it\u0026rsquo;s \u0026ldquo;Cat\u0026rdquo; and the last 2 it\u0026rsquo;s \u0026ldquo;Banana\u0026rdquo;. For each case, they highlighted the top 3 relevant attributes to the prediction along with their relevances and corresponding AM+PI outputs.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/FLINT/Local_interpretations.jpg\"\n  alt=\"Local interpretations for test samples\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAnalysis of the AM+PI outputs reveals that attribute functions generally activate for patterns corresponding to the same concept inferred during global analysis. This consistency is evident for attribute functions present in the previous figures. Additionaly, by looking at the figure showing the relevance of class-attribute pairs in section 4.1 for the QuickDRAW dataset we observe that the 3 most important features for each class in the local interpretations are also those having the highest relevence for these classes. For example for the \u0026ldquo;Banana\u0026rdquo; class, $\\phi_{16}$, which activates the curve shape, is by far the most important feature for identifying this class by looking at both the local interpretations and the class-attribute relevences. While for the \u0026ldquo;Cat\u0026rdquo; class, it seems that the most important features are in order $\\phi_{23}$, $\\phi_1$ and $\\phi_{19}$ when looking at both the local interpretations and the class-attribute relevences.\u003c/p\u003e\n\u003ch2 id=\"section-5\"\u003e5 Subjective evaluation\u003c/h2\u003e\n\u003cp\u003eIn the article,  a subjective evaluation survey with 20 respondents using the QuickDraw dataset to assess FLINT\u0026rsquo;s interpretability is conducted. The authors selected 10 attributes covering 17 class-attribute pairs and presented visualizations (3 MAS and AM+PI outputs) along with textual descriptions for each attribute to the respondents. They were asked to indicate their level of agreement with the association between the descriptions and the patterns in the visualizations using predefined choices.\u003c/p\u003e\n\u003cp\u003eDescriptions were manually generated, including 40% incorrect ones to ensure informed responses. Results showed that for correct descriptions, 77.5% of respondents agreed, 10.0% were unsure, and 12.5% disagreed. For incorrect descriptions, 83.7% disagreed, 7.5% were unsure, and 8.8% agreed. These results affirm that the concepts encoded in FLINT\u0026rsquo;s learned attributes are understandable to humans.\u003c/p\u003e\n\u003ch1 id=\"section-6\"\u003e6 Specialization of FLINT to post-hoc interpretability\u003c/h1\u003e\n\u003cp\u003eFLINT primarily aims for interpretability by design, but the authors of the article propose that it can also be adapted to provide post-hoc interpretations when a classifier $\\hat{f}$ is already available. Post-hoc interpretation learning, a special case of SLI, involves building an interpreter for $\\hat{f}$ by minimizing a certain objective function. Specifically, Given a classifier\n$\\hat{f} \\in F$ and a training set $S$, the goal is to build an interpreter of $\\hat{f}$ by solving:\n$$\n\\text{arg} \\min_{g \\in G_{f}} L_{int}(\\hat{f}, g, S)\n$$\nWhere $g(x)=h(\\Phi(\\hat{f_I} (x)))$ for a given set of $I$ hidden layers and an attribute dictionnary of size $J$. The learning is performed the same as before but we only keep the parameters $\\theta_\\Psi$, $\\theta_h$ and $\\theta_d$. We fix $\\theta_\\hat{f}$ and remove $L_{pred}$ from the training loss $L$.\u003c/p\u003e\n\u003cp\u003eThere are experimental results in the article and in the supplements that are not mentionned here that demonstrate the effectiveness of post-hoc interpretation within FLINT, showing that even without fine-tuning the internal layers of the classifier, meaningful interpretations can be generated with high fidelity.\u003c/p\u003e\n\u003ch1 id=\"section-7\"\u003e7 Conclusion\u003c/h1\u003e\n\u003cp\u003eIn conclusion, FLINT offers a robust framework for enhancing the interpretability of machine learning models, particularly deep neural networks, in critical domains like healthcare, law, and defense. By jointly learning predictor and interpreter models, FLINT addresses the challenge of providing both global and local interpretations of model predictions. Through carefully designed loss functions, FLINT ensures fidelity to input and output, promotes concise and diverse interpretations, and facilitates the representation of intricate patterns associated with input data. Reproducing experiments on datasets such as CIFAR10 and QuickDRAW showcases FLINT\u0026rsquo;s effectiveness in providing interpretable insights into model predictions. Subjective evaluations affirm the understandability of FLINT\u0026rsquo;s learned attributes, reinforcing its potential for real-world applications. Moreover, FLINT\u0026rsquo;s adaptability for post-hoc interpretability underscores its versatility, enabling meaningful interpretations without extensive modification of the underlying classifier. Overall, FLINT emerges as a valuable tool for fostering transparency and trust in complex machine learning models, contributing to the development of interpretable AI systems across various domains.\u003c/p\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/a-framework-to-learn-with-interpretation/","date_published":"13026-13-09T256:1313:00+01:00","date_modified":"13026-13-09T256:1313:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"de45490fdb0386b037083151783da0d850a9940c","title":"Do Perceptually Aligned Gradients imply Robustness?","summary":"","content_text":" Robustness and Perceptually Aligned Gradients : does the converse stand ? Author: Yohann Zerbib Table of Contents Introduction Adversarial Attacks Perceptually Aligned Gradients Experiment To go further Conclusion References Introduction In the context of image recognition in Machine Learning, one could quickly realize that building robust models is crucial. Having failures could potentially lead to worrying outcomes and it is part of the design to aim to implement models that would be prevented against adversarials attacks, that will be explained. At some point, when reaching models that are robust, it somehow occurs that small variations made are easily interpretable by humans, something which is not common in current ML models such as this one. Having noticed this phenomenon, the authors of the paper would try to verify the opposite assumption. By building models that verify this idea of alignment with human perception, do we create robust models ?\nAdversarial attacks But before explaining the article, it could be relevant to explain briefly what are adversarial attacks and how it led to the design of robustness.\nAdversarial attacks refer to a class of techniques in machine learning where intentionally crafted input data is used to deceive or mislead a model, leading it to make incorrect predictions or classifications. These attacks exploit vulnerabilities in the model\u0026rsquo;s decision-making process, taking advantage of the model\u0026rsquo;s sensitivity to small changes in input data that might be imperceptible to humans. They are most prominently associated with deep learning models, particularly neural networks, due to their high capacity and ability to learn complex patterns.\nConcretly, in a theoretical framework, the usual example is to make a model classify an image of a cat as a dog or another animal, without any way for the human to notice it. However, consequences can be more dreadful in real life as one could consider what would happen if an autonomous vehicles missclassified a stop sign as speed limit sign.\n(Eykholt et al. [1])\nNow, let\u0026rsquo;s dive a bit deeper to understand how these errors happen. Several points can be highlighted, such as the level of linearity of Neural Networks, but one acknowledged moot point dwells on the use of Loss function in Deep Learning methods. Indeed, especially when considering datasets of pictures, there are many directions where the loss is steep. It would mean that it can be highly delicate to propose a good minimization of the loss. Moreover, the main idea for our problem is that a small change of the input can cause abrupt shifts in the decision process of our model. This effect increases with the dimensionnality (quality of pictures\u0026hellip;) and therefore will still be relevant with time.\nThe basic modelisation of an attack would be the following. Let\u0026rsquo;s consider :\na model $f\\ :\\ \\mathcal{X} \\ \\rightarrow \\ \\mathcal{Y}$ the input to pertub : $x \\in \\mathcal{X}$ a potential target label : $t \\in \\mathcal{Y}$ a small perturbation : $\\eta$ Then, mathematically, the attacker would try to have something that verifies $f(x + \\eta) = t$ (or any other label than $f(x)$ for an untargeted attack).\nNow, as one can imagine, it is possible to compute attacking models related to this framework. Let\u0026rsquo;s understand two well-knowns algorithms that follow this goal.\nFast Gradient Sign Method (FGSM) : This method can be targeted or untargeted. Let\u0026rsquo;s study the targeted one. The algorithm is the following [3]: One compute the perturbation $\\eta \\ =\\ \\epsilon \\ \\cdotp \\ sign( \\ \\nabla x\\ L( x,\\ t) \\ )$ where $\\epsilon$ is the perturbation size. Then, one would have $x\u0026rsquo;\\ =\\ x\\ −\\ \\eta $ such that we remain espilon close from $x$ and that $f(x\u0026rsquo;) = t$. The perturbation has to remain small to ensure it will be undetected by human\u0026rsquo;s perception.\nBut, at this point, one question arises : how can we be sure that $x\u0026rsquo;$ is still close to $x$? How can we be sure that we have $||x\\ −\\ x\u0026rsquo;||_{p} \\ \\leq \\ \\epsilon $ where p is a particular norm? To answer this question, norms are introduced and two important ones, used in the article are the following.\n$L_{2 }$ norm : This norm captures the global quantity of changes. It is the euclidean distance.\n$L_{\\infty }$ : This norm captures the maximum change in the vector.\nSo, we have several ways to have a level of control over the changed features.\nNow that the first intuition for attack is understood, one should take a rapid look at PGD (Projected Gradient Descent) [4], which will be used for the results of this blog. Other more complex methods exist (AutoAttack), and they are taken into account by the authors but they will not be explained here.\nThe algorithm starts with an initial perturbation. At each iteration, the algorithm takes a step in the direction of the gradient of the loss function with respect to the input. The gradient is calculated using backpropagation, and represents the direction of steepest ascent in the loss function. However, since we\u0026rsquo;re trying to reach a specific target, we actually want to move in the opposite direction, so we multiply the gradient by -1 (it is a maximization). The step size is proportional to the norm of the gradient, so we don\u0026rsquo;t overshoot or undershoot our target. After taking a step, the perturbation is projected back onto the allowed range, which is defined by the epsilon parameter. This is done by calculating the difference between the current input and the original input, and then scaling this difference so that it falls within the allowed range. This process is repeated for a certain number of iterations. (In this version of the algorithm, there is no control that it will truly be missclassified : one has to set an improtant enough number of iterations).\nHowever, our role here is not to learn how to create the best attacks, but more to learn how to defend them! And suprisingly, what has been shown is that the best way to achieve this goal is to have a training that includes adversarial attacks. Then, it all comes down to this optimization problem :\n$\\min_{\\theta }$ $\\mathbb{E}_{(x, y)} $ [A] where\nA = $(\\max_{\\eta \\leqslant \\epsilon }$ $L( f_{\\theta}( x\\ +\\ \\eta ) ,\\ y))$\nThis is more or less an optimization problem to solve with $\\theta$ the parameters to be learnt and where each training sample has a perturbation (an attack). It is linked with adversarial accuracy. We can train a model to be more robust, but chances are it will be less performant. It is up to the trainer to choose the best trade-off on a model.\nPerceptually Aligned gradients Finally, it is possible to dive more in the subject of the article. Training models as presented before, with a particular care to robustness empirically leads to have perceptually aligned gradients. Here, one should understand \u0026ldquo;gradient\u0026rdquo; as the mathematical concept, a vector which points to the direction of the greatest increase of its function. In other words, Perceptually Aligned Gradients correspond to a property, a byproduct of robust models, where the gradients are meaningful to humans. When the input image is slightly modified, the corresponding gradient directions reflect the changes that are perceptually relevant. In other words, the gradients make sense from a human perspective.\nHere an example given by the author on the CIFAR dataset ([2], Ganz et al.). The intuition is that for models other than the vanilla one, the target class representative of the adversarial examples contains an information about the new class. For example, going from a bird to a frog will get the image much more green and in the shape of the frog. It looks like a ghost information.\nNow, is it a Bidirectional Connection ? Let\u0026rsquo;s try to have some hints about it.\nThe first step to tackle this issue is to create those Perceptually Aligned Gradients without adversarial training.\nThen, it is shown that models with aligneds gradients can be considered as robust.\nFinally, a demonstration of the improvement of robustness through the increase of gradient alignment is proposed.\n1. Algorithm of the Model\nTo disentangle the creation of PAG with the usual robust training, a new method is developed. It relies on two elements.\nthe classical cross-entropy loss from the usual categorization problem framework,\nan auxiliary loss on the input-gradients, differentiable.\nThen, our global loss function would look like this :\n$L( x,\\ y) \\ =LCE\\ ( f_{\\theta }( x) ,\\ y) \\ + \\lambda\\sum_{y_{t} =1}^{C}L_{cos}( \\nabla_{x}f_{\\theta }(x)_{y_t},\\ g( x,\\ y_t))$\nIt is similar to training with a regularization part ($\\lambda$ would control the power of the regularization). $L_{cos}$ is the cosine similarity loss (it gives information on the similarity of the arguments).\nThis does not use robust model of any sort, on the hypothesis that we have ground-true PAG in the input. This is a strong hypothesis, and it is crucial to choose well those grounds-truth. Indeed, a lack of rigor here could lead to a bias. If the ground-truth was obtained through adversarial training previously, then this new approach would only be an equivalent of adversarial training, and that is something that must be avoided. This hypotesis will be studied just a bit later.\nAfter minimizing the loss, the model is tested through adversarial attacks (here, targeted PGD on the test set) to see if there is clearly PAG and if the adversarial accuracy is good.\n2. Creation of Perceptually Aligned Gradients\nAs we have seen in the formula just above, it is mandatory to have a ground-truth perceptually gradient $g( x,\\ y_t)$ for each training image and for each target class. However, finding those gradients are difficult and they are approximated. Firstly, let\u0026rsquo;s consider the heuristics to understand what happens.\nWith this objective in mind, we follow a straightforward assumption: the gradient $g( x,\\ y_t)$ ought to align with the overall direction of images belonging to the target class $y_t$. Hence, when provided with a target class representative, $r_{y_t}$, we establish the gradient to direct away from the current image and towards the representative. In other words, $g( x,\\ y_t) = r_{y_t} - x$\nTo implement this heuristic, three setups are provided.\n$\\textbf{One Image (OI):}$ Choose an arbitrary training set image with label $y_t$, and set $r_{y_t}$ to be that image as a global destination for $y_t$-targeted gradients.\n$\\textbf{Class Mean (CM):}$ Set $r_{y_t}$ to be the mean of all the training images with label $y_t$. This mean can be multiplied by a constant to obtain an image-like norm.\n$\\textbf{Nearest Neighbor (NN):}$ For each image $x$ and each target class$\\ y_{t} \\ \\in \\ {{1,\\ 2\\ .\\ .\\ .\\ ,\\ C}}$, we set the class representative $r_{y_t}(x)$ (now dependent on the image) to be the image\u0026rsquo;s nearest neighbor amongst a limited set of samples from class $y_t$, using L2 distance in the pixel space. More formally, we define $r( x,\\ y_{t}) \\ \\ =\\ \\underset{ \\begin{array}{l} \\widehat{x\\ } \\in \\ D_{y_{t}} \\ s.t.\\ \\hat{x} =x \\end{array}}{\\arg\\min} ||x\\ −\\ \\hat{x} ||_{2}{}$\nwhere $ D_{y_{t}}$ is the set of sample images with class $y_t$.\nNow, the more theoretical approach is provided thanks to score-based gradients. Authors have used Denoising Diffusion Probabilistic Models (DDPMs), to generate approximations of PAG.\nLet\u0026rsquo;s consider noisy versions of an image $x$, noted as $({x_{t}})_{t=1}^{T}$ and their distribution\n$(p_t({x_{t})})_{t=1}^{T}$.\nAn iterative process is employed for sampling, which begins from Gaussian noise and proceeds along the direction of the score function, defined as $\\nabla_{x_t} \\log p(x_t)$ and approximated by a neural network. It is suggested to incorporate class information into these networks, allowing them to model a class-dependent score function $\\nabla_{x_t} \\log p(x_t|y)$. We identify a resemblance between the class-dependent score function and classification loss gradients with respect to the input image, leading us to propose that gradients derived from DDPM can serve as an enhanced source for perceptually aligned gradients. We would have (one term disappears with the gradient w.r.t the input image) using Bayes\u0026rsquo; formula.\n\\begin{equation} \\nabla_{x_t} \\log p(x_t|y) = \\nabla_{x_t} \\log p(y|x_t) + \\nabla_{x_t} \\log p(x_t), \\end{equation}\nwhich results in\n\\begin{equation} \\nabla_{x_t} \\log p(y|x_t) = \\nabla_{x_t} \\log p(x_t|y) - \\nabla_{x_t} \\log p(x_t). \\end{equation}\nThis formulation introduces a new application of diffusion models – a systematic approach to estimate the appropriate gradients for the expression $\\log p(y|x_t)$. However, classification networks operate on noise-free images ($x$) rather than noisy ones ($x_t$). To link classifier input-gradients with DDPMs, we assume that $\\log p(y|x) \\approx log p(y|x_t)$, for certain noise levels $t$. Consequently, the desired estimation of \u0026ldquo;ground-truth\u0026rdquo; classifier input-gradients can be acquired by subtracting an unconditional score function from a class-conditional one. The selection of $t$ when distilling gradients through this method presents a tradeoff – excessively large values yield gradients unrelated to the input image (too noisy), while excessively small values produce perceptually insignificant ones (in low noise levels, the conditional and unconditional scores are nearly identical). Therefore, we choose $t$ to be of moderate values, generating both perceptually and image-relevant gradients. We denote this method as Score-Based Gradients (SBG).\nTo understand a bit more how it works, one has to consider that the variations of the noise from every $x_t$ can be controlled. Indeed, each different iteration takes the direction of the distribution $\\log p(x_t)$ (with stochasticity). In other terms, it takes the direction of our score function that can be estimated thanks to Neural Networks. That\u0026rsquo;s how you obtain your set of ground-truth gradients related to the input images.\nAt this point, we have four ways to approximate ground-truth gradients. (Three heuristics and a more theoretical one). The experiments presented here will use the NN approach that are very intuitive. What was favoured for real datasets was the score-based approach.\nExperiment Now, let\u0026rsquo;s experiment a bit. In this article, to understand what is happening, we will play a bit with the toy dataset. A 2 dimensional synthetic dataset is built. It contains 6000 samples of 2 classes. Every sample is on the line of equation $x_2 -2x_1=0$. Finally, each class contains three mods (1000 samples per mode) drawn from a Gaussian distribution. The idea is to observe manifolds as decision boundaries. Background of the plan will be colored according to the predicted class. Evaluation will be made on a test set.\nThe code is available at this link.\nTo this prediction task, a simple 2 layers MLP with ReLU is used. Two training are made with the same seed. The first is based on the usual cross-entropy loss whereas the second is made on the explained new loss.\nAs expected, 100% accuracy is obtained for this very simple task for both models on the test set. However, what about predicting adversarial examples ?\nLet\u0026rsquo;s first try it out with a targeted $L2$ PGD. Vanilla is only correct for 35 out of 600 samples, whereas this new approach obtains 583 out of 600. How can this be explained ? One should observe the decision boundaries.\nThis is what is obtained for the regular neural network with cross-entropy Loss.\nHere is the result obtained for the particular neural network with the new loss.\nWhat one should notice is the decision boundaries. The vanilla neural network provides manifolds that really stick to the data points. Going just a bit further can on the graph really can create a shift in the prediction. And that is what is happening with a targeted pgd, where there is only a small variation (semantically invisible).\nHowever, in the case of the PAG Neural Network, one can observe that around a mode of points, there is a much greater margin of the same class. This can be understood from the setup to create perceptually aligned gradients. Indeed, as we have seen, a target class was set based on a nearest neighbour approach, and the gradient point away from the current image and towards the class representative. Only then the cosine similarity between this gradient and the ground-truth approximated one from DDPMs.\nAnother possibility would be to see the impact of the size of the perturbation on the performance. Indeed, here, the given results corresponded to an epsilon value of 15. Increasing it decreases the accuracy to 75%. However, at a certain point, an augmentation of epsilon will not change anything anymore, probably because of a normalizing step in the targeted PGD algorithm.\nTo go further What\u0026rsquo;s next ? Testing the hypothesis on real datasets. Among them, CIFAR-10, STL (higher resolution) and CIFAR-100 (higher number of classes). The architecture to achieve those tasks are classical (Resnet-18, ViT). Here are the main results that can be highlighted.\nPAG approach is often similar and sometimes outperforms adversarially training approach. Score-based gradient seems to be the most accurate ground-truth approximation setup. It is also more notable for the ViT architecture. It also globally performs well on STL and CIFAR-100 (sometimes even better than adversarially training).\nBut, the question is not yet answered : Do Perceptually Aligned Gradients imply Robustness?\nAnd that\u0026rsquo;s where the regularization aspect of the loss is very useful. One can make variation over the hyperparameter $\\lambda$ to see what brings a bigger focus on the PAG loss. The authors have done it and are summarized with this table.\nAs one can see, the robustness increases with the increase of the regularization hyperparameter. The more the ghost features of the target class are visible (even if it not always comprehensible), the more the model is robust.\nSo, it seems that yes, models with PAG would be more robust.\nConclusion To draw a conclusion, this paper has empirically shown that PAG lead to more robustness in models. It was also mentionned that it could potentially be combined with Adversarially Training to gain more robustness, and there are probably some experiments and tests that could optimize that. The performance are also good and can be seen as an alternative, potentially not too costly. Sometimes it ouperforms Adversarially Training and it would be up to the user to decide which framework to employ for creating robust models. Finally, approximating ground-truth PAG needs additionnal research and discussion as even if the results tend to favour Score-Based Gradients, it happens that heuristics function better and there are potentially other approaches that have yet to be discovered. One should shed light on the fact that the diffusion models used need to be trained, and the training time gained over adversarially training is not as significant as with other heuristics if we consider this aspect.\nReferences EYKHOLT, Kevin, EVTIMOV, Ivan, FERNANDES, Earlence, et al. Robust physical-world attacks on deep learning visual classification. In : Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. p. 1625-1634.\nGanz, R., Kawar, B., \u0026amp; Elad, M. (2023, July). Do perceptually aligned gradients imply robustness?. In International Conference on Machine Learning (pp. 10628-10648). PMLR.\nGoodfellow, I. J., Shlens, J., \u0026amp; Szegedy, C. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., \u0026amp; Vladu, A. (2017). Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083.\n","content_html":"\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n\u003ch1 style=\"font-size: 36px;\"\u003eRobustness and Perceptually Aligned Gradients : does the converse stand ?\u003c/h1\u003e\n\u003ch3 style=\"font-size: 24px;\"\u003eAuthor: Yohann Zerbib\u003c/h3\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eAdversarial Attacks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003ePerceptually Aligned Gradients\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eExperiment\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eTo go further\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn the context of image recognition in Machine Learning, one could quickly realize that building \u003cem\u003erobust\u003c/em\u003e models is crucial. Having failures could potentially lead to worrying outcomes and it is part of the design to aim to implement models that would be prevented against \u003cem\u003e\u003cstrong\u003eadversarials attacks\u003c/strong\u003e\u003c/em\u003e, that will be explained. At some point, when reaching models that are robust, it somehow occurs that small variations made are easily \u003cstrong\u003einterpretable by humans\u003c/strong\u003e, something which is not common in current ML models such as this one. Having noticed this phenomenon, the authors of the paper would try to verify the opposite assumption. By building models that verify this idea of alignment with human perception, do we create robust models ?\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eAdversarial attacks\u003c/h2\u003e\n\u003cp\u003eBut before explaining the article, it could be relevant to explain briefly what are adversarial attacks and how it led to the design of robustness.\u003c/p\u003e\n\u003cp\u003eAdversarial attacks refer to a class of techniques in machine learning where \u003cstrong\u003eintentionally crafted input data\u003c/strong\u003e is used to deceive or mislead a model, leading it to make incorrect predictions or classifications. These attacks exploit vulnerabilities in the model\u0026rsquo;s decision-making process, taking advantage of the model\u0026rsquo;s sensitivity to small changes in input data that might be \u003cstrong\u003eimperceptible to humans\u003c/strong\u003e.\nThey are most prominently associated with deep learning models, particularly neural networks, due to their high capacity and ability to learn complex patterns.\u003c/p\u003e\n\u003cp\u003eConcretly, in a theoretical framework, the usual example is to make a model classify an image of a cat as a dog or another animal, without any way for the human to notice it. However, consequences can be more dreadful in real life as one could consider what would happen if an autonomous vehicles missclassified a \u003cem\u003e\u003cstrong\u003estop sign as speed limit sign\u003c/strong\u003e\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Yohann_Zerbib/stop.png\"\n  alt=\"stop\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003e(Eykholt et al. [1])\u003c/p\u003e\n\u003cp\u003eNow, let\u0026rsquo;s dive a bit deeper to understand how these errors happen.\nSeveral points can be highlighted, such as the level of linearity of Neural Networks, but one acknowledged moot point dwells on the use of Loss function in Deep Learning methods. Indeed, especially when considering datasets of pictures, there are many directions where the \u003cstrong\u003eloss is steep\u003c/strong\u003e. It would mean that it can be highly delicate to propose a good minimization of the loss. Moreover, the main idea for our problem is that a \u003cstrong\u003esmall change\u003c/strong\u003e of the input can cause \u003cstrong\u003eabrupt shifts\u003c/strong\u003e in the decision process of our model. This effect increases with the dimensionnality (quality of pictures\u0026hellip;) and therefore will still be relevant with time.\u003c/p\u003e\n\u003cp\u003eThe basic modelisation of an attack would be the following. Let\u0026rsquo;s consider :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea model $f\\ :\\ \\mathcal{X} \\ \\rightarrow \\ \\mathcal{Y}$\u003c/li\u003e\n\u003cli\u003ethe input to pertub : $x \\in \\mathcal{X}$\u003c/li\u003e\n\u003cli\u003ea potential target label : $t \\in  \\mathcal{Y}$\u003c/li\u003e\n\u003cli\u003ea small perturbation : $\\eta$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen, mathematically, the attacker would try to have something that verifies $f(x + \\eta) = t$ (or any other label than $f(x)$ for an untargeted attack).\u003c/p\u003e\n\u003cp\u003eNow, as one can imagine, it is possible to compute attacking models related to this framework. Let\u0026rsquo;s understand two well-knowns algorithms that follow this goal.\u003c/p\u003e\n\u003ch3 id=\"fast-gradient-sign-method-fgsm-\"\u003eFast Gradient Sign Method (FGSM) :\u003c/h3\u003e\n\u003cp\u003eThis method can be \u003cem\u003e\u003cstrong\u003etargeted\u003c/strong\u003e\u003c/em\u003e or \u003cem\u003e\u003cstrong\u003euntargeted\u003c/strong\u003e\u003c/em\u003e. Let\u0026rsquo;s study the targeted one. The algorithm is the following [3]:\nOne compute the perturbation $\\eta \\ =\\ \\epsilon \\ \\cdotp \\ sign( \\ \\nabla x\\ L( x,\\ t) \\ )$ where $\\epsilon$ is the perturbation size. Then, one would have $x\u0026rsquo;\\ =\\ x\\ −\\ \\eta $ such that we remain espilon close from $x$ and that $f(x\u0026rsquo;) = t$.\nThe perturbation has to remain small to ensure it will be undetected by human\u0026rsquo;s perception.\u003c/p\u003e\n\u003cp\u003eBut, at this point, one question arises : how can we be sure that $x\u0026rsquo;$ is still close to $x$? How can we be sure that we have $||x\\ −\\ x\u0026rsquo;||_{p} \\ \\leq \\ \\epsilon $ where p is a particular norm? To answer this question, norms are introduced and two important ones, used in the article are the following.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e$L_{2 }$ norm : This norm captures the \u003cstrong\u003eglobal quantity of changes\u003c/strong\u003e. It is the euclidean distance.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e$L_{\\infty }$ : This norm captures the \u003cstrong\u003emaximum change\u003c/strong\u003e in the vector.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSo, we have several ways to have a level of control over the changed features.\u003c/p\u003e\n\u003cp\u003eNow that the first intuition for attack is understood, one should take a rapid look at \u003cstrong\u003ePGD\u003c/strong\u003e (Projected Gradient Descent) [4], which will be used for the results of this blog. Other more complex methods exist (AutoAttack), and they are taken into account by the authors but they will not be explained here.\u003c/p\u003e\n\u003cp\u003eThe algorithm starts with an initial perturbation. At each iteration, the algorithm takes a step in the direction of the gradient of the loss function with respect to the input. The gradient is calculated using backpropagation, and represents the direction of steepest ascent in the loss function. However, since we\u0026rsquo;re trying to reach a specific target, we actually want to move in the \u003cstrong\u003eopposite direction\u003c/strong\u003e, so we multiply the gradient by -1 (it is a maximization). The step size is proportional to the norm of the gradient, so we don\u0026rsquo;t overshoot or undershoot our target.\nAfter taking a step, the perturbation is \u003cem\u003eprojected\u003c/em\u003e back onto the allowed range, which is defined by the epsilon parameter. This is done by calculating the difference between the current input and the original input, and then scaling this difference so that it falls within the allowed range.\nThis process is repeated for a certain number of iterations. (In this version of the algorithm, there is no control that it will truly be missclassified : one has to set an improtant enough number of iterations).\u003c/p\u003e\n\u003cp\u003eHowever, our role here is not to learn how to create the best attacks, but more to learn how to \u003cem\u003e\u003cstrong\u003edefend\u003c/strong\u003e\u003c/em\u003e them! And suprisingly, what has been shown is that the best way to achieve this goal is to have a training that includes adversarial attacks.\nThen, it all comes down to this optimization problem :\u003c/p\u003e\n\u003cp\u003e$\\min_{\\theta }$ $\\mathbb{E}_{(x, y)} $ [A] where\u003c/p\u003e\n\u003cp\u003eA = $(\\max_{\\eta \\leqslant \\epsilon }$ $L( f_{\\theta}( x\\ +\\ \\eta ) ,\\ y))$\u003c/p\u003e\n\u003cp\u003eThis is more or less an optimization problem to solve with $\\theta$ the parameters to be learnt and where each training sample has a perturbation (an attack). It is linked with adversarial accuracy. We can train a model to be more robust, but chances are it will be less performant. It is up to the trainer to choose the \u003cstrong\u003ebest trade-off\u003c/strong\u003e on a model.\u003c/p\u003e\n\u003ch2 id=\"section-2\"\u003ePerceptually Aligned gradients\u003c/h2\u003e\n\u003cp\u003eFinally, it is possible to dive more in the subject of the article. Training models as presented before, with a particular care to robustness empirically leads to have \u003cem\u003eperceptually aligned gradients\u003c/em\u003e.\nHere, one should understand \u0026ldquo;gradient\u0026rdquo; as the mathematical concept, a vector which points to the direction of the greatest increase of its function. In other words, Perceptually Aligned Gradients correspond to a property, a byproduct of robust models, where the gradients are meaningful to humans. When the input image is slightly modified, the corresponding gradient directions reflect the changes that are \u003cstrong\u003eperceptually relevant\u003c/strong\u003e. In other words, the gradients \u003cem\u003emake sense\u003c/em\u003e from a human perspective.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Yohann_Zerbib/pagdemo.png\"\n  alt=\"demopag\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eHere an example given by the author on the CIFAR dataset ([2], Ganz et al.). The intuition is that for models other than the vanilla one, the target class representative of the adversarial examples contains an information about the new class. For example, going from a bird to a frog will get the image much more green and in the shape of the frog. It looks like a \u003cem\u003eghost\u003c/em\u003e information.\u003c/p\u003e\n\u003cp\u003eNow, is it a Bidirectional Connection ? Let\u0026rsquo;s try to have some hints about it.\u003c/p\u003e\n\u003cp\u003eThe first step to tackle this issue is to create those Perceptually Aligned Gradients without adversarial training.\u003c/p\u003e\n\u003cp\u003eThen, it is shown that models with aligneds gradients can be considered as robust.\u003c/p\u003e\n\u003cp\u003eFinally, a demonstration of the improvement of robustness through the increase of gradient alignment is proposed.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Algorithm of the Model\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTo disentangle the creation of PAG with the usual robust training, a new method is developed. It relies on two elements.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ethe classical cross-entropy loss from the usual categorization problem framework,\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ean auxiliary loss on the input-gradients, differentiable.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen, our global loss function would look like this :\u003c/p\u003e\n\u003cp\u003e$L( x,\\ y) \\ =LCE\\ ( f_{\\theta }( x) ,\\ y) \\ + \\lambda\\sum_{y_{t} =1}^{C}L_{cos}( \\nabla_{x}f_{\\theta }(x)_{y_t},\\ g( x,\\ y_t))$\u003c/p\u003e\n\u003cp\u003eIt is similar to training with a regularization part ($\\lambda$ would control the power of the regularization). $L_{cos}$ is the cosine similarity loss (it gives information on the similarity of the arguments).\u003c/p\u003e\n\u003cp\u003eThis does not use robust model of any sort, on the hypothesis that we have \u003cstrong\u003eground-true PAG\u003c/strong\u003e in the input. This is a \u003cstrong\u003estrong hypothesis\u003c/strong\u003e, and it is crucial to choose well those grounds-truth. Indeed, a lack of rigor here could lead to a bias. If the ground-truth was obtained through adversarial training previously, then this new approach would only be an equivalent of adversarial training, and that is something that must be avoided. This hypotesis will be studied just a bit later.\u003c/p\u003e\n\u003cp\u003eAfter minimizing the loss, the model is tested through adversarial attacks (here, targeted PGD on the test set) to see if there is clearly PAG and if the adversarial accuracy is good.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Creation of Perceptually Aligned Gradients\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAs we have seen in the formula just above, it is mandatory to have a ground-truth perceptually gradient $g( x,\\ y_t)$ for each training image and for each target class. However, finding those gradients are difficult and they are \u003cstrong\u003eapproximated\u003c/strong\u003e. Firstly, let\u0026rsquo;s consider the heuristics to understand what happens.\u003c/p\u003e\n\u003cp\u003eWith this objective in mind, we follow a straightforward assumption: the gradient $g( x,\\ y_t)$ ought to align with the overall direction of images belonging to the target class $y_t$. Hence, when provided with a target class representative, $r_{y_t}$, we establish the gradient to direct away from the current image and towards the representative. In other words, $g( x,\\ y_t) = r_{y_t} - x$\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Yohann_Zerbib/target.png\"\n  alt=\"target\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eTo implement this heuristic, three setups are provided.\u003c/p\u003e\n\u003cp\u003e$\\textbf{One Image (OI):}$ Choose an arbitrary training set image with label $y_t$, and set $r_{y_t}$ to be that image as a global destination for $y_t$-targeted gradients.\u003c/p\u003e\n\u003cp\u003e$\\textbf{Class Mean (CM):}$ Set $r_{y_t}$ to be the mean of all the training images with label $y_t$. This mean can be multiplied by a constant to obtain an image-like norm.\u003c/p\u003e\n\u003cp\u003e$\\textbf{Nearest Neighbor (NN):}$  For each image $x$ and each target class$\\ y_{t} \\ \\in \\ {{1,\\ 2\\ .\\ .\\ .\\ ,\\ C}}$, we set the class representative $r_{y_t}(x)$ (now dependent on the image) to be the image\u0026rsquo;s nearest neighbor amongst a limited set of samples from class $y_t$, using L2 distance in the pixel space. More formally, we define\n$r( x,\\ y_{t}) \\ \\ =\\ \\underset{ \\begin{array}{l}\n\\widehat{x\\ } \\in \\ D_{y_{t}} \\ s.t.\\ \\hat{x} =x\n\\end{array}}{\\arg\\min} ||x\\ −\\ \\hat{x} ||_{2}{}$\u003c/p\u003e\n\u003cp\u003ewhere $ D_{y_{t}}$\nis the set of sample images with class $y_t$.\u003c/p\u003e\n\u003cp\u003eNow, the more theoretical approach is provided thanks to score-based gradients. Authors have used \u003cstrong\u003eDenoising Diffusion Probabilistic Models\u003c/strong\u003e (DDPMs), to generate approximations of PAG.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s consider noisy versions of an image $x$, noted as $({x_{t}})_{t=1}^{T}$ and their distribution\u003c/p\u003e\n\u003cp\u003e$(p_t({x_{t})})_{t=1}^{T}$.\u003c/p\u003e\n\u003cp\u003eAn iterative process is employed for sampling, which begins from Gaussian noise and proceeds along the direction of the score function, defined as $\\nabla_{x_t} \\log p(x_t)$ and approximated by a neural network. It is suggested to incorporate class information into these networks, allowing them to model a class-dependent score function $\\nabla_{x_t} \\log p(x_t|y)$. We identify a resemblance between the class-dependent score function and classification loss gradients with respect to the input image, leading us to propose that gradients derived from DDPM can serve as an enhanced source for perceptually aligned gradients. We would have (one term disappears with the gradient w.r.t the input image) using Bayes\u0026rsquo; formula.\u003c/p\u003e\n\u003cp\u003e\\begin{equation}\n\\nabla_{x_t} \\log p(x_t|y) = \\nabla_{x_t} \\log p(y|x_t) + \\nabla_{x_t} \\log p(x_t),\n\\end{equation}\u003c/p\u003e\n\u003cp\u003ewhich results in\u003c/p\u003e\n\u003cp\u003e\\begin{equation}\n\\nabla_{x_t} \\log p(y|x_t) = \\nabla_{x_t} \\log p(x_t|y) - \\nabla_{x_t} \\log p(x_t).\n\\end{equation}\u003c/p\u003e\n\u003cp\u003eThis formulation introduces a new application of diffusion models – a systematic approach to estimate the appropriate gradients for the expression $\\log p(y|x_t)$. However, classification networks operate on noise-free images ($x$) rather than noisy ones ($x_t$). To link classifier input-gradients with DDPMs, we assume that $\\log p(y|x) \\approx log p(y|x_t)$, for certain noise levels $t$. Consequently, the desired estimation of \u0026ldquo;ground-truth\u0026rdquo; classifier input-gradients can be acquired by subtracting an unconditional score function from a class-conditional one. The selection of $t$ when distilling gradients through this method presents a tradeoff – excessively large values yield gradients unrelated to the input image (too noisy), while excessively small values produce perceptually insignificant ones (in low noise levels, the conditional and unconditional scores are nearly identical). Therefore, we choose $t$ to be of moderate values, generating both perceptually and image-relevant gradients. We denote this method as Score-Based Gradients (SBG).\u003c/p\u003e\n\u003cp\u003eTo understand a bit more how it works, one has to consider that the variations of the noise from every $x_t$ can be controlled. Indeed, each different iteration takes the direction of the distribution $\\log p(x_t)$ (with stochasticity). In other terms, it takes the direction of our score function that can be estimated thanks to Neural Networks. That\u0026rsquo;s how you obtain your set of ground-truth gradients related to the input images.\u003c/p\u003e\n\u003cp\u003eAt this point, we have four ways to approximate ground-truth gradients. (Three heuristics and a more theoretical one). The experiments presented here will use the NN approach that are very intuitive. What was favoured for real datasets was the score-based approach.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003eExperiment\u003c/h2\u003e\n\u003cp\u003eNow, let\u0026rsquo;s experiment a bit. In this article, to understand what is happening, we will play a bit with the toy dataset. A 2 dimensional synthetic dataset is built. It contains 6000 samples of 2 classes. Every sample is on the line of equation $x_2 -2x_1=0$. Finally, each class contains \u003cstrong\u003ethree mods\u003c/strong\u003e (1000 samples per mode) drawn from a Gaussian distribution. The idea is to observe manifolds as decision boundaries. Background of the plan will be colored according to the predicted class. Evaluation will be made on a test set.\u003c/p\u003e\n\u003cp\u003eThe code is available at this \u003ca href=\"https://github.com/YohannZe/responsible-ai-datascience-ipParis.github.io.git\"\u003elink\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eTo this prediction task, a simple 2 layers MLP with ReLU is used. Two training are made with the same seed. The first is based on the usual cross-entropy loss whereas the second is made on the explained new loss.\u003c/p\u003e\n\u003cp\u003eAs expected, 100% accuracy is obtained for this very simple task for both models on the test set. However, what about predicting adversarial examples ?\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s first try it out with a targeted $L2$ PGD. Vanilla is only correct for 35 out of 600 samples, whereas this new approach obtains 583 out of 600.\nHow can this be explained ? One should observe the decision boundaries.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Yohann_Zerbib/vanilla_l2_toy.png\"\n  alt=\"vanillal2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThis is what is obtained for the regular neural network with cross-entropy Loss.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Yohann_Zerbib/nn_l2_toy.png\"\n  alt=\"nnl2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eHere is the result obtained for the particular neural network with the new loss.\u003c/p\u003e\n\u003cp\u003eWhat one should notice is the decision boundaries. The vanilla neural network provides manifolds that really \u003cem\u003e\u003cstrong\u003estick\u003c/strong\u003e\u003c/em\u003e to the data points. Going just a bit further can on the graph really can create a shift in the prediction. And that is what is happening with a targeted pgd, where there is only a small variation (semantically invisible).\u003c/p\u003e\n\u003cp\u003eHowever, in the case of the PAG Neural Network, one can observe that around a mode of points, there is a \u003cstrong\u003emuch greater margin\u003c/strong\u003e of the same class. This can be understood from the setup to create perceptually aligned gradients. Indeed, as we have seen, a target class was set based on a nearest neighbour approach, and the gradient point away from the current image and towards the class representative. Only then the cosine similarity between this gradient and the ground-truth approximated one from DDPMs.\u003c/p\u003e\n\u003cp\u003eAnother possibility would be to see the impact of the size of the perturbation on the performance. Indeed, here, the given results corresponded to an epsilon value of 15. Increasing it decreases the accuracy to 75%. However, at a certain point, an augmentation of epsilon will not change anything anymore, probably because of a normalizing step in the targeted PGD algorithm.\u003c/p\u003e\n\u003ch2 id=\"section-4\"\u003eTo go further\u003c/h2\u003e\n\u003cp\u003eWhat\u0026rsquo;s next ? Testing the hypothesis on real datasets. Among them, CIFAR-10, STL (higher resolution) and CIFAR-100 (higher number of classes). The architecture to achieve those tasks are classical (Resnet-18, ViT). Here are the main results that can be highlighted.\u003c/p\u003e\n\u003cp\u003ePAG approach is often similar and sometimes outperforms adversarially training approach. Score-based gradient seems to be the most accurate ground-truth approximation setup. It is also more notable for the ViT architecture. It also globally performs well on STL and CIFAR-100 (sometimes even better than adversarially training).\u003c/p\u003e\n\u003cp\u003eBut, the question is not yet answered : \u003cem\u003e\u003cstrong\u003eDo Perceptually Aligned Gradients imply Robustness?\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eAnd that\u0026rsquo;s where the regularization aspect of the loss is very useful. One can make variation over the hyperparameter $\\lambda$ to see what brings a bigger focus on the PAG loss. The authors have done it and are summarized with this table.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Yohann_Zerbib/regu.png\"\n  alt=\"regu\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAs one can see, the robustness increases with the increase of the regularization hyperparameter. The more the \u003cem\u003eghost\u003c/em\u003e features of the target class are visible (even if it not always comprehensible), the more the model is robust.\u003c/p\u003e\n\u003cp\u003eSo, it seems that yes, models with \u003cstrong\u003ePAG would be more robust\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"section-5\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eTo draw a conclusion, this paper has empirically shown that \u003cstrong\u003ePAG lead to more robustness\u003c/strong\u003e in models. It was also mentionned that it could potentially be combined with Adversarially Training to gain more robustness, and there are probably some experiments and tests that could optimize that. The performance are also good and can be seen as an alternative, potentially not too costly. Sometimes it \u003cstrong\u003eouperforms Adversarially Training\u003c/strong\u003e and it would be up to the user to decide which framework to employ for creating robust models. Finally, approximating ground-truth PAG needs additionnal research and discussion as even if the results tend to favour Score-Based Gradients, it happens that heuristics function better and there are potentially other approaches that have yet to be discovered. One should shed light on the fact that the diffusion models used need to be trained, and the training time gained over adversarially training is not as significant as with other heuristics if we consider this aspect.\u003c/p\u003e\n\u003ch2 id=\"section-6\"\u003eReferences\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eEYKHOLT, Kevin, EVTIMOV, Ivan, FERNANDES, Earlence, et al. Robust physical-world attacks on deep learning visual classification. In : Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. p. 1625-1634.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGanz, R., Kawar, B., \u0026amp; Elad, M. (2023, July). Do perceptually aligned gradients imply robustness?. In International Conference on Machine Learning (pp. 10628-10648). PMLR.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGoodfellow, I. J., Shlens, J., \u0026amp; Szegedy, C. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMadry, A., Makelov, A., Schmidt, L., Tsipras, D., \u0026amp; Vladu, A. (2017). Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/robustness-and-pag-the-converse/","date_published":"7026-07-09T26:77:00+01:00","date_modified":"7026-07-09T26:77:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"546ff9c63aab59b8a7ddb24d9f5939b60b89ee70","title":"To update or not to update? Neurons at equilibrium in deep models","summary":"","content_text":"To update or not to update? Neurons at equilibrium in deep models Author: Alexis WINTER Augustin CREUSILLET Table of content Introduction NEq Experiments Results Reproducibility Conclusion References Introduction Background Recent advances in deep learning have undeniably propelled the field to unprecedented heights, revolutionizing various domains from computer vision to natural language processing. However, these strides forward have not come without a significant toll on computational resources. As models grow increasingly complex, the demand for computational power has surged exponentially. One of the most expensive tasks in deep learning is undoubtedly the training of models. This process entails iteratively adjusting millions or even billions of parameters to minimize a predefined loss function, requiring extensive computational power and time-intensive operations. This process poses challenges in terms of both affordability and environmental sustainability, highlighting the need for innovative solutions to make deep learning more efficient and accessible in the face of escalating computational demands.\nThis paper tries to focus on the overall behavior of neurons, leveraging the notion of neuronal equilibrium (NEq). When a neuron reaches a state of equilibrium, indicating that it has learned a particular input-output mapping, it ceases its updates. The result is that we can reduce the number of operations needed for the computation of the backpropagation and optimizer and thus reduce the number of resources necessary for the model.\nRelated works Pruning strategies Pruning strategies consist in the systematic removal of redundant or less important parameters, connections or units within a model to improve efficiency and reduce computational complexity. These strategies are inspired by the biological concept of pruning, where unnecessary connections in neural networks are eliminated to enhance neural efficiency. Pruning can take various forms, including magnitude-based pruning, where parameters with small weights are pruned, or structured pruning, which removes entire neurons, channels, or layers based on specific criteria. Pruning strategies effectively reduce the model size leading to a more frugal and compact model With the development of computational resources and the creation of more complex model, pruning strategies such as dropout are being exploited again.\nDespite its effectiveness in reducing model size and improving inference efficiency, pruning strategies typically do not alleviate the computational complexity associated with training neural networks. While pruning removes parameters or connections during the inference phase, the training process still requires the full model to be trained initially, often resulting in high computational demands. In fact, pruning can even increase training complexity due to the need for additional iterations to fine-tune the remaining parameters and adapt the model to compensate for the pruned components. Consequently, while pruning offers significant benefits in terms of model deployment and inference efficiency, it does not directly address the computational burden of training models.\nLottery ticket hypothesis The lottery ticket hypothesis is a concept in deep learning that suggests that within a dense neural network, there exist sparse subnetworks, or \u0026ldquo;winning tickets,\u0026rdquo; that are capable of achieving high accuracy when trained in isolation. These winning tickets are characterized by having a small subset of well-initialized weights, which when pruned to remove the remaining connections, can maintain or even surpass the performance of the original dense network.\nThe hypothesis was introduced by Jonathan Frankle and Michael Carbin in 2018. They conducted experiments demonstrating that randomly-initialized, dense neural networks contain subnetworks that can achieve high performance when trained properly. These subnetworks or winning tickets tend to emerge during the training process and possess a specific initialization that allows them to be effectively trained within the broader network.\nThe significance of the lottery ticket hypothesis lies in its potential to improve the efficiency of training deep neural networks. By identifying these winning tickets and training only the sparse subnetworks, researchers can reduce computational costs associated with training while maintaining or even improving model accuracy. This concept has led to the development of pruning techniques aimed at discovering these winning tickets and accelerating the training process.\nNEq Neuronal equilibrium The concept of neuronal equilibrium aims to detect when a neuron reaches a state of equilibrium, indicating that it has learned a particular input-output mapping. The idea is to understand when the neuron has reach a configuration in which he does not require further updates.\nTo assess this we can evaluate cosine similarity between all the outputs of the $i$-th neuron at time $t$ and at time $t-1$ for the whole validation set $\\Xi_{val}$ as follows:\nThe neuron $i$-th reaches the equilibrium when $(\\phi_{i})_t$ stops evolving. In this sense to know when the neuron has reached the equilibrium we need to detect when :\n$$\\lim_{t\\rightarrow \\infty} \\phi_{i}^t = k,$$\nSince it is not trivial to assess this statment we prefer to work with variations of $(\\phi_{i})_t$ that can be defined as :\n\\begin{equation} v_{\\Delta \\phi_i}^t = \\Delta \\phi_i^t - \\mu_{eq} v_{\\Delta \\phi_i}^{t-1}, \\end{equation}\nWith $\\mu_{eq}$ the momentum coefficient.\nThis only lead to a reformulation of the problem as the equilibrium is reached when we have : $$\\Delta \\phi_i^t \\rightarrow 0$$\nSince we want to track the evolution of $\\Delta \\phi_i^t$ over time we introduce the velocity of the variations: $$ v_{\\Delta \\phi_i}^t = \\Delta \\phi_i^t - \\mu_{eq} v_{\\Delta \\phi_i}^{t-1}, $$\nWith $\\mu_{eq}$ the momentum coefficient.\nRewrited:\nWe need to have $$\\mu_{eq} \\in [0; 0.5]$$ to prevent the velocity from exploding.\nFinally we can set the condition for the neuron to be at the equilibrium as: \\begin{equation} \\left| v_{\\Delta \\phi}^t \\right | \u0026lt; \\varepsilon,~~~~~\\varepsilon \\geq 0. \\end{equation}\nIt is important to know that this relation might not hold for all $t$ since there could be an instant $t\u0026rsquo; \u0026lt; t$ where the relation does not hold anymore and the neuron is attracted to a new state and need to be updated again.\nTraining scheme The training scheme can be presented according to this scheme:\nAt the first epoch each neuron is considered to be at non-equilibrium. After the first epoch the training scheme can be described as followed:\nAn epoch of training is made for all trainable neurons on the training set. The training either stops due to the end of training criterion being met or continues to the next step. The velocity of the similarities is evaluated for every neuron. The set of trainable neurons is determined for the next step according to the equilibrium criterion. Comparing with regular training, we can see two more hyper-parameters:\n$\\epsilon$ which determines the threshold at which a neuron is considered to be at equilibrium according to the velocity of the similarities. $\\mu_{eq}$ which intervenes into the calculation of the velocity of the similarities. Experiments SGD vs Adam The authors conducted an experiment comparing two training methods for a ResNet-32 neural network on the CIFAR-10 dataset. The methods compared are SGD (Stochastic Gradient Descent) with momentum and Adam, which are both optimization algorithms used to update network weights iteratively.\nIn the experiment, the authors observe the FLOPs required for a back-propagation step and the number of updated neurons during training. They note that at high learning rates, more neurons are trained and more FLOPs are required. This is attributed to the network not being at equilibrium—essentially, the network parameters are still very fluid and subject to change, thus requiring more computation.\nAs training progresses and the learning rate is reduced, fewer neurons need updating, as the network moves towards its final, more stable configuration. The authors find that Adam brings the network towards this equilibrium faster than SGD, but also note that in this specific task, SGD achieves a slightly higher final accuracy than Adam. This may suggest that while Adam is efficient in reaching a state where few neuron weights are updated, SGD\u0026rsquo;s ability to explore the solution space more thoroughly leads to a better generalization on the test data.\nThe experiment also highlights an interesting behavior at the first learning rate decay around epoch 100 for SGD. The number of updated neurons decreases and then increases, which is not observed with Adam. This difference illustrates the contrasting approaches of the two optimizers: SGD, by reducing the learning rate, encourages continued exploration, which temporarily stabilizes the network until it adjusts to the new learning rate and begins exploring again. Adam, with its adaptive learning rate for each parameter, does not exhibit this behavior because it consistently steers the network towards a stable state.\nDistribution of $\\phi$ \u0026amp; choice of $µ_{eq}$ The paper also discusses the distribution of $\\phi$ and the choice of a parameter called $µ_{eq}$ during the training of neural networks.\nThe parameter $\\phi$ measures the cosine similarity between the outputs of a particular neuron at two consecutive training epochs, over the validation set. It is used to determine if a neuron\u0026rsquo;s output has reached equilibrium, meaning its outputs do not significantly change over successive epochs. If $\\phi$ equals 1, it indicates that the neuron\u0026rsquo;s output is stable across the epochs, signifying it has reached equilibrium.\nThe paper further discusses the dynamics of neurons as they approach equilibrium. To quantify this, they introduce a metric called ∆φ, which is the difference in the $\\phi$ values across epochs, and $v_{∆\\phi}$, which measures the velocity of this change considering a momentum coefficient $µ_{eq}$. This coefficient is important as it determines how much previous changes impact the current measurement of the equilibrium state.\nBy examining different values for $µ_{eq}$, the paper finds that setting $µ_{eq}$ to 0.5 provides a good compromise, as it ensures a balance between memory of past variations and responsiveness to new changes. This finding is illustrated in the paper\u0026rsquo;s Figure 5, which shows the distribution of $\\phi$, $∆\\phi$, and $v_{∆\\phi}$ for a ResNet-32 model trained on CIFAR-10.\nIn summary, the authors find that a neuron is at equilibrium if the velocity of the similarity changes, considering the momentum, is below a certain threshold. They also observe that during training, even after reaching equilibrium, neurons may occasionally \u0026ldquo;unfreeze\u0026rdquo; and require updates if the learning dynamics change, for instance, if the learning rate is adjusted.\nImpact of the validation set size and ε The authors found that the size of the validation set does not significantly impact the performance of the model. Interestingly, even with a validation set as small as a single image, the method yields good results. This is attributed to the presence of convolutional layers in the network, which, even with a small number of images, generate high-dimensional outputs in each neuron. Additionally, the homogeneity of the dataset (CIFAR-10) likely contributes to the robustness of the performance against changes in the validation set size.\nWhen examining the impact of the parameter ε, which is used to determine when a neuron is at equilibrium and hence does not need to be updated, the authors observe a drop in model performance at very high values of ε. They suggest a value of 0.001 as a good compromise for classification tasks, striking a balance between model performance and computational efficiency.\nResults Reproducibility Using the author\u0026rsquo;s implementation, we were able to replicate partially the results obtained using the ResNet32 model. Access to both the datasets and the code greatly facilitated the reproducibility process. However, our initial challenge stemmed from limited computational resources. Nonetheless, the method was transparently elucidated alongside its implementation, thus enabling a straightforward reproduction of the results without encountering any significant obstacles. The authors provided a detailed explanation of the method, including the training scheme, the parameters involved, and the expected outcomes. This clarity and transparency were crucial in ensuring the reproducibility of the results.\nExperiment This experiment aims to replicate the section 4.1.1 \u0026ldquo;SGD vs Adam\u0026rdquo; described in the study. Implementing this part is straightforward after cloning the GitHub repository. We simply need to execute the following command:\npython3 train_classification.py --amp=1 --arch=resnet32-cifar --batch-size=100 --dataset=cifar10 --device=cuda --epochs=250 --eps=0.001 --lr=0.1 --momentum=0.9 --optim=sgd --val-size=0.01 --velocity-mu=0.5 --weight-decay=0.0005 The code runs flawlessly, although we were significantly constrained by the lack of access to a powerful GPU, limiting our experiment. All the important parameters like the learning rate or the number of epochs are easily modifiable, making experimenting really easy. To obtain results for both SGD and Adam, we simply needed to change the optim parameter to the desired optimizer. The authors employ an application named Weights \u0026amp; Biases (wandb) to monitor the training process. This application is useful as it not only allows for the saving of training results but also provides a lot of valuable information.\nAs expected, as training progresses and the learning rate is reduced, more neuron are frozen and the pattern found on the plot follow the one found by the authors with Adam freezing neuron faster than SGD. We also get the same accuracy level where Adam brings the network towards this equilibrium faster than SGD, but with SGD achieving a slightly higher final accuracy.\nConclusion From the initial problem of computational resources saving, we have seen that NEq differs for others works that try to focus on finding optimal sub-graph for deep neural networks. By focusing on the entirety of the network and evaluating the behaviour of each neuron, NEq produces a new knowledge that is easily transposable to other experiments or any neural network model. The method results seem promising as it produces new insight on the learning behaviour of deep neural networks and might lead to new training strategies.\nOne possible development could be one of the limitations of the paper cited by the authors. The paper only focuses on individual neurons and evaluating the behaviour of ensembles of neurons could lead to other interesting results as some neurons might be at equilibrium only as a group at some step of the training process. This possibility could be explored further.\nReferences Bragagnolo, A., Tartaglione, E., Grangetto, M.: To update or not to update? neurons at equilibrium in deep models. Advances in neural information processing systems, 2022. Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural networks. In International Conference on Machine Learning, pages 2498–2507. PMLR, 2017. J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. 2019. ","content_html":"\u003ch1 style=\"font-size: 36px;\"\u003eTo update or not to update? Neurons at equilibrium in deep models\n\u003c/h1\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eAuthor: Alexis WINTER Augustin CREUSILLET\u003c/h1\u003e\n\u003ch1 id=\"table-of-content\"\u003eTable of content\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eNEq\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eExperiments\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eResults\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eReproducibility\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0\"\u003eIntroduction\u003c/h2\u003e\n\u003ch3 id=\"background\"\u003eBackground\u003c/h3\u003e\n\u003cp\u003eRecent advances in \u003cstrong\u003edeep learning\u003c/strong\u003e have undeniably propelled the field to unprecedented heights, revolutionizing various domains from computer vision to natural language processing. However, these strides forward have not come without a significant toll on computational resources. As models grow increasingly complex, the demand for \u003cstrong\u003ecomputational power\u003c/strong\u003e has surged exponentially. One of the most expensive tasks in deep learning is undoubtedly the training of models. This process entails iteratively adjusting millions or even billions of parameters to minimize a predefined loss function, requiring extensive computational power and time-intensive operations. This process poses challenges in terms of both \u003cstrong\u003eaffordability and environmental sustainability\u003c/strong\u003e, highlighting the need for innovative solutions to make deep learning more efficient and accessible in the face of escalating computational demands.\u003c/p\u003e\n\u003cp\u003eThis paper tries to focus on the overall behavior of neurons, leveraging the notion of \u003cstrong\u003eneuronal equilibrium (NEq)\u003c/strong\u003e. When a neuron reaches a state of equilibrium, indicating that it has learned a particular input-output mapping, it ceases its updates. The result is that we can reduce the number of operations needed for the computation of the backpropagation and optimizer and thus reduce the number of resources necessary for the model.\u003c/p\u003e\n\u003ch3 id=\"related-works\"\u003eRelated works\u003c/h3\u003e\n\u003ch4 id=\"pruning-strategies\"\u003ePruning strategies\u003c/h4\u003e\n\u003cp\u003ePruning strategies consist in the systematic removal of redundant or less important parameters, connections or units within a model to \u003cstrong\u003eimprove efficiency and reduce computational complexity\u003c/strong\u003e. These strategies are inspired by the biological concept of pruning, where unnecessary connections in neural networks are eliminated to enhance neural efficiency. Pruning can take various forms, including magnitude-based pruning, where parameters with small weights are pruned, or structured pruning, which removes entire neurons, channels, or layers based on specific criteria. Pruning strategies effectively reduce the model size leading to a more frugal and compact model With the development of computational resources and the creation of more complex model, pruning strategies such as dropout are being exploited again.\u003c/p\u003e\n\u003cp\u003eDespite its effectiveness in reducing model size and improving inference efficiency, pruning strategies typically \u003cstrong\u003edo not alleviate the computational complexity associated with training neural networks\u003c/strong\u003e. While pruning removes parameters or connections during the inference phase, the training process still requires the full model to be trained initially, often resulting in high computational demands. In fact, pruning can even increase training complexity due to the need for additional iterations to fine-tune the remaining parameters and adapt the model to compensate for the pruned components. Consequently, while pruning offers significant benefits in terms of model deployment and inference efficiency, it does not directly address the computational burden of training models.\u003c/p\u003e\n\u003ch4 id=\"lottery-ticket-hypothesis\"\u003eLottery ticket hypothesis\u003c/h4\u003e\n\u003cp\u003eThe \u003cstrong\u003elottery ticket hypothesis\u003c/strong\u003e is a concept in deep learning that suggests that within a dense neural network, there exist sparse subnetworks, or \u0026ldquo;winning tickets,\u0026rdquo; that are capable of achieving high accuracy when trained in isolation. These winning tickets are characterized by having a small subset of well-initialized weights, which when pruned to remove the remaining connections, can maintain or even \u003cstrong\u003esurpass the performance of the original dense network\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eThe hypothesis was introduced by Jonathan Frankle and Michael Carbin in 2018. They conducted experiments demonstrating that randomly-initialized, dense neural networks contain subnetworks that can achieve high performance when trained properly. These subnetworks or winning tickets tend to emerge during the training process and possess a specific initialization that allows them to be effectively trained within the broader network.\u003c/p\u003e\n\u003cp\u003eThe significance of the lottery ticket hypothesis lies in its potential to improve the efficiency of training deep neural networks. By identifying these \u003cstrong\u003ewinning tickets\u003c/strong\u003e and training only the sparse subnetworks, researchers can reduce computational costs associated with training while maintaining or even improving model accuracy. This concept has led to the development of pruning techniques aimed at discovering these winning tickets and accelerating the training process.\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eNEq\u003c/h2\u003e\n\u003ch3 id=\"neuronal-equilibrium\"\u003eNeuronal equilibrium\u003c/h3\u003e\n\u003cp\u003eThe concept of \u003cstrong\u003eneuronal equilibrium\u003c/strong\u003e aims to detect when a neuron reaches a state of equilibrium, indicating that it has learned a particular input-output mapping. The idea is to understand when the neuron has reach a configuration in which he does not require further updates.\u003c/p\u003e\n\u003cp\u003eTo assess this we can evaluate cosine similarity between all the outputs of the $i$-th neuron at time $t$ and at time $t-1$ for the whole validation set $\\Xi_{val}$ as follows:\u003c/p\u003e\n\u003cimg src=\"/images/images_Winter_Creusillet/neq_formula.png\" width=\"300\"/\u003e\n\u003cp\u003eThe neuron $i$-th reaches the equilibrium when $(\\phi_{i})_t$ stops evolving. In this sense to know when the neuron has reached the equilibrium  we need to detect when :\u003c/p\u003e\n\u003cp\u003e$$\\lim_{t\\rightarrow \\infty} \\phi_{i}^t = k,$$\u003c/p\u003e\n\u003cp\u003eSince it is not trivial to assess this statment we prefer to work with variations of $(\\phi_{i})_t$ that can be defined as :\u003c/p\u003e\n\u003cp\u003e\\begin{equation}\nv_{\\Delta \\phi_i}^t = \\Delta \\phi_i^t - \\mu_{eq} v_{\\Delta \\phi_i}^{t-1},\n\\end{equation}\u003c/p\u003e\n\u003cp\u003eWith $\\mu_{eq}$ the momentum coefficient.\u003c/p\u003e\n\u003cp\u003eThis only lead to a reformulation of the problem as the equilibrium is reached when we have : $$\\Delta \\phi_i^t \\rightarrow 0$$\u003c/p\u003e\n\u003cp\u003eSince we want to track the evolution of $\\Delta \\phi_i^t$ over time we introduce the \u003cstrong\u003evelocity of the variations\u003c/strong\u003e:\n$$\nv_{\\Delta \\phi_i}^t = \\Delta \\phi_i^t - \\mu_{eq} v_{\\Delta \\phi_i}^{t-1},\n$$\u003c/p\u003e\n\u003cp\u003eWith $\\mu_{eq}$ the momentum coefficient.\u003c/p\u003e\n\u003cp\u003eRewrited:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/images_Winter_Creusillet/momentum_coef.png\"\n  alt=\"creusilet/winter\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eWe need to have $$\\mu_{eq} \\in [0; 0.5]$$ to prevent the velocity from exploding.\u003c/p\u003e\n\u003cp\u003eFinally we can set the condition for the neuron to be at the equilibrium as:\n\\begin{equation}\n\\left| v_{\\Delta \\phi}^t \\right | \u0026lt; \\varepsilon,~~~~~\\varepsilon \\geq 0.\n\\end{equation}\u003c/p\u003e\n\u003cp\u003eIt is important to know that this relation might not hold for all $t$ since there could be an instant $t\u0026rsquo; \u0026lt; t$ where the relation does not hold anymore and the neuron is attracted to a new state and need to be updated again.\u003c/p\u003e\n\u003ch3 id=\"training-scheme\"\u003eTraining scheme\u003c/h3\u003e\n\u003cp\u003eThe training scheme can be presented according to this scheme:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/images_Winter_Creusillet/prunedbackprop-scheme_full-1.png\"\n  alt=\"creusilet/winter\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAt the first epoch each neuron is considered to be at non-equilibrium. After the first epoch the training scheme can be described as followed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAn epoch of training is made for all trainable neurons on the training set.\u003c/li\u003e\n\u003cli\u003eThe training either stops due to the end of training criterion being met or continues to the next step.\u003c/li\u003e\n\u003cli\u003eThe velocity of the similarities is evaluated for every neuron.\u003c/li\u003e\n\u003cli\u003eThe set of trainable neurons is determined for the next step according to the equilibrium criterion.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eComparing with regular training, we can see two more hyper-parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\epsilon$ which determines the threshold at which a neuron is considered to be at equilibrium according to the velocity of the similarities.\u003c/li\u003e\n\u003cli\u003e$\\mu_{eq}$ which intervenes into the calculation of the velocity of the similarities.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-2\"\u003eExperiments\u003c/h2\u003e\n\u003ch3 id=\"sgd-vs-adam\"\u003eSGD vs Adam\u003c/h3\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/images_Winter_Creusillet/sgd_vs_adam.png\"\n  alt=\"adam/sgd\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe authors conducted an experiment comparing two training methods for a ResNet-32 neural network on the CIFAR-10 dataset. The methods compared are SGD (Stochastic Gradient Descent) with momentum and Adam, which are both optimization algorithms used to update network weights iteratively.\u003c/p\u003e\n\u003cp\u003eIn the experiment, the authors observe the FLOPs required for a back-propagation step and the number of updated neurons during training. They note that at high learning rates, more neurons are trained and more FLOPs are required. This is attributed to the network not being at equilibrium—essentially, the network parameters are still very fluid and subject to change, thus requiring more computation.\u003c/p\u003e\n\u003cp\u003eAs training progresses and the learning rate is reduced, fewer neurons need updating, as the network moves towards its final, more stable configuration. The authors find that \u003cstrong\u003eAdam brings the network towards this equilibrium faster than SGD\u003c/strong\u003e, but also note that in this specific task, \u003cstrong\u003eSGD achieves a slightly higher final accuracy than Adam\u003c/strong\u003e. This may suggest that while Adam is efficient in reaching a state where few neuron weights are updated, SGD\u0026rsquo;s ability to explore the solution space more thoroughly leads to a better generalization on the test data.\u003c/p\u003e\n\u003cp\u003eThe experiment also highlights an interesting behavior at the first learning rate decay around epoch 100 for SGD. The number of updated neurons decreases and then increases, which is not observed with Adam. This difference illustrates the contrasting approaches of the two optimizers: SGD, by reducing the learning rate, encourages continued exploration, which temporarily stabilizes the network until it adjusts to the new learning rate and begins exploring again. Adam, with its adaptive learning rate for each parameter, does not exhibit this behavior because it consistently steers the network towards a stable state.\u003c/p\u003e\n\u003ch3 id=\"distribution-of-phi--choice-of-µ_eq\"\u003eDistribution of $\\phi$ \u0026amp; choice of $µ_{eq}$\u003c/h3\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/images_Winter_Creusillet/mu-line-1.png\"\n  alt=\"creusilet/winter\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe paper also discusses the distribution of $\\phi$ and the choice of a parameter called $µ_{eq}$ during the training of neural networks.\u003c/p\u003e\n\u003cp\u003eThe parameter $\\phi$ measures the \u003cstrong\u003ecosine similarity between the outputs of a particular neuron at two consecutive training epochs\u003c/strong\u003e, over the validation set. It is used to determine if a neuron\u0026rsquo;s output has reached equilibrium, meaning its outputs do not significantly change over successive epochs. If $\\phi$ equals 1, it indicates that the neuron\u0026rsquo;s output is stable across the epochs, signifying it has reached equilibrium.\u003c/p\u003e\n\u003cp\u003eThe paper further discusses the dynamics of neurons as they approach equilibrium. To quantify this, they introduce a metric called ∆φ, which is the difference in the $\\phi$ values across epochs, and $v_{∆\\phi}$, which measures the velocity of this change considering a \u003cstrong\u003emomentum coefficient $µ_{eq}$\u003c/strong\u003e. This coefficient is important as it determines how much previous changes impact the current measurement of the equilibrium state.\u003c/p\u003e\n\u003cp\u003eBy examining different values for $µ_{eq}$, the paper finds that setting $µ_{eq}$ to 0.5 provides a good compromise, as it ensures a balance between memory of past variations and responsiveness to new changes. This finding is illustrated in the paper\u0026rsquo;s Figure 5, which shows the distribution of $\\phi$, $∆\\phi$, and $v_{∆\\phi}$ for a ResNet-32 model trained on CIFAR-10.\u003c/p\u003e\n\u003cp\u003eIn summary, the authors find that a neuron is at equilibrium if the velocity of the similarity changes, considering the momentum, is below a certain threshold. They also observe that during training, even after reaching equilibrium, neurons may occasionally \u0026ldquo;unfreeze\u0026rdquo; and require updates if the learning dynamics change, for instance, if the learning rate is adjusted.\u003c/p\u003e\n\u003ch3 id=\"impact-of-the-validation-set-size-and-ε\"\u003eImpact of the validation set size and ε\u003c/h3\u003e\n\u003cp\u003eThe authors found that the size of the validation set \u003cstrong\u003edoes not significantly impact the performance of the model\u003c/strong\u003e. Interestingly, even with a validation set as small as a single image, the method yields good results. This is attributed to the presence of convolutional layers in the network, which, even with a small number of images, generate high-dimensional outputs in each neuron. Additionally, the homogeneity of the dataset (CIFAR-10) likely contributes to the robustness of the performance against changes in the validation set size.\u003c/p\u003e\n\u003cp\u003eWhen examining the impact of the parameter ε, which is used to determine when a neuron is at equilibrium and hence does not need to be updated, the authors observe a drop in model performance at very high values of ε. They suggest a value of 0.001 as a good compromise for classification tasks, \u003cstrong\u003estriking a balance between model performance and computational efficiency\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003eResults\u003c/h2\u003e\n\u003ch2 id=\"section-4\"\u003eReproducibility\u003c/h2\u003e\n\u003cp\u003eUsing the author\u0026rsquo;s implementation, we were able to replicate partially the results obtained using the ResNet32 model. Access to both the datasets and the code greatly facilitated the reproducibility process. However, our initial challenge stemmed from \u003cstrong\u003elimited computational resources\u003c/strong\u003e. Nonetheless, the method was transparently elucidated alongside its implementation, thus enabling a straightforward reproduction of the results without encountering any significant obstacles. The authors provided a detailed explanation of the method, including the training scheme, the parameters involved, and the expected outcomes. This clarity and transparency were crucial in ensuring the reproducibility of the results.\u003c/p\u003e\n\u003ch3 id=\"experiment\"\u003eExperiment\u003c/h3\u003e\n\u003cp\u003eThis experiment aims to replicate the section 4.1.1 \u0026ldquo;SGD vs Adam\u0026rdquo; described in the study. Implementing this part is straightforward after cloning the GitHub repository. We simply need to execute the following command:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epython3 train_classification.py --amp\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e --arch\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eresnet32-cifar --batch-size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e100\u003c/span\u003e --dataset\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003ecifar10 --device\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003ecuda --epochs\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e250\u003c/span\u003e --eps\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.001 --lr\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.1 --momentum\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.9 --optim\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003esgd --val-size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.01 --velocity-mu\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.5 --weight-decay\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.0005\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe code runs flawlessly, although we were significantly constrained by the lack of access to a powerful GPU, limiting our experiment. All the important parameters like the learning rate or the number of epochs are easily modifiable, making experimenting really easy. To obtain results for both SGD and Adam, we simply needed to change the optim parameter to the desired optimizer. The authors employ an application named \u003cstrong\u003eWeights \u0026amp; Biases (wandb)\u003c/strong\u003e to monitor the training process. This application is useful as it not only allows for the saving of training results but also provides a lot of valuable information.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/images_Winter_Creusillet/frozen_sgd_vs_adam1.png\"\n  alt=\"creusilet/winter\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\n\u003cimg\n  src=\"/images/images_Winter_Creusillet/accuracy_sgd_vs_adam1.png\"\n  alt=\"creusilet/winter\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAs expected, as training progresses and the learning rate is reduced, more neuron are frozen and the pattern found on the plot follow the one found by the authors with Adam freezing neuron faster than SGD. We also get the same accuracy level where Adam brings the network towards this equilibrium faster than SGD, but with SGD achieving a slightly higher final accuracy.\u003c/p\u003e\n\u003ch2 id=\"section-5\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eFrom the initial problem of \u003cstrong\u003ecomputational resources saving\u003c/strong\u003e, we have seen that NEq differs for others works that try to focus on finding optimal sub-graph for deep neural networks. By focusing on the entirety of the network and evaluating the behaviour of each neuron, \u003cstrong\u003eNEq produces a new knowledge\u003c/strong\u003e that is easily transposable to other experiments or any neural network model. The method results seem promising as it produces new insight on the learning behaviour of deep neural networks and \u003cstrong\u003emight lead to new training strategies\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eOne possible development could be one of the limitations of the paper cited by the authors. The paper only focuses on individual neurons and evaluating the behaviour of ensembles of neurons could lead to other interesting results as some neurons might be at equilibrium only as a group at some step of the training process. This possibility could be explored further.\u003c/p\u003e\n\u003ch2 id=\"section-6\"\u003eReferences\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eBragagnolo, A., Tartaglione, E., Grangetto, M.: To update or not to update? neurons at equilibrium in deep models. Advances in neural information processing systems, 2022.\u003c/li\u003e\n\u003cli\u003eDmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural networks. In International Conference on Machine Learning, pages 2498–2507. PMLR, 2017.\u003c/li\u003e\n\u003cli\u003eJ. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. 2019.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/neq/","date_published":"7026-07-09T255:77:00+01:00","date_modified":"7026-07-09T255:77:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"00988a4ecebb51af6cf28a7b318568929bf7a01d","title":"Statistical Minimax Rates Under Privacy","summary":"","content_text":"Estimating Privacy in Data Science: A Comprehensive Guide Author: Antoine Klein Github Link Table of Contents Incentives Introduction Definition Theory The case of multinomial estimation The case of density estimation Experiment Conclusion Quizz Why do we care about privacy ? Imagine, you\u0026rsquo;re quietly at home when the doorbell rings. You open the door and a government official appears: population census. Even though he shows you his official badge and you\u0026rsquo;d like to help him in the public interest, you find it hard to answer his questions as you go along. Indeed, the first questions about the date of your move are easy and public. On the other hand, when he asks about the number of children, marital status or your salary and what you do with it, you struggle. Not because you don\u0026rsquo;t know the answer, but because you\u0026rsquo;re faced with an ethical dilemma: transparency towards the state versus protection of personal data.\n$$\\text{In short, transparency goes against your privacy. }$$\nThis stress has major consequences: as you doubt what could happen to you with this data, but you still want to answer it, you underestimate your answers. On a wider scale, this leads to a suffrage bias and therefore a lack of knowledge of the real situation of your population. Warner [1], the first to tackle this problem from a statistical angle talks of an evasive bias and says:\n\u0026ldquo;for reasons of modesty, fear of being thought bigoted, or merely a reluctance to confide secrets to strangers, respondents to surveys might prefer to be able to answer certain questions non-truthfully, or at least without the interviewer knowing their true response\u0026rdquo;\nThis situation presented a trusted agent, in that he wasn\u0026rsquo;t trying to harm you directly. Now imagine that you agree to give him your personal data, but that on the way home, this agent of the state is mugged and someone steals his documents. Not only is this an attack on his person, it\u0026rsquo;s also an attack on yours: as the guarantor of your data, it\u0026rsquo;s now at the mercy of the attacker. The problem here is not to have protected yourself against a malicious agent.\nAdmittedly, these situations are rare, but with the densification of data, their analogies are omnipresent: cookies on the Internet, cyber-attacks, datacenter crashes\u0026hellip;One area for improvement is quite simply to better certify usage by means of cyber protection labels and leads to such a norm to achieve trust: In this blog, we propose to tackle this problem from a completely different angle: how to both enable the agent to take global measures and prevent it and any subsequent malicious agents from being able to re-identify my personal data. We\u0026rsquo;ll also use minimax bounds to answer the question: for a given privacy criterion, what\u0026rsquo;s the loss in terms of estimation? (fundamental trade-offs between privacy and convergence rate)\nScientific introduction Our blog will follow the same plan as the article that inspired it (John C. Duchi [2]),i.e. to show that response randomization achieves optimal convergence in the case of multinomial estimation, and then that this process can be generalized to any nonparametric distribution estimation. To this end, we will introduce the notion of local differential privacy as well as the minimax theory for obtaining optimal limits. All this will shed light on the trade-off between privacy and estimation rates. We will also explain algorithms to implement these optimal strategies. Finally, we will propose some experimental results.\nSome key definitions Let assume that you want to make private $X_1 , \u0026hellip; , X_n \\in X$ random variable and, as the statistician, you only observe $Z_1, . . . , Z_n ∈ Z$. The paper assumes that there exist a markov kernel that links the true ramdom variables and the observed ones as follow: $Q_i(Z_i | X_i = x)$.\nThe privacy mechanism is to be said non interactive if each $Z_i$ is obtained only conditionnaly on $X_i$ (and not on the others). This represents the fact that the privacy mechanism is memory less. If not, the mechnism is said to be interactive.\nIn the following, we will work only with non-interactive privacy mechanism but in the conlusion we will claim that newer studies showed that it is not enough for some larger problems.\n$Z_i$ is said to be α-local-differentially private for the original data $X_i$ if $$sup(\\frac{Q(Z | X_i = x)}{Q(Z | X_i = x\u0026rsquo;)} | x, x\u0026rsquo; ∈ X) ≤ exp(α)$$.\nAn intuitive way of understanding this definition is to see that the smaller α is (the more private it is), the more difficult it is to distinguish the distribution of Z conditional on two different X data.\nTheoretical results The case of multinomial estimation In this section, we return back to the problem of the private survey. For the statistician view, estimating a survey is estimating the parameter θ from the Bernouilli distribution $B(θ)$. This problem is a special case of multinomial estimation, where θ is now a multidimensional parameter that is amenable to simplex probability. $∆d := (θ ∈ ℝ+ |∑θ_j = 1)$.\nTheorem : Given α-local-differentially private $Z_i$, there exists some arbitrary constants $C_1$, $C_2$ such that for all $\\alpha\\in [0,1]$: $$C_1 min(1, \\frac{1}{\\sqrt{n\\alpha^2}}, \\frac{d}{n\\alpha^2}) ≤ E[|θ_{hat} - θ|^2] ≤ C_2 min(1, \\frac{d}{n\\alpha^2})$$ and $$C_1 min(1,\\frac{1}{\\sqrt{n\\alpha^2}}) ≤ E[||θ_{hat} - θ||_1] ≤ C_2 min(1,\\frac{d}{\\sqrt{n\\alpha^2}})$$.\nRecall from standard statistics: For non private independant $Z_i$ with finite variance, there exists some arbitrary constants $C_3$ such that: $$E[|θ_{hat} - θ|^2] ≤ \\frac{C_3}{n}$$\nIn others term, providing α-local-differentially privacy causes a reduction in the effective sample size of a factor $\\frac{\\alpha^2}{d}$ for best situations. It thus means that the asymptotically rate of convergences remains unchanged which is a really good news !\nPractical strategies The paper deals with one of the 2 standard methods to implement such a strategy that obtains the minimax rates:\nRandomized responses Laplace Noise (beyond paper) Randomized responses The intuition of this section is the following : to not allow the statistician to retrieve your personnal data in case of Bernouilli distribution, you toss a coin. If it is heads, you say to him your reel answer, if it is tails, you say the opposite. In his point of view, as he doesn\u0026rsquo;t know what was the result of the coin, he can\u0026rsquo;t distinguish if you tell the true or not but in a large scale, he knows that he will have half correct answer, half lies so that he can retrieve information.\nFor the multinomial estimation now, you will generalize this procedure to the multidimensionnal setting. For each coordinate, you will tell to the statistician the reel answer with a certain probability and lies otherwise. More precisely, its leads to :\n$$[Z]_j = x_j \\text{ with probability } \\frac{e^\\frac{\\alpha}{2}} {1 + e^\\frac{\\alpha}{2}}$$ $$[Z]_j = 1 - x_j \\text{ with probability } \\frac{1}{1 + e^\\frac{\\alpha}{2}}$$\nSuch a mechanism achieves α-local-differentially privacy because one can show that :\n$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} = e^\\frac{\\alpha}{2}(||z - x||_1 - ||z - x\u0026rsquo;||_1) \\in [e^{-\\alpha}, e^\\alpha]$$ which is the criteria given above.\nWith the notation as $1_d=[1, 1, 1, \u0026hellip;, 1]$ corresponds to a d-vector with each coordinate equals 1, we can also show that :\n$$E[Z | x] = \\frac{e^\\frac{\\alpha}{2} - 1}{e^\\frac{\\alpha}{2} + 1} * x + \\frac{1}{1 + e^\\frac{\\alpha}{2}}1_d$$\nThis leads to the natural moment-estimator :\n$$θ_{hat} = \\frac{1}{n} ∑_{i=1}^{n} \\frac{Z_i - 1_d}{1 + e^\\frac{\\alpha}{2}} * \\frac{e^\\frac{\\alpha}{2} + 1}{e^\\frac{\\alpha}{2} - 1}$$\nOne can also show that it verifies :\n$$E[ ||θ_{hat}- θ||_2] ≤ \\frac{d}{n} * \\frac{(e^\\frac{\\alpha}{2} + 1)^2}{(e^\\frac{\\alpha}{2} - 1)^2} \u0026lt; \\frac{C_3}{nα^2}$$ which is the announced result.\nLaplace Noise (beyond paper) Instead of saying the truth with some probability, one may think of adding noise to the answer so that the statistician can\u0026rsquo;t retrieve his real answer. This is exactly the mechanism we propose to dive in and which is not covered in the paper.\nDefinition: A noise is said to be a Laplace noise with parameters (μ, b) if it verifies:\n$$f(x|μ, b) = \\frac{1}{2b} * exp(\\frac{-|x - μ|}{b})$$\nA visualisation for differents parameters is given below. We can see that Laplace distribution is a shaper verson of the gaussian distribution : The trick is to use such a noise. Let assume $X_i \\in [-M,M]$ and construct the private mechanism as follow:\n$$Z_i = X_i + \\sigma W_i$$ where $W_i$ is drawn from a Laplace noise (0,1).\nOne can show that :\n$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} \\leq e^{\\frac{1}{\\sigma} * |x - x\u0026rsquo;|} \\leq e^{\\frac{2M}{\\sigma}}$$\nThus, with the choice of $\\sigma = \\frac{2M}{\\alpha}$, it verifies α-local-differentially privacy. The proposed estimator is the following :\n$$\\hat{Z} = \\bar{X} + \\frac{2M}{\\alpha} \\bar{W}$$\nOne can show that it is an unbiaised estimator that achieves the optimal rates:\n$$E[\\hat{Z}] = E[X]$$\n$$V[\\hat{Z}] = \\frac{V(X)}{n} + \\frac{4M^2}{n\\alpha^2} V[\\bar{W}] = \\frac{V(X)}{n} + \\frac{8M^2}{n\\alpha^2}$$ $$E[ |\\hat{Z}- X|^2] \\leq \\frac{C_3}{n\\alpha^2}.$$\nThis is exactly the optimal rates, quite outstanding !\nThe case of density estimation One accurate question that can raise is : what about others distribution ? Is privacy more costly in general cases ? What is the trade-off ?\nTo answer this question, let\u0026rsquo;s precise the problem.\nWe want to estimate in a non-parametric way a 1D-density function f belonging to one of theses classes :\n-Hölder Class (β, L): $\\text{For all }x, y \\in \\mathbb{R} \\text{ and } m \\leq \\beta, \\quad \\left| f^{(m)}(x) - f^{(m)}(y) \\right| \\leq L \\left| x - y \\right|^{\\beta - m}$\n-Sobolev Class: $F_{\\beta}[C] := \\left( f \\in L^2([0, 1]) , \\middle| , f = \\sum_{j=1}^{\\infty} \\theta_j \\phi_j \\text{ such that } \\sum_{j=1}^{\\infty} j^{2\\beta} \\phi_j^2 \\leq C^2 \\right)$\nIn a intuitition way, those two classes express that f is smooth enough to admits Lipschitz constant to its derivative so that it doesn\u0026rsquo;t \u0026ldquo;vary\u0026rdquo; locally too much.\nTheorem Without privacy One can show that without privacy, the minimax rate achievable for estimating a Hölder Class function is:\n$$\\text{MSE}(\\hat{f} - f) \\leq C_1 \\cdot n^{-\\frac{2\\beta}{1+2\\beta}}$$ with the estimator\n$$\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h} K\\left(\\frac{x - X_i}{h}\\right) \\text{with } h = C_2 \\cdot n^{-\\frac{1}{2\\beta+1}}$$\nIn the case of d-multidimensionnal density f, the optimal rate is :\n$$\\text{MSE}(\\hat{f} - f) \\leq C_4 \\cdot n^{-\\frac{2\\beta}{d+ 2\\beta}}$$ with the estimator\n$$\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h^d} K^d\\left(\\frac{x-X_i}{h}\\right) \\quad \\text{with} \\quad h = C_5 \\cdot n^{-\\frac{1}{2\\beta + d}}$$\nThis illustrates once again the curse of dimensionnality.\nWith privacy Let assume that f bellongs to one of the two classes with β as smoothness parameter.\nThen, the optimal α-local-differentially private optimal rate is :\n$$\\text{MSE}(\\hat{f} - f) \\leq C_1 \\cdot (n\\alpha^2)^{-\\frac{2\\beta}{2\\beta+2}}.$$\nOne may observe two pessimistic news:\n-The rate is affected by a factor of $\\alpha^2$ as for the multinomial estimation\n-More damageable: the rate is slower in term of n unlike the previous problem which make privacy in this case more costly.\nPractical strategies Eventhough this rate is pessimistic and proves that privacy comes at a cost, it remains to illustrates how can we achieves this best but not great rate. For this end, once again, two strategies are possible.\nRandomized responses Laplace Noise (beyond paper) Randomized responses This is the strategy illustrated in the paper and consists of sampling for each coordinate according the realisation of a Bernouilli variable with the correct probability as function of α. As it is not the most comprehensive and straightforward method, we prefer to dive in depth into the second one; uncovered in the paper.\nLaplace Noise (beyond paper) Let assume that $X_i \\in [0,M]$ almost surely. We note $G_j = [\\frac{j-1}{K},\\quad \\frac{j}{K}]$ the bin of length $\\frac{1}{K}$.\nWe consider the histogramm estimator: $$\\hat{f}(x) = \\frac{K}{n} \\sum_{j=1}^{K} \\sum_{i=1}^{n} 1_{X_i \\in G_j} \\cdot 1_{x \\in G_j}.$$\nWe now construct the private mechanism as follow:\n$$Z_i = \\left[1_{X_i \\in G_1} + \\frac{2}{\\alpha} W_1, \\ldots, 1_{X_i \\in G_K} + \\frac{2}{\\alpha} W_K\\right]$$\nIn an intuitive way, we add a Laplace noise realisation for each bin.\nThis guarantees α-local-differentially privacy as : $$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} \\leq \\exp\\left(\\frac{\\alpha}{2} \\sum_{j=1}^{K} |1_{x \\in G_j} - 1_{x\u0026rsquo; \\in G_j}| \\right) \\leq \\exp\\left(\\frac{\\alpha}{2} \\cdot 2\\right).$$\nThis leads to the α-local-differentially private estimator :\n$$f_{\\text{private_estimate}} = \\hat{f} + \\frac{2K}{n\\alpha} \\sum_{j=1}^{K} W_j$$\nThe biais is the same as the unprivate case as :\n$$E[f_{\\text{private_estimate}}] = E[\\hat{f}] + 0 .$$\nOne may prove that if f bellongs to the β-Hölder Class:\n$$Biais(f_{\\text{private_estimate}}, f) \\leq C_1 * K^{-\\beta}$$\nMeanwhile, $$V[f_{\\text{private_estimate}}] \\leq \\frac{C_2}{n} + \\frac{4K^2}{\\alpha^2} \\frac{V[W]}{n}$$, such that in total :\n$$\\text{MSE}(f_{\\text{private_estimate}} - f) \\leq C_1 K^{-2\\beta} + \\frac{C_2}{n} + \\frac{C_3 K^2}{n\\alpha^2}.$$ Minimizing over K (hyperparameters) leads to : $K = C_4 \\cdot (n\\alpha^2)^{-\\frac{1}{2\\beta+2}}$ and thus to:\n$$\\text{MSE}(f_{\\text{private_estimate}} - f) \\leq C_5 \\cdot (n\\alpha^2)^{-\\frac{2\\beta}{2\\beta + 2}}$$, which is the expected bound.\nExperiment: Illustration of the Minimax privacy rate Overview The aim of this section is to provide illustrations of the theoretical results set out above. Emphasis is placed on convergence results, with empirical confirmation of the latter.\nFor the sake of reproducibility and transparency, the source code can be found in the notebook at this: Github link.\nMethodology Data Preparation: Rather than working with real datasets, we decide to work with simulated data, as this allows us to maintain control over all aspects. More precisely, we give ourselves $n=1000$ samples of the normal distribution $N(100,1)$ on which we add a Laplace noise $L(0,\\alpha).$\nAs for the different alpha values, we iterate through them: $[0.2, 0.3, 0.5, 0.7]$\nPrivacy Metric Calculation: We will look at the use case of estimating the mean of a distribution.\nEvaluation: The results will be compared in terms of Mean Square Error (MSE).\nResults In terms of the observed distribution (private because subject to Laplace noise) relative to the true data, we obtain the following figure:\nAs expected, the greater the desired privacy (low $\\alpha$), the more spread out the distribution of observed data.\nWhen it comes to estimating the true average from private data, we obtain the following figure:\nThis figure illustrates two major points:\n-The first is that whatever the level of privacy, we have an unbiased estimator of the mean. It\u0026rsquo;s a beautiful property, empirically verified !\n-The second is that, unfortunately, the greater the privacy (low alpha), the greater the variance of this estimator.\nWe recall our main theorem demonstrated above Previous theorem :\nTheorem : Given α-local-differentially private $Z_i$, there exists some arbitrary constants $C_1$, $C_2$ such that for all $\\alpha\\in [0,1]$: $$C_1 min(1, \\frac{1}{\\sqrt{n\\alpha^2}}, \\frac{d}{n\\alpha^2}) ≤ E[|θ_{hat} - θ|^2] ≤ C_2 min(1, \\frac{d}{n\\alpha^2})$$\nWe now want to compare the theoretical optimal rate with empirical results. To do this, we distinguish two situations:\n-The first is with fixed alpha, and determines the MSE as a function of the number of samples n. This leads to these empirical results:\nThe dotted line represents the regime of the theoretical bound of the form $n \\rightarrow \\frac{C1}{n}$ . This is the shape of the empirical curves!\n-The second has a fixed n and determines the MSE as a function of alpha. This leads to these empirical results:\nThe dotted line represents the regime of the theoretical bound of the form $\\alpha \\rightarrow \\frac{C1}{\\alpha^2}$ . This is once again the shape of the empirical curves quite surprisingly!\nConclusion From a problem rooted in an ethical dilemma (privacy versus completeness and transparency), we have looked at the cost of guaranteeing one at the expense of the other, to better sketch out desirable situations.\nThis has enabled us to develop theoretical results in terms of minimax rates. There is indeed a trade-off between these criteria, which is even more costly in the case of non-parametric density estimation.\nFinally, we have compared these theoretical limits with empirical results, which confirm the conformity of the statements.\nThe aim of all this work is to disseminate this important yet under-exploited notion: privacy. To this end, we invite the reader to take the following quiz to ensure his or her understanding.\nQuizz To test yourself abour privacy:\nWhat is privacy?\nAvoid asking questions that can raise private information A mechanism that prevents other agent to retrieve personnal information in your answer An ethical-washing trend Which situation is α-local-differentially privacy?\nsup {Q(Z | Xi = x)/Q(Z | Xi = x')} | x, x' ∈ X} \u003e= exp(α) You tell the truth half the time, you lie otherwise. Z_i = X_i + (2M/α) W_i with W_i drawn from a Laplace Noise(0,1) What is the privacy cost in term of optimal rate ?\nMultinomial estimation: A factor α^2/d Density estimation: from n^(-2β/2β+2) (without privacy) to (nα^2)^(-2β/(2β+2)) We loose nothing, that's the surprising finding of the paper Submit Annexes References Warner SL. Randomized response: a survey technique for eliminating evasive answer bias. J Am Stat Assoc. 1965 Mar;60(309):63-6. PMID: 12261830. John C. Duchi, Michael I. Jordan, and Martin Wainwright. Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation. Advances in Neural Information Processing Systems (2013) Dwork, C., \u0026amp; Roth, A. (2014). The algorithmic foundations of differential privacy. Foundations and Trends® in Theoretical Computer Science, 9(3-4), 211-407. Narayanan, A., \u0026amp; Shmatikov, V. (2008). Robust de-anonymization of large sparse datasets. In Security and Privacy, 2008. SP 2008. IEEE Symposium on (pp. 111-125). IEEE. ","content_html":"\u003ch1 style=\"font-size: 36px;\"\u003eEstimating Privacy in Data Science: A Comprehensive Guide\u003c/h1\u003e\n\u003ch1 style=\"font-size: 24px;\"\u003eAuthor: Antoine Klein \u003ca href=\"https://github.com/AntoineTSP\"\u003eGithub Link\u003c/a\u003e\u003c/h1\u003e\n\u003ch1 id=\"table-of-contents\"\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-0\"\u003eIncentives\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-1\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-2\"\u003eDefinition\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-3\"\u003eTheory\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-4\"\u003eThe case of multinomial estimation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-5\"\u003eThe case of density estimation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-6\"\u003eExperiment\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-7\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-8\"\u003eQuizz\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"section-0\"\u003eWhy do we care about privacy ?\u003c/h2\u003e\n\u003cp\u003eImagine, you\u0026rsquo;re quietly at home when the doorbell rings. You open the door and a government official appears: population census. Even though he shows you his official badge and you\u0026rsquo;d like to help him in the public interest, you find it hard to answer his questions as you go along. Indeed, the first questions about the date of your move are easy and public. On the other hand, when he asks about the number of children, marital status or your salary and what you do with it, you \u003cem\u003estruggle\u003c/em\u003e. Not because you don\u0026rsquo;t know the answer, but because you\u0026rsquo;re faced with an \u003cstrong\u003eethical dilemma\u003c/strong\u003e: transparency towards the state versus protection of personal data.\u003cbr\u003e\n$$\\text{In short, transparency goes against your privacy. }$$\u003c/p\u003e\n\u003cp\u003eThis stress has major consequences: as you doubt what could happen to you with this data, but you still want to answer it, you \u003cstrong\u003eunderestimate\u003c/strong\u003e your answers. On a wider scale, this leads to a \u003cstrong\u003esuffrage bias\u003c/strong\u003e and therefore a lack of knowledge of the real situation of your population. Warner [1], the first to tackle this problem from a statistical angle talks of an evasive bias and says:\u003cbr\u003e\n\u003cstrong\u003e\u0026ldquo;for reasons of modesty, fear of being thought bigoted, or merely a reluctance to confide secrets to strangers, respondents to surveys might prefer to be able to answer certain questions non-truthfully, or at least without the interviewer knowing their true response\u0026rdquo;\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis situation presented a trusted agent, in that he wasn\u0026rsquo;t trying to harm you directly. Now imagine that you agree to give him your personal data, but that on the way home, this agent of the state is mugged and someone steals his documents. Not only is this an attack on his person, it\u0026rsquo;s also an attack on yours: as the guarantor of your data, it\u0026rsquo;s now at the mercy of the attacker. The problem here is \u003cstrong\u003enot to have protected yourself against a malicious agent\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eAdmittedly, these situations are rare, but with the densification of data, their analogies are omnipresent: cookies on the Internet, cyber-attacks, datacenter crashes\u0026hellip;One area for improvement is quite simply to better \u003cstrong\u003ecertify usage\u003c/strong\u003e by means of cyber protection labels and leads to such a norm to achieve trust:\n\u003cimg\n  src=\"/images/Antoine_Klein/Umbrella.png\"\n  alt=\"Data Privacy2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eIn this blog, we propose to tackle this problem from a completely different angle: \u003cstrong\u003ehow to both enable the agent to take global measures and prevent it and any subsequent malicious agents from being able to re-identify my personal data\u003c/strong\u003e. We\u0026rsquo;ll also use minimax bounds to answer the question: \u003cstrong\u003efor a given privacy criterion, what\u0026rsquo;s the loss in terms of estimation?\u003c/strong\u003e (fundamental trade-offs between privacy and convergence rate)\u003c/p\u003e\n\u003ch2 id=\"section-1\"\u003eScientific introduction\u003c/h2\u003e\n\u003cp\u003eOur blog will follow the same plan as the article that inspired it (John C. Duchi [2]),i.e. to show that \u003cstrong\u003eresponse randomization achieves optimal convergence\u003c/strong\u003e in the case of multinomial estimation, and then that this process can be generalized to any \u003cem\u003enonparametric distribution estimation\u003c/em\u003e. To this end, we will introduce the notion of \u003cstrong\u003elocal differential privacy\u003c/strong\u003e as well as the \u003cstrong\u003eminimax theory\u003c/strong\u003e for obtaining optimal limits. All this will shed light on the \u003cstrong\u003etrade-off between privacy and estimation rates\u003c/strong\u003e. We will also explain algorithms to implement these optimal strategies. Finally, we will propose some experimental results.\u003c/p\u003e\n\u003ch2 id=\"section-2\"\u003eSome key definitions\u003c/h2\u003e\n\u003cp\u003eLet assume that you want to make private $X_1 , \u0026hellip; , X_n \\in X$ random variable and, as the statistician, you only observe $Z_1, . . . , Z_n ∈ Z$. The paper assumes that there exist a \u003cstrong\u003emarkov kernel\u003c/strong\u003e that links the true ramdom variables and the observed ones as follow: $Q_i(Z_i | X_i = x)$.\u003c/p\u003e\n\u003cp\u003eThe privacy mechanism is to be said \u003cstrong\u003enon interactive\u003c/strong\u003e if each $Z_i$ is obtained only conditionnaly on $X_i$ (and not on the others). This represents the fact that the privacy mechanism is \u003cstrong\u003ememory less\u003c/strong\u003e. If not, the mechnism is said to be interactive.\u003c/p\u003e\n\u003cp\u003eIn the following, we will work only with non-interactive privacy mechanism but in the conlusion we will claim that newer studies showed that it is not enough for some larger problems.\u003c/p\u003e\n\u003cp\u003e$Z_i$ is said to be \u003cstrong\u003eα-local-differentially private\u003c/strong\u003e for the original data $X_i$ if $$sup(\\frac{Q(Z | X_i = x)}{Q(Z | X_i = x\u0026rsquo;)} | x, x\u0026rsquo; ∈ X) ≤ exp(α)$$.\u003c/p\u003e\n\u003cp\u003eAn intuitive way of understanding this definition is to see that the smaller α is (the more private it is), the more \u003cstrong\u003edifficult it is to distinguish\u003c/strong\u003e the distribution of Z conditional on two different X data.\u003c/p\u003e\n\u003ch2 id=\"section-3\"\u003eTheoretical results\u003c/h2\u003e\n\u003ch3 id=\"section-4\"\u003eThe case of multinomial estimation\u003c/h3\u003e\n\u003cp\u003eIn this section, we return back to the problem of the private survey. For the statistician view, estimating a survey is estimating the parameter θ from the Bernouilli distribution $B(θ)$.\nThis problem is a special case of multinomial estimation, where \u003ccode\u003eθ\u003c/code\u003e is now a multidimensional parameter that is amenable to simplex probability. $∆\u003cem\u003ed := (θ ∈ ℝ\u003c/em\u003e+ |∑θ_j = 1)$.\u003c/p\u003e\n\u003cp\u003e\u003ca name=\"Recall\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTheorem :\u003c/strong\u003e Given α-local-differentially private $Z_i$, there exists some arbitrary constants $C_1$, $C_2$ such that for all $\\alpha\\in [0,1]$:\n$$C_1 min(1, \\frac{1}{\\sqrt{n\\alpha^2}}, \\frac{d}{n\\alpha^2}) ≤ E[|θ_{hat} - θ|^2] ≤ C_2 min(1, \\frac{d}{n\\alpha^2})$$ and\n$$C_1 min(1,\\frac{1}{\\sqrt{n\\alpha^2}}) ≤ E[||θ_{hat} - θ||_1] ≤ C_2 min(1,\\frac{d}{\\sqrt{n\\alpha^2}})$$.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRecall from standard statistics:\u003c/strong\u003e For non private independant $Z_i$ with finite variance, there exists some arbitrary constants $C_3$ such that:\n$$E[|θ_{hat} - θ|^2] ≤ \\frac{C_3}{n}$$\u003c/p\u003e\n\u003cp\u003eIn others term, providing α-local-differentially privacy \u003cstrong\u003ecauses a reduction\u003c/strong\u003e in the effective sample size of a factor $\\frac{\\alpha^2}{d}$ for best situations. It thus means that the \u003cstrong\u003easymptotically rate of convergences remains unchanged\u003c/strong\u003e which is a really good news !\u003c/p\u003e\n\u003ch4 id=\"practical-strategies\"\u003ePractical strategies\u003c/h4\u003e\n\u003cp\u003eThe paper deals with one of the 2 standard methods to implement such a strategy that obtains the minimax rates:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-10\"\u003eRandomized responses\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-11\"\u003eLaplace Noise (beyond paper)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"section-10\"\u003eRandomized responses\u003c/h5\u003e\n\u003cp\u003eThe \u003cem\u003eintuition\u003c/em\u003e of this section is the following : \u003cstrong\u003eto not allow the statistician to retrieve your personnal data\u003c/strong\u003e in case of Bernouilli distribution, you toss a coin. If it is heads, you say to him your reel answer, if it is tails, you say the opposite. In his point of view, as he doesn\u0026rsquo;t know what was the result of the coin, \u003cstrong\u003ehe can\u0026rsquo;t distinguish\u003c/strong\u003e if you tell the true or not but in a large scale, he knows that he will have half correct answer, half lies so that he can retrieve information.\u003c/p\u003e\n\u003cp\u003eFor the multinomial estimation now, you will generalize this procedure to the multidimensionnal setting. For each coordinate, you will tell to the statistician the reel answer with a certain probability and lies otherwise. More precisely, its leads to :\u003c/p\u003e\n\u003cp\u003e$$[Z]_j = x_j \\text{ with probability } \\frac{e^\\frac{\\alpha}{2}} {1 + e^\\frac{\\alpha}{2}}$$\n$$[Z]_j = 1 - x_j \\text{ with probability } \\frac{1}{1 + e^\\frac{\\alpha}{2}}$$\u003c/p\u003e\n\u003cp\u003eSuch a mechanism achieves \u003cem\u003eα-local-differentially privacy\u003c/em\u003e because one can show that :\u003c/p\u003e\n\u003cp\u003e$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} = e^\\frac{\\alpha}{2}(||z - x||_1 - ||z - x\u0026rsquo;||_1) \\in [e^{-\\alpha}, e^\\alpha]$$ which is the criteria given above.\u003c/p\u003e\n\u003cp\u003eWith the notation as $1_d=[1, 1, 1, \u0026hellip;, 1]$ corresponds to a d-vector with each coordinate equals 1, we can also show that :\u003c/p\u003e\n\u003cp\u003e$$E[Z | x] = \\frac{e^\\frac{\\alpha}{2} - 1}{e^\\frac{\\alpha}{2} + 1} * x + \\frac{1}{1 + e^\\frac{\\alpha}{2}}1_d$$\u003c/p\u003e\n\u003cp\u003eThis leads to the natural moment-estimator :\u003c/p\u003e\n\u003cp\u003e$$θ_{hat} = \\frac{1}{n} ∑_{i=1}^{n} \\frac{Z_i - 1_d}{1 + e^\\frac{\\alpha}{2}} * \\frac{e^\\frac{\\alpha}{2} + 1}{e^\\frac{\\alpha}{2} - 1}$$\u003c/p\u003e\n\u003cp\u003eOne can also show that it verifies :\u003c/p\u003e\n\u003cp\u003e$$E[ ||θ_{hat}- θ||_2] ≤  \\frac{d}{n} * \\frac{(e^\\frac{\\alpha}{2} + 1)^2}{(e^\\frac{\\alpha}{2} - 1)^2} \u0026lt; \\frac{C_3}{nα^2}$$ which is the announced result.\u003c/p\u003e\n\u003ch5 id=\"section-11\"\u003eLaplace Noise (beyond paper)\u003c/h5\u003e\n\u003cp\u003eInstead of saying the truth with some probability, one may think of \u003cstrong\u003eadding noise\u003c/strong\u003e to the answer so that the statistician can\u0026rsquo;t retrieve his real answer. This is exactly the mechanism we propose to dive in and which is \u003cstrong\u003enot covered in the paper\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDefinition:\u003c/strong\u003e A noise is said to be a Laplace noise with parameters (μ, b) if it verifies:\u003cbr\u003e\n$$f(x|μ, b) = \\frac{1}{2b} * exp(\\frac{-|x - μ|}{b})$$\u003c/p\u003e\n\u003cp\u003eA visualisation for differents parameters is given below. We can see that Laplace distribution is a \u003cstrong\u003eshaper verson of the gaussian distribution\u003c/strong\u003e :\n\u003cimg\n  src=\"/images/Antoine_Klein/Laplace.png\"\n  alt=\"Laplace\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe trick is to use such a noise. Let assume $X_i \\in [-M,M]$ and construct the private mechanism as follow:\u003cbr\u003e\n$$Z_i = X_i + \\sigma W_i$$ where $W_i$ is drawn from a Laplace noise (0,1).\u003c/p\u003e\n\u003cp\u003eOne can show that :\u003c/p\u003e\n\u003cp\u003e$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} \\leq e^{\\frac{1}{\\sigma} * |x - x\u0026rsquo;|} \\leq e^{\\frac{2M}{\\sigma}}$$\u003c/p\u003e\n\u003cp\u003eThus, with the choice of $\\sigma = \\frac{2M}{\\alpha}$, \u003cstrong\u003eit verifies α-local-differentially privacy\u003c/strong\u003e. The proposed estimator is the following :\u003cbr\u003e\n$$\\hat{Z} = \\bar{X} + \\frac{2M}{\\alpha} \\bar{W}$$\u003c/p\u003e\n\u003cp\u003eOne can show that it is an unbiaised estimator that achieves the optimal rates:\u003cbr\u003e\n$$E[\\hat{Z}] = E[X]$$\u003cbr\u003e\n$$V[\\hat{Z}] = \\frac{V(X)}{n} + \\frac{4M^2}{n\\alpha^2} V[\\bar{W}] = \\frac{V(X)}{n} + \\frac{8M^2}{n\\alpha^2}$$\n$$E[ |\\hat{Z}- X|^2] \\leq \\frac{C_3}{n\\alpha^2}.$$\u003c/p\u003e\n\u003cp\u003eThis is \u003cstrong\u003eexactly the optimal rates\u003c/strong\u003e, quite outstanding !\u003c/p\u003e\n\u003ch3 id=\"section-5\"\u003eThe case of density estimation\u003c/h3\u003e\n\u003cp\u003eOne accurate question that can raise is : \u003cstrong\u003ewhat about others distribution ?\u003c/strong\u003e Is privacy more costly in general cases ? What is the trade-off ?\u003c/p\u003e\n\u003cp\u003eTo answer this question, let\u0026rsquo;s precise the problem.\u003c/p\u003e\n\u003cp\u003eWe want to estimate in a non-parametric way a 1D-density function \u003ccode\u003ef\u003c/code\u003e belonging to one of theses classes :\u003cbr\u003e\n-\u003cstrong\u003eHölder Class (β, L):\u003c/strong\u003e $\\text{For all }x, y \\in \\mathbb{R} \\text{ and } m \\leq \\beta, \\quad \\left| f^{(m)}(x) - f^{(m)}(y) \\right| \\leq L \\left| x - y \\right|^{\\beta - m}$\u003cbr\u003e\n-\u003cstrong\u003eSobolev Class:\u003c/strong\u003e $F_{\\beta}[C] := \\left( f \\in L^2([0, 1]) , \\middle| , f = \\sum_{j=1}^{\\infty} \\theta_j \\phi_j \\text{ such that } \\sum_{j=1}^{\\infty} j^{2\\beta} \\phi_j^2 \\leq C^2 \\right)$\u003c/p\u003e\n\u003cp\u003eIn a intuitition way, those two classes express that \u003ccode\u003ef\u003c/code\u003e is \u003cstrong\u003esmooth enough\u003c/strong\u003e to admits Lipschitz constant to its derivative so that it doesn\u0026rsquo;t \u0026ldquo;vary\u0026rdquo; locally too much.\u003c/p\u003e\n\u003ch4 id=\"theorem\"\u003eTheorem\u003c/h4\u003e\n\u003ch5 id=\"without-privacy\"\u003eWithout privacy\u003c/h5\u003e\n\u003cp\u003eOne can show that without privacy, the minimax rate achievable for estimating a Hölder Class function is:\u003cbr\u003e\n$$\\text{MSE}(\\hat{f} - f) \\leq C_1 \\cdot n^{-\\frac{2\\beta}{1+2\\beta}}$$ with the estimator\u003cbr\u003e\n$$\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h} K\\left(\\frac{x - X_i}{h}\\right) \\text{with } h = C_2 \\cdot n^{-\\frac{1}{2\\beta+1}}$$\u003c/p\u003e\n\u003cp\u003eIn the case of d-multidimensionnal density \u003ccode\u003ef\u003c/code\u003e, the optimal rate is :\u003cbr\u003e\n$$\\text{MSE}(\\hat{f} - f) \\leq C_4 \\cdot n^{-\\frac{2\\beta}{d+ 2\\beta}}$$ with the estimator\u003cbr\u003e\n$$\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h^d} K^d\\left(\\frac{x-X_i}{h}\\right) \\quad \\text{with} \\quad h = C_5 \\cdot n^{-\\frac{1}{2\\beta + d}}$$\u003c/p\u003e\n\u003cp\u003eThis illustrates once again the \u003cstrong\u003ecurse of dimensionnality\u003c/strong\u003e.\u003c/p\u003e\n\u003ch5 id=\"with-privacy\"\u003eWith privacy\u003c/h5\u003e\n\u003cp\u003eLet assume that \u003ccode\u003ef\u003c/code\u003e bellongs to one of the two classes with  \u003ccode\u003eβ\u003c/code\u003e as smoothness parameter.\u003cbr\u003e\nThen, the optimal α-local-differentially private optimal rate is :\u003cbr\u003e\n$$\\text{MSE}(\\hat{f} - f) \\leq C_1 \\cdot (n\\alpha^2)^{-\\frac{2\\beta}{2\\beta+2}}.$$\u003c/p\u003e\n\u003cp\u003eOne may observe \u003cstrong\u003etwo pessimistic news\u003c/strong\u003e:\u003cbr\u003e\n-The rate is \u003cstrong\u003eaffected by a factor\u003c/strong\u003e of $\\alpha^2$ as for the multinomial estimation\u003cbr\u003e\n-More damageable: the \u003cstrong\u003erate is slower\u003c/strong\u003e in term of \u003ccode\u003en\u003c/code\u003e unlike the previous problem which make privacy in this case \u003cstrong\u003emore costly\u003c/strong\u003e.\u003c/p\u003e\n\u003ch5 id=\"practical-strategies-1\"\u003ePractical strategies\u003c/h5\u003e\n\u003cp\u003eEventhough this rate is pessimistic and proves that \u003cstrong\u003eprivacy comes at a cost\u003c/strong\u003e, it remains to illustrates how can we achieves this best but not great rate.\nFor this end, once again, two strategies are possible.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#section-12\"\u003eRandomized responses\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#section-13\"\u003eLaplace Noise (beyond paper)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"section-12\"\u003eRandomized responses\u003c/h5\u003e\n\u003cp\u003eThis is the strategy illustrated in the paper and consists of sampling for each coordinate according the realisation of a Bernouilli variable with the correct probability as function of \u003ccode\u003eα\u003c/code\u003e.\nAs it is not the most comprehensive and straightforward method, \u003cstrong\u003ewe prefer to dive in depth into the second one; uncovered in the paper\u003c/strong\u003e.\u003c/p\u003e\n\u003ch5 id=\"section-13\"\u003eLaplace Noise (beyond paper)\u003c/h5\u003e\n\u003cp\u003eLet assume that $X_i \\in [0,M]$ almost surely. We note $G_j = [\\frac{j-1}{K},\\quad \\frac{j}{K}]$ the bin of length $\\frac{1}{K}$.\u003c/p\u003e\n\u003cp\u003eWe consider the histogramm estimator:\n$$\\hat{f}(x) = \\frac{K}{n} \\sum_{j=1}^{K} \\sum_{i=1}^{n} 1_{X_i \\in G_j} \\cdot 1_{x \\in G_j}.$$\u003c/p\u003e\n\u003cp\u003eWe now construct the private mechanism as follow:\u003cbr\u003e\n$$Z_i = \\left[1_{X_i \\in G_1} + \\frac{2}{\\alpha} W_1, \\ldots, 1_{X_i \\in G_K} + \\frac{2}{\\alpha} W_K\\right]$$\u003c/p\u003e\n\u003cp\u003eIn an intuitive way, we add a Laplace noise realisation for each bin.\u003c/p\u003e\n\u003cp\u003eThis guarantees α-local-differentially privacy as :\n$$\\frac{Q(Z = z | x)}{Q(Z = z | x\u0026rsquo;)} \\leq \\exp\\left(\\frac{\\alpha}{2} \\sum_{j=1}^{K} |1_{x \\in G_j} - 1_{x\u0026rsquo; \\in G_j}| \\right) \\leq \\exp\\left(\\frac{\\alpha}{2} \\cdot 2\\right).$$\u003c/p\u003e\n\u003cp\u003eThis leads to the α-local-differentially private estimator :\u003cbr\u003e\n$$f_{\\text{private_estimate}} = \\hat{f} + \\frac{2K}{n\\alpha} \\sum_{j=1}^{K} W_j$$\u003c/p\u003e\n\u003cp\u003eThe biais is the same as the unprivate case as :\u003cbr\u003e\n$$E[f_{\\text{private_estimate}}] = E[\\hat{f}] + 0 .$$\u003c/p\u003e\n\u003cp\u003eOne may prove that if f bellongs to the β-Hölder Class:\u003cbr\u003e\n$$Biais(f_{\\text{private_estimate}}, f) \\leq C_1 * K^{-\\beta}$$\u003c/p\u003e\n\u003cp\u003eMeanwhile, $$V[f_{\\text{private_estimate}}] \\leq \\frac{C_2}{n} + \\frac{4K^2}{\\alpha^2} \\frac{V[W]}{n}$$, such that in total  :\u003cbr\u003e\n$$\\text{MSE}(f_{\\text{private_estimate}} - f) \\leq C_1 K^{-2\\beta} + \\frac{C_2}{n} + \\frac{C_3 K^2}{n\\alpha^2}.$$\nMinimizing over K (hyperparameters) leads to :  $K = C_4 \\cdot (n\\alpha^2)^{-\\frac{1}{2\\beta+2}}$ and thus to:\u003cbr\u003e\n$$\\text{MSE}(f_{\\text{private_estimate}} - f) \\leq C_5 \\cdot (n\\alpha^2)^{-\\frac{2\\beta}{2\\beta + 2}}$$, which is the expected bound.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"section-6\"\u003eExperiment: Illustration of the Minimax privacy rate\u003c/h2\u003e\n\u003ch3 id=\"section-111\"\u003eOverview\u003c/h3\u003e\n\u003cp\u003eThe aim of this section is to \u003cstrong\u003eprovide illustrations of the theoretical results\u003c/strong\u003e set out above. Emphasis is placed on convergence results, with empirical confirmation of the latter.\u003c/p\u003e\n\u003cp\u003eFor the sake of \u003cstrong\u003ereproducibility and transparency\u003c/strong\u003e, the source code can be found in the notebook at this: \u003ca href=\"https://github.com/AntoineTSP/responsible-ai-datascience-ipParis.github.io.git\"\u003eGithub link\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"methodology\"\u003eMethodology\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eData Preparation\u003c/strong\u003e: Rather than working with real datasets, we decide to work with simulated data, as this allows us to maintain control over all aspects.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eMore precisely, we give ourselves $n=1000$ samples of the normal distribution $N(100,1)$ on which we add a Laplace noise $L(0,\\alpha).$\u003cbr\u003e\nAs for the different alpha values, we iterate through them: $[0.2, 0.3, 0.5, 0.7]$\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePrivacy Metric Calculation\u003c/strong\u003e: We will look at the use case of estimating the mean of a distribution.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEvaluation\u003c/strong\u003e: The results will be compared in terms of Mean Square Error (MSE).\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"results\"\u003eResults\u003c/h3\u003e\n\u003cp\u003eIn terms of the observed distribution (private because subject to Laplace noise) relative to the true data, we obtain the following figure:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Antoine_Klein/Private_distribution.png\"\n  alt=\"Data Privacy2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAs expected, the greater the desired privacy (low $\\alpha$), \u003cstrong\u003ethe more spread out\u003c/strong\u003e the distribution of observed data.\u003c/p\u003e\n\u003cp\u003eWhen it comes to estimating the true average from private data, we obtain the following figure:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Antoine_Klein/Estimated_mean.png\"\n  alt=\"Data Privacy2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThis figure illustrates two major points:\u003cbr\u003e\n-The first is that whatever the level of privacy, we have an \u003cstrong\u003eunbiased estimator\u003c/strong\u003e of the mean. It\u0026rsquo;s a beautiful property, empirically verified !\u003cbr\u003e\n-The second is that, unfortunately, the greater the privacy (low alpha), \u003cstrong\u003ethe greater the variance\u003c/strong\u003e of this estimator.\u003c/p\u003e\n\u003cp\u003eWe recall our main theorem demonstrated above \u003ca href=\"#Recall\" style=\"background-color: yellow; padding: 2px 5px; border-radius: 3px;\"\u003ePrevious theorem\u003c/a\u003e :\u003cbr\u003e\n\u003cstrong\u003eTheorem\u003c/strong\u003e : Given α-local-differentially private $Z_i$, there exists some arbitrary constants $C_1$, $C_2$ such that for all $\\alpha\\in [0,1]$:\n$$C_1 min(1, \\frac{1}{\\sqrt{n\\alpha^2}}, \\frac{d}{n\\alpha^2}) ≤ E[|θ_{hat} - θ|^2] ≤ C_2 min(1, \\frac{d}{n\\alpha^2})$$\u003c/p\u003e\n\u003cp\u003eWe now want to \u003cstrong\u003ecompare the theoretical optimal rate with empirical results\u003c/strong\u003e. To do this, we distinguish two situations:\u003cbr\u003e\n-The first is with \u003cstrong\u003efixed alpha\u003c/strong\u003e, and determines the MSE as a function of the number of samples n. This leads to these empirical results:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Antoine_Klein/Minimax_rate_n.png\"\n  alt=\"Data Privacy2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe dotted line represents the regime of the theoretical bound of the form $n \\rightarrow \\frac{C1}{n}$ . This is the shape of the empirical curves!\u003c/p\u003e\n\u003cp\u003e-The second has a \u003cstrong\u003efixed n\u003c/strong\u003e and determines the MSE as a function of alpha. This leads to these empirical results:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"/images/Antoine_Klein/Minimax_rate_alpha.png\"\n  alt=\"Data Privacy2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eThe dotted line represents the regime of the theoretical bound of the form $\\alpha \\rightarrow \\frac{C1}{\\alpha^2}$ . This is once again the shape of the empirical curves quite surprisingly!\u003c/p\u003e\n\u003ch3 id=\"section-7\"\u003eConclusion\u003c/h3\u003e\n\u003cp\u003eFrom a problem rooted in an \u003cstrong\u003eethical dilemma\u003c/strong\u003e (privacy versus completeness and transparency), we have looked at the \u003cstrong\u003ecost of guaranteeing\u003c/strong\u003e one at the expense of the other, to better sketch out desirable situations.\u003cbr\u003e\nThis has enabled us to develop theoretical results in terms of \u003cstrong\u003eminimax rates\u003c/strong\u003e. There is indeed a \u003cstrong\u003etrade-off\u003c/strong\u003e between these criteria, which is even more costly in the case of non-parametric density estimation.\u003cbr\u003e\nFinally, we have compared these theoretical limits with empirical results, which \u003cstrong\u003econfirm the conformity of the statements\u003c/strong\u003e.\u003cbr\u003e\nThe aim of all this work is to disseminate this important yet under-exploited notion: privacy. To this end, we invite the reader to take the following \u003cstrong\u003equiz\u003c/strong\u003e to ensure his or her understanding.\u003c/p\u003e\n\u003ch1 id=\"section-8\"\u003eQuizz\u003c/h1\u003e\n\u003cp\u003eTo test yourself abour privacy:\u003c/p\u003e\n\u003cform id=\"quiz-form\" class=\"quiz-form\"\u003e\n    \u003cdiv class=\"quiz-question\"\u003e\n        \u003cp\u003eWhat is privacy?\u003c/p\u003e\n        \u003cdiv class=\"quiz-options\"\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question1\" value=\"1\"\u003e\n                Avoid asking questions that can raise private information\n            \u003c/label\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question1\" value=\"2\"\u003e\n                A mechanism that prevents other agent to retrieve personnal information in your answer\n            \u003c/label\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question1\" value=\"3\"\u003e\n                An ethical-washing trend\n            \u003c/label\u003e\n        \u003c/div\u003e\n        \u003cp\u003eWhich situation is α-local-differentially privacy?\u003c/p\u003e\n        \u003cdiv class=\"quiz-options\"\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question2\" value=\"1\"\u003e\n                sup {Q(Z | Xi = x)/Q(Z | Xi = x')} | x, x' ∈ X} \u003e= exp(α)\n            \u003c/label\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question2\" value=\"2\"\u003e\n                You tell the truth half the time, you lie otherwise.\n            \u003c/label\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question2\" value=\"3\"\u003e\n                Z_i = X_i + (2M/α) W_i with W_i drawn from a Laplace Noise(0,1)\n            \u003c/label\u003e\n        \u003c/div\u003e\n        \u003cp\u003eWhat is the privacy cost in term of optimal rate ?\u003c/p\u003e\n        \u003cdiv class=\"quiz-options\"\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question3\" value=\"1\"\u003e\n                Multinomial estimation: A factor α^2/d\n            \u003c/label\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question3\" value=\"2\"\u003e\n                Density estimation: from n^(-2β/2β+2) (without privacy) to (nα^2)^(-2β/(2β+2))\n            \u003c/label\u003e\n            \u003clabel\u003e\n                \u003cinput type=\"radio\" name=\"question3\" value=\"3\"\u003e\n                We loose nothing, that's the surprising finding of the paper\n            \u003c/label\u003e\n        \u003c/div\u003e\n    \u003c/div\u003e\n    \u003c!-- Add more quiz questions as needed --\u003e\n    \u003cbutton type=\"submit\" class=\"quiz-submit\"\u003eSubmit\u003c/button\u003e\n\u003c/form\u003e\n\u003cdiv id=\"quiz-results\" class=\"quiz-results\"\u003e\u003c/div\u003e\n\u003cscript\u003e\n    // Define quiz questions and correct answers\n    const quizQuestions = [\n        {\n            question: \"What is privacy?\",\n            answer: \"2\"\n        },\n        //Add more quiz questions as needed\n        {\n            question: \"Which situation is α-local-differentially privacy?\",\n            answer: \"3\"\n        },\n        //Add more quiz questions as needed\n        {\n            question: \"What is the privacy cost in term of optimal rate ?\",\n            answer: \"1\"\n        }\n    ];\n\n    // Handle form submission\n    document.getElementById('quiz-form').addEventListener('submit', function(event) {\n        event.preventDefault();\n\n        // Calculate quiz score\n        let score = 0;\n        quizQuestions.forEach(question =\u003e {\n            const selectedAnswer = document.querySelector(`input[name=\"question${quizQuestions.indexOf(question) + 1}\"]:checked`);\n            if (selectedAnswer) {\n                if (selectedAnswer.value.toLowerCase() === question.answer) {\n                    score++;\n                    selectedAnswer.parentElement.classList.add('correct');\n                } else {\n                    selectedAnswer.parentElement.classList.add('incorrect');\n                }\n            }\n        });\n\n        // Display quiz results\n        const quizResults = document.getElementById('quiz-results');\n        quizResults.innerHTML = `\u003cp\u003eYou scored ${score} out of ${quizQuestions.length}.\u003c/p\u003e`;\n    });\n\u003c/script\u003e\n\u003chr\u003e\n\u003chr\u003e\n\u003ch2 id=\"annexes\"\u003eAnnexes\u003c/h2\u003e\n\u003ch3 id=\"references\"\u003eReferences\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eWarner SL. Randomized response: a survey technique for eliminating evasive answer bias. J Am Stat Assoc. 1965 Mar;60(309):63-6. PMID: 12261830.\u003c/li\u003e\n\u003cli\u003eJohn C. Duchi, Michael I. Jordan, and Martin Wainwright. Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation. Advances in Neural Information Processing Systems (2013)\u003c/li\u003e\n\u003cli\u003eDwork, C., \u0026amp; Roth, A. (2014). The algorithmic foundations of differential privacy. Foundations and Trends® in Theoretical Computer Science, 9(3-4), 211-407.\u003c/li\u003e\n\u003cli\u003eNarayanan, A., \u0026amp; Shmatikov, V. (2008). Robust de-anonymization of large sparse datasets. In Security and Privacy, 2008. SP 2008. IEEE Symposium on (pp. 111-125). IEEE.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cscript\u003e\nfunction highlight(text) {\n  var inputText = document.getElementById(\"markdown-content\");\n  var innerHTML = inputText.innerHTML;\n  var index = innerHTML.indexOf(text);\n  if (index \u003e= 0) {\n    innerHTML = innerHTML.substring(0,index) + \"\u003cspan class='highlight'\u003e\" + innerHTML.substring(index,index+text.length) + \"\u003c/span\u003e\" + innerHTML.substring(index + text.length);\n    inputText.innerHTML = innerHTML;\n  }\n}\nhighlight(\"Estimating Privacy in Data Science\");\n\n\u003c/script\u003e\n\u003chr\u003e\n\u003cscript\u003e\n    function displayInput() {\n        var inputValue = document.getElementById(\"inputField\").value;\n        document.getElementById(\"output\").innerText = \"You typed: \" + inputValue;\n    }\n\u003c/script\u003e\n\u003cstyle\u003e\n.highlight {\n  background-color: red;\n}\n.highlight-on-hover:hover {\n        background-color: yellow;\n    }\n/* Quiz form styles */\n.quiz-form {\n        max-width: 500px;\n        margin: auto;\n        padding: 20px;\n        border: 1px solid #ccc;\n        border-radius: 5px;\n        background-color: #f9f9f9;\n}\n\n.quiz-question {\n        margin-bottom: 20px;\n}\n\n.quiz-options label {\n        display: block;\n        margin-bottom: 10px;\n}\n\n.quiz-submit {\n        background-color: #4caf50;\n        color: white;\n        padding: 10px 20px;\n        border: none;\n        border-radius: 5px;\n        cursor: pointer;\n}\n\n.quiz-submit:hover {\n        background-color: #45a049;\n}\n\n/* Quiz results styles */\n.quiz-results {\n        margin-top: 20px;\n        font-weight: bold;\n}\n.quiz-options label {\n        display: block;\n        margin-bottom: 10px;\n    }\n.quiz-options label.correct {\n        color: green;\n}\n.quiz-options label.incorrect {\n        color: red;\n}\na[name]:hover {\n        background-color: yellow; /* Change to the same color as normal state to maintain yellow highlight */\n        text-decoration: none; /* Optionally remove underline on hover */\n}\n\u003c/style\u003e\n\u003cstyle TYPE=\"text/css\"\u003e\ncode.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n\u003c/style\u003e\n\u003cscript type=\"text/x-mathjax-config\"\u003e\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n    }\n});\nMathJax.Hub.Queue(function() {\n    var all = MathJax.Hub.getAllJax(), i;\n    for(i = 0; i \u003c all.length; i += 1) {\n        all[i].SourceElement().parentNode.className += ' has-jax';\n    }\n});\n\u003c/script\u003e\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full\"\u003e\u003c/script\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/statistical_minimax_rates_under_privacy/","date_published":"31016-31-09T122:3131:00+01:00","date_modified":"31016-31-09T122:3131:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"64ca00e483d62f1f64025e3844d00befbed90629","title":"Measuring the Transferability of Pre-trained Models: a link with Neural Collapse Distances on Target Datasets","summary":"","content_text":"Authors : Marion Chadal and Julie Massé\nThis blog post discusses the paper \u0026ldquo;How Far Pre-trained Models Are from Neural Collapse on the Target Dataset Informs their Transferability\u0026rdquo; [1]. It provides an explanation of it so that you can understand the usefulness of measuring transferability, and a reproduction of the authors\u0026rsquo; experiment so that you can better visualize their methodology.\nPre-trained models and fine-tuning Pre-trained models are currently one of the most active fields in Machine Learning. They can be found in a wide range of applications, from image recognition and natural language processing to autonomous driving and medical diagnosis. These models are \u0026ldquo;pre-trained\u0026rdquo; on massive datasets, most of the time encompassing millions of examples across diverse domains. The training process leverages Deep Learning algorithms and can take weeks or even months, utilizing powerful computing resources to iteratively adjust the model\u0026rsquo;s parameters until it achieves high accuracy on the training data.\nThe first purpose of pre-training is to enable the model to learn a broad understanding of the world, capturing intricate patterns, relationships, and features that are not easily discernible. This extensive learning phase allows the model to develop a deep amount of knowledge, which it can then apply to more specific tasks through a process known as fine-tuning.\nWhat is fine-tuning? It consists in adapting a general-purpose model to perform well on a specific task. This adaptation allows the model to fine-tune its learned features to better align with the nuances of the new task, enhancing its accuracy and performance. Whether it\u0026rsquo;s identifying specific types of objects in images, understanding the subtleties of natural language in a particular context, or diagnosing medical conditions from scans, fine-tuning enables pre-trained models to become specialized tools capable of tackling a wide range of applications.\nFine-tuning begins with a pre-trained model—a model that has already learned a vast array of features and patterns from a comprehensive dataset, often spanning millions of examples. This model, equipped with a deep understanding of various data representations, serves as a robust starting point. The fine-tuning process then adapts this model to a specific task by continuing the training process on a smaller, task-specific dataset. This additional training phase is typically shorter and requires significantly fewer data and computational resources than training a model from scratch, as the model already possesses a foundational knowledge base.\nOne of the key aspects of fine-tuning is its efficiency in data utilization. Since the model has already learned general features and patterns, the fine-tuning process can achieve high performance with relatively small datasets. This characteristic is particularly valuable in domains where collecting large amounts of labeled data is challenging or expensive.\nTraining from scratch is the complete opposite of fine-tuned pre-trained models, as it involves starting with randomly initialized parameters and requires a substantial dataset specific to the task at hand, along with considerable computational resources and time to achieve comparable performance to a fine-tuned pre-trained model. While training from scratch can be beneficial in certain scenarios where highly specialized knowledge is required or when a suitable pre-trained model is not available, the efficiency and effectiveness of leveraging pre-trained models are nowadays undeniable.\nTransferability Transferability caracterizes the ability of pre-trained models to run on downstream tasks without performing fine-tuning, but achieving comparable results. Models that exhibit high transferability are those that have learned generalizable features during pre-training—features that are not overly specific to the training data but that capture universal patterns or structures present across different datasets and domains.\nBeside, transferability arises as an attempt of improvement in scalable AI, as it enables researchers and practitioners to build upon existing knowledge without reinventing the wheel for every new task. This characteristic is especially crucial in our current case where data is abundant, but labeled data is scarce or expensive to obtain. Transferable models can leverage unlabeled data from similar domains, or even entirely different domains, to achieve impressive results with minimal effort.\nMoreover, the pursuit of enhancing transferability has led to innovations in model architecture, training strategies, and domain adaptation techniques. Few-shot learning for instance, where models learn from a very small amount of labeled data, and zero-shot learning, where models apply their knowledge to tasks they have not explicitly been trained on.\nThe concept of transferability also intersects with ethical AI development, as it encourages the use of more generalizable models that can perform equitably across diverse datasets and demographics, reducing the risk of biased or unfair outcomes.\nWhy measuring transferability? Fine-tuning pre-trained models works as follows. First, you pick a downstream task, for which you have at your disposal several pre-trained models candidates. You want to compare their performances to pick the best one on test set, with the optimal fine-tuning configuration. Then, you have to fine-tune each of them. Even if the dataset to train on is smaller, thanks to fine-tuning, you have to repeat it for all your models candidates, and one does not want that, as it can quickly become computationnally expensive.\nTransferability estimation arises as a solution to anticipate and avoid unnecessary fine-tuning, by ranking the performances of pre-trained models on a downstream task without any fine-tuning. Having a benchmark on the pre-trained models\u0026rsquo; transferability would allow you to pick the relevant ones for your own downstream task.\nThis measure is also in line with frugality in AI, which means using limited resources at every step of the Machine Learning lifecycle, while maintaining an acceptable accuracy. This frugality is especially relevant for small and medium-sized enterprises (SMEs) or startups, which may not have the vast computational resources that larger corporations possess. Transferable models democratize access to advanced AI capabilities, enabling these smaller entities to innovate and compete effectively. Frugality in AI also speaks to the broader goal of creating models that are not only powerful but also lean and efficient. Models with high transferability can achieve excellent performance across multiple tasks using significantly less data and fewer computational resources. This efficiency reduces the carbon footprint of training models and makes AI more accessible to a wider range of users and applications.\nNeural Collapse Neural Collapse happens when training beyond 0 training error, i.e training error is at 0 while pushing training loss approaching 0 even further down. Imagine training a deep neural network on a dataset for a classification task. As the training process nears its end—particularly when the model is trained to a point of perfect or near-perfect classification accuracy on the training data. Intuitively, one would expect a highly overfitted and noisy model. Instead, a remarkable simplification occurs in the way the model represents the data, as it was shown in [2]. This training approach offers better generalization performance, better robustness, and better interpretability.\nNeural Collapse is characterized by three distinct proxies:\nWithin-Class Variability Collapse: for any given class, the feature vectors of all samples converge to a singular point or a tightly compact cluster in the high-dimensional feature space. This collapsing effect reduces the within-class variance to near zero, meaning that all samples of a class are represented almost identically from the model\u0026rsquo;s perspective ; Simplex Encoded Label Interpolation (SELI) geometry: measures the gap between the features extracted by the pre-trained model and SELI geometry with the rank of the feature matrix. The higher the rank, the smaller the difference, the closer to Neural Collapse ; Nearest Center Classifier: ensures that the means of the collapsed points for different classes are maximally separated in the feature space. Let\u0026rsquo;s look at this visual example of neural collapse :\nWhere :\nThe Green Balls represent the coordinates of a simplex equiangular tight frame (ETF). The Red Lines represent the Final Layer Classifier. The direction of the sticks indicates the orientation of its decision boundaries, while the ball-end represents the centroid in the feature space used for classification. The Blue Lines represent the class means of the activations in the last hidden layer. The sticks show the variance around these means. The Small Blue Balls represent the last hidden layer activations. It shows how data points from each class are distributed around the class means, forming tight clusters. Initially these elements are all scattered, but as training progresses and neuronal collapse occurs, at each epoch, they move and converged gradually as shown in the GIF.\nWhy choosing Neural Collapse proxies? Let\u0026rsquo;s go back to imagining you have to perform a downstream task, and to do so you have to measure transferability between pre-trained models candidates. The three Neural Collapse proxies were previously defined, but we did not mention yet the three model\u0026rsquo;s aspects that are crucial to evaluate when choosing one:\nGeneralization: through Within-Class Variability Collapse, we gain insight into a model\u0026rsquo;s ability to generalize ; Interpretability: the convergence toward SELI geometry not only enhances the model\u0026rsquo;s interpretability but also its alignment with optimal data representation structures. This alignment signifies a model\u0026rsquo;s capacity to distill and encode information in a way that mirrors the inherent structure of the data itself ; Robustness: the Nearest Center Classifier proxy underscores a model\u0026rsquo;s robustness. By ensuring that class means are well-separated, the model demonstrates resilience against noise and variability in data. Authors in [3] demonstrate both theoretically and empirically that Neural Collapse not only generalizes to new samples from the same classes seen during training but also, and more crucially, to entirely new classes. Also, a more recent research [4] proposes a fine-tuning method based on Neural Collapse that achieves even better performance while reducing fine-tuning parameters by at least 70% !\nThe NCTI Given these promising results, the authors developed a transferability estimation metric : the Neural Collapse Transferability Index (NCTI). This metric measures the proximity between the current state of a pre-trained model and its final fine-tuning stage on target, using the three neural collapse proxies defined above : Within-Class Variability Collapse, SELI geometry and Nearest Center Classifier. For each of them, a score is established : $S^m_{vc}$, $S^m_{seli}$ and $S^{m}_{ncc}$. These three scores are then grouped together using normalization to prevent one score from dominating due to different scales. The final transferability estimation metric is obtained by adding the normalized scores:\n$$ S^m_{total} = S^m_{vc}(H^m) + S^m_{seli}(H^m) + S^{m}_{ncc}(H^m) $$\nWhere $H_m$ is the feature extracted by the $m$-th pre-trained model (after ranking a set of $M$ pre-trained models).\nThe higher the score $S^m_{total}$, the better the transferability of the model for target dataset.\nLet\u0026rsquo;s detail the scores $S^m_{vc}$, $S^m_{seli}$ and $S^{m}_{ncc}$:\nWithin-Class Variability Collapse The authors noticed that larger singular values indicate higher within-class variability because the features within the class exhibit significant variation from the mean, which is desirable for effective feature representation. But since singular value decomposition (SVD) is computationally expensive for large matrices, the nuclear norm which calculates the sum of singular values in a less expensive way was used. Additionally, as feature spaces are high dimensionnal, noise may appear and affect the calculation of variability. Therefore, instead of using the feature matrix $H^m_c$ directly, the classwise logits $Z^m_c$ are substituted to calculate the feature variability.\nThus, the score $S_{vc}$ is calculated as follow :\n$$ S^m_{vc}(H^m) = - \\sum_{c=1}^{C} ||Z^m_c||_* $$\nWhere $Z^m_c$ denotes the logits of the $c$-th class extracted by the $m$-th model.\nThe higher the score $S_{vc}$, the higher the within-class variability, which means that the pre-trained model is closer to the final fine-tuning stage.\nSELI geometry SELI geometry is a concept proposed in [6] as a generalized geometric structure version of the simplex equiangular tight frame (ETF). ETF is defined in the context of the phenomenon of neuronal collapse, but it is limited to balanced datasets. In contrast, SELI extends this concept to both balanced and unbalanced datasets. Difference between the two geometries is shown in the figure below :\nEmbeddings $H$ (in blue) and classifiers $W$ (in red) follow the SELI geometry if :\n$$ W^T W \\alpha V \\Lambda V^T, H^T H \\alpha U \\Lambda U^T \\text{and} W^T H \\alpha \\hat{Z} $$\nWhere $\\hat{Z} = V \\Lambda U^T$ is the SEL matrix [6]. $U$ and $V$ denote the left and right singular vector matrix of $\\hat{Z}$. $\\Lambda$ represents the diagonal singular value matrix.\nA method to assess the SELI geometry structure involves computing the difference between the logits $Z^m$ extracted from the pre-trained model and the optimal logits $\\hat{Z}$. However, obtaining $Z^m$ directly without fine-tuning on the target dataset is time-consuming. Therefore, features $H^m$ of the model are extracted and their difference is measured to form the SELI structure. The complexity of achieving the optimal logits $\\hat{Z}$ through features $H_m$ is approximated via the nuclear norm.\nThus, the score $S^m_{seli}$ is calculated as :\n$$S^m_{seli}(H^m) = ||H^m||_*$$\nThe higher the score $S^m_{seli}$ the higher the rank of the feature matrix $H_m$, making $Z$ closer to a full rank matrix.\nNearest Center Classifier First, the posterior probability $P(y = c|h)$ for each class $c$ is calculated using Bayes\u0026rsquo; Rule:\n$$ \\log P(y = c|h) = \\frac{1}{2}(h_i - \\mu_c)^T \\Sigma (h_j - \\mu_c) + \\log P(y = c) $$\nWhere:\n$\\mu_c$ is the mean vector for class $c$. $\\Sigma$ is the covariance matrix. $P(y = c)$ is the prior probability of class $c$. $h$ is the feature vector extracted by the pre-trained model. Next, the softmax function is applied to obtain the normalized posterior probability $z^m_{i,c}$ for each class $c$ of the $i$-th sample:\n$$ z^m_{i,c} = \\frac{\\exp(\\log P(y = c|h^m_i))}{\\Sigma ^C_{k=1} \\exp(\\log P(y = k|h^m_i))} $$\nWhere:\n$C$ is the number of classes. $h^m_i$ is the feature vector of the $i$-th sample extracted by the m-th pre-trained model. Finally, the score $S^m_{ncc}$ is computed as the average of the dot product of the normalized posterior probabilities $z^m_i$ and the ground truth labels $y_i$ for all samples:\n$$ S^m_{ncc}(H^m) = \\frac{1}{N} \\Sigma ^N_{i=1} z^m_i \\cdot y_i $$\nWhere:\n$N$ is the number of samples. $y_i$ is the ground truth label of the $i$-th sample (in one-hot encoding). The higher the score $S^{m}_{ncc}(H^m)$, the smaller the deviation to the nearest optimal centroid classifier and therefore the greater the transferability to the target dataset.\nNumerical Experiment To reproduce their experiment, the authors\u0026rsquo; code available on a Github repository was used. A first encountered issue was the required torch and torchvision versions, which are quite old, and thus not always available to install, which was the case here. Fortunately, the most recent versions were compatible with the code. A requirements.txt file would have been welcome.\nA second issue is that there are remaining personal paths in some scripts, which should be replaced by downloading paths to PyTorch source models. As a consequence, the loading method from torch should also be replaced.\nOther issues considering the datasets loading remained unsolved.\nAfter these modifications, it is possible to run the authors\u0026rsquo; experiments on the CIFAR10 dataset for the group of supervised pre-trained models. Consisting of 60 000 32x32 colour images in 10 classes, this dataset is broadly used in benchmarks for image classification. 12 pre-trained models were ran on CIFAR10 to establish a ranking based on their performances in terms of NCTI available below.\nModel NCTI Score ResNet152 2.0 ResNet101 1.799 DenseNet201 1.434 DenseNet169 1.146 ResNet34 0.757 ResNet50 0.709 DenseNet121 0.655 MnasNet1_0 0.031 GoogleNet -0.251 MobileNetV2 -0.444 InceptionV3 -0.732 Results show that the deepest architectures offer the best NCTI scores. The depth of a network is closely related to its ability to learn and represent complex features and patterns from the training data, which contributes to a model\u0026rsquo;s superior transferability. The different performances between ResNet and DenseNet could be attributed to the way DenseNet connects each layer to every other layer in a feed-forward fashion, which, while efficient in parameter use and reducing overfitting, may not capture as complex a feature hierarchy as ResNet. Models like MnasNet, MobileNetV2, and InceptionV3, designed for efficiency and speed with a compromise on depth, understandably score lower in transferability, as reflected by their NCTI scores.\nThen, we evaluated the transferability of the supervised pre-trained models, in terms of weighted Kendall\u0026rsquo; τ, and obtained the exact same result as the one presented in the paper: 0.843.\nIt was not possible for us to run the experiment on the group of self-supervised pre-trained models as the authors\u0026rsquo; code included personal paths, and we were not able to find them online.\nA Github repository with all the necessary modifications from the original code is at your disposal here.\nWhat about source features? Through extensive testing, authors have identified that two specific attributes related to neural collapse, observed in the source features, consistently predicted the model\u0026rsquo;s performance on new tasks. These attributes were the diversity within data categories and the compactness of category representations. Remarkably, models showing higher within-category diversity and more compact category representations in their source features tended to adapt better to new tasks. On the other hand, SELI did not consistently correlate with transferability.\nChallenges Authors did experiments on the effectiveness of each individual component in NCTI. They used the three terms individually and removed them one at a time from the full system, and it turned out that for supervised learning, the NCTI without NCC achieved the best weighted Kendall\u0026rsquo; τ. Instead of having normalized the three NCTI components equally, it could have been interesting to tune hyperparameters. Moreover, the current implementation and validation of NCTI are confined to image classification tasks, suggesting its applicability may be limited to similar types of problems. Future work could extend the method\u0026rsquo;s applicability to a broader range of tasks beyond classification, such as detection or segmentation​​. Pre-trained language models could also be considered to measure their transferability based on Neural Collapse. For example, the Fair Collapse (FaCe) method [7] considers both Computer Vision and Natural Language Processing tasks, using different proxies of Neural Collapse than NCTI, and producing a slightly less good τ on the CIFAR-10 dataset (0.81).\nTakeaways Key points to remember are :\nCalculating model transferability and choosing the optimal pre-trained model is important for reasons of computational cost, environmental impact, and overall performance.\nThe authors have developed a new metric, the Neural Collapse informed Transferability Index (NCTI), which is based on the concept of neural collapse and measures the gap between the current feature geometry and the geometry at the terminal stage after hypothetical fine-tuning on the downstream task.\nThe NCTI metric integrates three aspects equally: SELI geometry, within-class variability, and nearest center classifier.\nThis method is light to compute, enabling rapid evaluation of model transferability.\nEmpirical results demonstrate that the ranking of model transferability has a very strong correlation with the ground truth ranking and compares with state-of-the-art methods, highlighting its effectiveness in selecting pre-trained models for specific tasks.\nIn summary, the development of metrics such as NCTI is crucial for optimizing the use of pre-trained models, considering both performance and associated costs in real-world applications.\nReferences 1. Z. Wang Y.Luo, L.Zheng, Z.Huang, M.Baktashmotlagh (2023), How far pre-trained models are from neural collapse on the target dataset informs their transferabilityWang, ICCV.\n2. V. Papyan,1 , X. Y. Hanb,1 , and D.L. Donoho (2020), Prevalence of neural collapse during the terminal phase of deep learning training, National Academy of Sciences.\n3. Galanti, T., György, A., \u0026amp; Hutter, M. (2021). On the role of neural collapse in transfer learning. arXiv preprint arXiv:2112.15121.\n4. Li, X., Liu, S., Zhou, J., Lu, X., Fernandez-Granda, C., Zhu, Z., \u0026amp; Qu, Q. (2022). Principled and efficient transfer learning of deep models via neural collapse. arXiv preprint arXiv:2212.12206.\n5. Vignesh Kothapalli, (2023). Neural Collapse: A Review on Modelling Principles and Generalization. arXiv preprint arXiv:2206.04041.\n6. Christos Thrampoulidis, Ganesh R Kini, Vala Vakilian, and Tina Behnia. (2022). Imbalance trouble: Revisiting neural-collapse geometry. arXiv preprint arXiv:2208.05512.\n7. Yuhe Ding, Bo Jiang, Lijun Sheng, Aihua Zheng, Jian Liang. (2023). Unleashing the power of neural collapse for transferability estimation. arXiv preprint arXiv:2310.05754v1.\nStart writing here !\n","content_html":"\u003cp\u003e\u003cstrong\u003eAuthors\u003c/strong\u003e : Marion Chadal and Julie Massé\u003c/p\u003e\n\u003cp\u003eThis blog post discusses the paper \u0026ldquo;How Far Pre-trained Models Are from Neural Collapse on the Target Dataset Informs their Transferability\u0026rdquo; \u003ca href=\"#ref1\"\u003e[1]\u003c/a\u003e. It provides an explanation of it so that you can understand the usefulness of measuring transferability, and a reproduction of the authors\u0026rsquo; experiment so that you can better visualize their methodology.\u003c/p\u003e\n\u003ch1 id=\"pre-trained-models-and-fine-tuning\"\u003ePre-trained models and fine-tuning\u003c/h1\u003e\n\u003cp\u003ePre-trained models are currently one of the most active fields in Machine Learning. They can be found in a wide range of applications, from image recognition and natural language processing to autonomous driving and medical diagnosis. These models are \u0026ldquo;pre-trained\u0026rdquo; on massive datasets, most of the time encompassing millions of examples across diverse domains. The training process leverages Deep Learning algorithms and can take weeks or even months, utilizing powerful computing resources to iteratively adjust the model\u0026rsquo;s parameters until it achieves high accuracy on the training data.\u003c/p\u003e\n\u003cp\u003eThe first purpose of pre-training is to enable the model to learn a broad understanding of the world, capturing intricate patterns, relationships, and features that are not easily discernible. This extensive learning phase allows the model to develop a deep amount of knowledge, which it can then apply to more specific tasks through a process known as fine-tuning.\u003c/p\u003e\n\u003cp\u003eWhat is \u003cstrong\u003efine-tuning\u003c/strong\u003e? It consists in adapting a general-purpose model to perform well on a specific task. This adaptation allows the model to fine-tune its learned features to better align with the nuances of the new task, enhancing its accuracy and performance. Whether it\u0026rsquo;s identifying specific types of objects in images, understanding the subtleties of natural language in a particular context, or diagnosing medical conditions from scans, fine-tuning enables pre-trained models to become specialized tools capable of tackling a wide range of applications.\u003c/p\u003e\n\u003cp\u003eFine-tuning begins with a pre-trained model—a model that has already learned a vast array of features and patterns from a comprehensive dataset, often spanning millions of examples. This model, equipped with a deep understanding of various data representations, serves as a robust starting point. The fine-tuning process then adapts this model to a specific task by continuing the training process on a smaller, task-specific dataset. This additional training phase is typically shorter and requires significantly fewer data and computational resources than training a model from scratch, as the model already possesses a foundational knowledge base.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003cimg src=\"https://github.com/marionchadal/responsible-ai-datascience-ipParis.github.io/blob/main/static/images/ChadalMasse/schema.png\" width=\"600\"/\u003e\n\u003c/p\u003e\n\u003cp\u003eOne of the key aspects of fine-tuning is its efficiency in data utilization. Since the model has already learned general features and patterns, the fine-tuning process can achieve high performance with relatively small datasets. This characteristic is particularly valuable in domains where collecting large amounts of labeled data is challenging or expensive.\u003c/p\u003e\n\u003cp\u003eTraining from scratch is the complete opposite of fine-tuned pre-trained models, as it involves starting with randomly initialized parameters and requires a substantial dataset specific to the task at hand, along with considerable computational resources and time to achieve comparable performance to a fine-tuned pre-trained model. While training from scratch can be beneficial in certain scenarios where highly specialized knowledge is required or when a suitable pre-trained model is not available, the efficiency and effectiveness of leveraging pre-trained models are nowadays undeniable.\u003c/p\u003e\n\u003ch1 id=\"transferability\"\u003eTransferability\u003c/h1\u003e\n\u003cp\u003eTransferability caracterizes the \u003cem\u003eability of pre-trained models to run on downstream tasks without performing fine-tuning, but achieving comparable results\u003c/em\u003e. Models that exhibit \u003cstrong\u003ehigh transferability\u003c/strong\u003e are those that have learned \u003cstrong\u003egeneralizable features\u003c/strong\u003e during pre-training—features that are not overly specific to the training data but that capture universal patterns or structures present across different datasets and domains.\u003c/p\u003e\n\u003cp\u003eBeside, transferability arises as an attempt of improvement in \u003cstrong\u003escalable AI\u003c/strong\u003e, as it enables researchers and practitioners to build upon existing knowledge without reinventing the wheel for every new task. This characteristic is especially crucial in our current case where data is abundant, but labeled data is scarce or expensive to obtain. Transferable models can leverage unlabeled data from similar domains, or even entirely different domains, to achieve impressive results with minimal effort.\u003c/p\u003e\n\u003cp\u003eMoreover, the pursuit of enhancing transferability has led to innovations in model architecture, training strategies, and domain adaptation techniques. \u003cstrong\u003eFew-shot learning\u003c/strong\u003e for instance, where models learn from a very small amount of labeled data, and zero-shot learning, where models apply their knowledge to tasks they have not explicitly been trained on.\u003c/p\u003e\n\u003cp\u003eThe concept of transferability also intersects with \u003cstrong\u003eethical AI\u003c/strong\u003e development, as it encourages the use of more generalizable models that can perform equitably across diverse datasets and demographics, reducing the risk of biased or unfair outcomes.\u003c/p\u003e\n\u003ch1 id=\"why-measuring-transferability\"\u003eWhy measuring transferability?\u003c/h1\u003e\n\u003cp\u003eFine-tuning pre-trained models works as follows. First, you \u003cstrong\u003epick a downstream task\u003c/strong\u003e, for which you have at your disposal several pre-trained models candidates. You want to compare their performances to pick the best one on test set, with the \u003cstrong\u003eoptimal fine-tuning configuration\u003c/strong\u003e. Then, you have to fine-tune each of them. Even if the dataset to train on is smaller, thanks to fine-tuning, you have to repeat it for all your models candidates, and one does not want that, as it can quickly become \u003cstrong\u003ecomputationnally expensive\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eTransferability estimation arises as a solution to anticipate and avoid unnecessary fine-tuning, by \u003cstrong\u003eranking the performances of pre-trained models\u003c/strong\u003e on a downstream task without any fine-tuning. Having a \u003cstrong\u003ebenchmark on the pre-trained models\u0026rsquo; transferability\u003c/strong\u003e would allow you to pick the relevant ones for your own downstream task.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003cimg src=\"https://github.com/marionchadal/responsible-ai-datascience-ipParis.github.io/blob/main/static/images/ChadalMasse/machine-learning-file-cycle.png\" width=\"250\" height=\"250\"/\u003e\n\u003c/p\u003e\n\u003cp\u003eThis measure is also in line with \u003cstrong\u003efrugality in AI\u003c/strong\u003e, which means using limited resources at every step of the Machine Learning lifecycle, while maintaining an acceptable accuracy. This frugality is especially relevant for small and medium-sized enterprises (SMEs) or startups, which may not have the vast computational resources that larger corporations possess. Transferable models democratize access to advanced AI capabilities, enabling these smaller entities to innovate and compete effectively. Frugality in AI also speaks to the broader goal of creating models that are not only powerful but also lean and efficient. Models with high transferability can achieve excellent performance across multiple tasks using significantly less data and fewer computational resources. This efficiency reduces the carbon footprint of training models and makes AI more accessible to a wider range of users and applications.\u003c/p\u003e\n\u003ch1 id=\"neural-collapse\"\u003eNeural Collapse\u003c/h1\u003e\n\u003cp\u003eNeural Collapse happens when training beyond 0 training error, i.e training error is at 0 while pushing training loss approaching 0 even further down. Imagine training a deep neural network on a dataset for a classification task. As the training process nears its end—particularly when the model is trained to a point of perfect or near-perfect classification accuracy on the training data. Intuitively, one would expect a highly overfitted and noisy model. Instead, a remarkable simplification occurs in the way the model represents the data, as it was shown in \u003ca href=\"#ref2\"\u003e[2]\u003c/a\u003e. This training approach offers better \u003cstrong\u003egeneralization\u003c/strong\u003e performance, better \u003cstrong\u003erobustness\u003c/strong\u003e, and better \u003cstrong\u003einterpretability\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eNeural Collapse is characterized by three distinct proxies:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eWithin-Class Variability Collapse:\u003c/strong\u003e for any given class, the feature vectors of all samples converge to a singular point or a tightly compact cluster in the high-dimensional feature space. This collapsing effect reduces the within-class variance to near zero,  meaning that all samples of a class are represented almost identically from the model\u0026rsquo;s perspective ;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSimplex Encoded Label Interpolation (SELI) geometry:\u003c/strong\u003e measures the gap between the features extracted by the pre-trained model and SELI geometry with the rank of the feature matrix. The higher the rank, the smaller the difference, the closer to Neural Collapse ;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNearest Center Classifier:\u003c/strong\u003e ensures that the means of the collapsed points for different classes are maximally separated in the feature space.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLet\u0026rsquo;s look at this visual example of neural collapse :\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003cimg src=\"https://github.com/marionchadal/responsible-ai-datascience-ipParis.github.io/blob/main/static/images/ChadalMasse/neural_collapse.gif\" width=\"250\" height=\"250\"/\u003e\n\u003c/p\u003e\n\u003cp\u003eWhere :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe \u003cstrong\u003eGreen Balls\u003c/strong\u003e  represent the coordinates of a simplex equiangular tight frame (ETF).\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003eRed Lines\u003c/strong\u003e represent the Final Layer Classifier. The direction of the sticks indicates the orientation of its decision boundaries, while the ball-end represents the centroid in the feature space used for classification.\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003eBlue Lines\u003c/strong\u003e represent the class means of the activations in the last hidden layer. The sticks show the variance around these means.\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003eSmall Blue Balls\u003c/strong\u003e represent the last hidden layer activations. It shows how data points from each class are distributed around the class means, forming tight clusters.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInitially these elements are all scattered, but as training progresses and neuronal collapse occurs, at each epoch, they move and converged gradually as shown in the GIF.\u003c/p\u003e\n\u003ch1 id=\"why-choosing-neural-collapse-proxies\"\u003eWhy choosing Neural Collapse proxies?\u003c/h1\u003e\n\u003cp\u003eLet\u0026rsquo;s go back to imagining you have to perform a downstream task, and to do so you have to measure transferability between pre-trained models candidates. The three Neural Collapse proxies were previously defined, but we did not mention yet the three model\u0026rsquo;s aspects that are crucial to evaluate when choosing one:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGeneralization:\u003c/strong\u003e through Within-Class Variability Collapse, we gain insight into a model\u0026rsquo;s ability to generalize ;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInterpretability:\u003c/strong\u003e the convergence toward SELI geometry not only enhances the model\u0026rsquo;s interpretability but also its alignment with optimal data representation structures. This alignment signifies a model\u0026rsquo;s capacity to distill and encode information in a way that mirrors the inherent structure of the data itself ;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRobustness:\u003c/strong\u003e the Nearest Center Classifier proxy underscores a model\u0026rsquo;s robustness. By ensuring that class means are well-separated, the model demonstrates resilience against noise and variability in data.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAuthors in \u003ca href=\"#ref3\"\u003e[3]\u003c/a\u003e demonstrate \u003cstrong\u003eboth theoretically and empirically\u003c/strong\u003e that Neural Collapse not only generalizes to new samples from the same classes seen during training but also, and more crucially, to entirely new classes. Also, a more recent research \u003ca href=\"#ref4\"\u003e[4]\u003c/a\u003e proposes a fine-tuning method based on Neural Collapse that achieves even better performance while reducing fine-tuning parameters by at least \u003cstrong\u003e70%\u003c/strong\u003e !\u003c/p\u003e\n\u003ch1 id=\"the-ncti\"\u003eThe NCTI\u003c/h1\u003e\n\u003cp\u003eGiven these promising results, the authors developed a transferability estimation metric : the Neural Collapse Transferability Index (NCTI). This metric measures the proximity between the current state of a pre-trained model and its final fine-tuning stage on target, using the three neural collapse proxies defined above : Within-Class Variability Collapse, SELI geometry and Nearest Center Classifier. For each of them, a score is established :  $S^m_{vc}$, $S^m_{seli}$ and $S^{m}_{ncc}$. These three scores are then grouped together using normalization to prevent one score from dominating due to different scales. The final transferability estimation metric is obtained by adding the normalized scores:\u003c/p\u003e\n\u003cp\u003e$$ S^m_{total} = S^m_{vc}(H^m) + S^m_{seli}(H^m) + S^{m}_{ncc}(H^m) $$\u003c/p\u003e\n\u003cp\u003eWhere $H_m$ is the feature extracted by the $m$-th pre-trained model (after ranking a set of $M$ pre-trained models).\u003c/p\u003e\n\u003cp\u003eThe higher the score $S^m_{total}$, the better the transferability of the model for target dataset.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s detail the scores $S^m_{vc}$, $S^m_{seli}$ and $S^{m}_{ncc}$:\u003c/p\u003e\n\u003ch3 id=\"within-class-variability-collapse\"\u003eWithin-Class Variability Collapse\u003c/h3\u003e\n\u003cp\u003eThe authors noticed that larger singular values indicate higher within-class variability because the features within the class exhibit significant variation from the mean, which is desirable for effective feature representation. But since singular value decomposition (SVD) is computationally expensive for large matrices, the nuclear norm which calculates the sum of singular values in a less expensive way was used. Additionally, as feature spaces are high dimensionnal, noise may appear and affect the calculation of variability. Therefore, instead of using the feature matrix $H^m_c$ directly, the classwise logits $Z^m_c$ are substituted to calculate the feature variability.\u003c/p\u003e\n\u003cp\u003eThus, the score $S_{vc}$ is calculated as follow :\u003c/p\u003e\n\u003cp\u003e$$ S^m_{vc}(H^m) = - \\sum_{c=1}^{C} ||Z^m_c||_* $$\u003c/p\u003e\n\u003cp\u003eWhere $Z^m_c$ denotes the logits of the $c$-th class extracted by the $m$-th model.\u003c/p\u003e\n\u003cp\u003eThe higher the score $S_{vc}$, the higher the within-class variability, which means that the pre-trained model is closer to the final fine-tuning stage.\u003c/p\u003e\n\u003ch3 id=\"seli-geometry\"\u003eSELI geometry\u003c/h3\u003e\n\u003cp\u003eSELI geometry is a concept proposed in \u003ca href=\"#ref6\"\u003e[6]\u003c/a\u003e as a generalized geometric structure version of the simplex equiangular tight frame (ETF). ETF is defined in the context of the phenomenon of neuronal collapse, but it is limited to balanced datasets. In contrast, SELI extends this concept to both balanced and unbalanced datasets. Difference between the two geometries is shown in the figure below :\u003c/p\u003e\n\u003cdiv style=\"display: flex; justify-content: center; align-items: center;\"\u003e\n    \u003cimg src=\"https://github.com/marionchadal/responsible-ai-datascience-ipParis.github.io/blob/main/static/images/ChadalMasse/geometry.png\" alt=\"Image 1\" style=\"width: 49%; max-width: 100%; height: auto;\"\u003e\n    \u003cimg src=\"https://github.com/marionchadal/responsible-ai-datascience-ipParis.github.io/blob/main/static/images/ChadalMasse/neural_network.png\" alt=\"Image 2\" style=\"width: 49%; max-width: 100%; height: auto;\"\u003e\n\u003c/div\u003e\n\u003cp\u003eEmbeddings $H$ (in blue) and classifiers $W$ (in red) follow the SELI geometry if :\u003c/p\u003e\n\u003cp\u003e$$ W^T W \\alpha V \\Lambda V^T,  H^T H \\alpha U \\Lambda U^T \\text{and} W^T H \\alpha \\hat{Z} $$\u003c/p\u003e\n\u003cp\u003eWhere $\\hat{Z} = V \\Lambda U^T$ is the SEL matrix \u003ca href=\"#ref6\"\u003e[6]\u003c/a\u003e. $U$ and $V$ denote the left and right singular vector matrix of $\\hat{Z}$. $\\Lambda$ represents the diagonal singular value matrix.\u003c/p\u003e\n\u003cp\u003eA method to assess the SELI geometry structure involves computing the difference between the logits $Z^m$ extracted from the pre-trained model and the optimal logits $\\hat{Z}$. However, obtaining $Z^m$ directly without fine-tuning on the target dataset is time-consuming. Therefore, features $H^m$ of the model are extracted and their difference is measured to form the SELI structure. The complexity of achieving the optimal logits $\\hat{Z}$ through features $H_m$ is approximated via the nuclear norm.\u003c/p\u003e\n\u003cp\u003eThus, the score $S^m_{seli}$ is calculated as :\u003c/p\u003e\n\u003cp\u003e$$S^m_{seli}(H^m) = ||H^m||_*$$\u003c/p\u003e\n\u003cp\u003eThe higher the score $S^m_{seli}$ the higher the rank of the feature matrix $H_m$, making $Z$ closer to a full rank matrix.\u003c/p\u003e\n\u003ch3 id=\"nearest-center-classifier\"\u003eNearest Center Classifier\u003c/h3\u003e\n\u003cp\u003eFirst, the posterior probability $P(y = c|h)$ for each class $c$ is calculated using Bayes\u0026rsquo; Rule:\u003c/p\u003e\n\u003cp\u003e$$ \\log P(y = c|h) = \\frac{1}{2}(h_i - \\mu_c)^T \\Sigma (h_j - \\mu_c) + \\log P(y = c) $$\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\mu_c$ is the mean vector for class $c$.\u003c/li\u003e\n\u003cli\u003e$\\Sigma$ is the covariance matrix.\u003c/li\u003e\n\u003cli\u003e$P(y = c)$ is the prior probability of class $c$.\u003c/li\u003e\n\u003cli\u003e$h$ is the feature vector extracted by the pre-trained model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNext, the softmax function is applied to obtain the normalized posterior probability $z^m_{i,c}$ for each class $c$ of the $i$-th sample:\u003c/p\u003e\n\u003cp\u003e$$ z^m_{i,c} = \\frac{\\exp(\\log P(y = c|h^m_i))}{\\Sigma ^C_{k=1} \\exp(\\log P(y = k|h^m_i))} $$\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$C$ is the number of classes.\u003c/li\u003e\n\u003cli\u003e$h^m_i$ is the feature vector of the $i$-th sample extracted by the m-th pre-trained model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFinally, the score $S^m_{ncc}$ is computed as the average of the dot product of the normalized posterior probabilities $z^m_i$ and the ground truth labels $y_i$ for all samples:\u003c/p\u003e\n\u003cp\u003e$$ S^m_{ncc}(H^m) = \\frac{1}{N} \\Sigma ^N_{i=1} z^m_i \\cdot y_i $$\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$N$ is the number of samples.\u003c/li\u003e\n\u003cli\u003e$y_i$ is the ground truth label of the $i$-th sample (in one-hot encoding).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe higher the score $S^{m}_{ncc}(H^m)$, the smaller the deviation to the nearest optimal centroid classifier and therefore the greater the transferability to the target dataset.\u003c/p\u003e\n\u003ch1 id=\"numerical-experiment\"\u003eNumerical Experiment\u003c/h1\u003e\n\u003cp\u003eTo reproduce their experiment, the authors\u0026rsquo; code available on a \u003ca href=\"https://github.com/BUserName/NCTI/tree/main\"\u003eGithub\u003c/a\u003e repository was used. A first encountered issue was the required \u003ccode\u003etorch\u003c/code\u003e and \u003ccode\u003etorchvision\u003c/code\u003e versions, which are quite old, and thus not always available to install, which was the case here. Fortunately, the  most recent versions were compatible with the code. A \u003ccode\u003erequirements.txt\u003c/code\u003e file would have been welcome.\u003c/p\u003e\n\u003cp\u003eA second issue is that there are remaining personal paths in some scripts, which should be replaced by downloading paths to PyTorch source models. As a consequence, the loading method from \u003ccode\u003etorch\u003c/code\u003e should also be replaced.\u003c/p\u003e\n\u003cp\u003eOther issues considering the datasets loading remained unsolved.\u003c/p\u003e\n\u003cp\u003eAfter these modifications, it is possible to run the authors\u0026rsquo; experiments on the CIFAR10 dataset for the group of supervised pre-trained models. Consisting of 60 000 32x32 colour images in 10 classes, this dataset is broadly used in benchmarks for image classification. 12 pre-trained models were ran on CIFAR10 to establish a ranking based on their performances in terms of NCTI available below.\u003c/p\u003e\n\u003ctable style=\"width:100%; border-collapse: collapse;\" border=\"1\"\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align:left; padding: 8px;\"\u003eModel\u003c/th\u003e\n      \u003cth style=\"text-align:left; padding: 8px;\"\u003eNCTI Score\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eResNet152\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e2.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eResNet101\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e1.799\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eDenseNet201\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e1.434\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eDenseNet169\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e1.146\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eResNet34\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e0.757\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eResNet50\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e0.709\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eDenseNet121\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e0.655\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eMnasNet1_0\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e0.031\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eGoogleNet\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e-0.251\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eMobileNetV2\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e-0.444\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"padding: 8px;\"\u003eInceptionV3\u003c/td\u003e\n      \u003ctd style=\"padding: 8px;\"\u003e-0.732\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eResults show that the deepest architectures offer the best NCTI scores.  The depth of a network is closely related to its ability to learn and represent complex features and patterns from the training data, which contributes to a model\u0026rsquo;s superior transferability. The different performances between ResNet and DenseNet could be attributed to the way DenseNet connects each layer to every other layer in a feed-forward fashion, which, while efficient in parameter use and reducing overfitting, may not capture as complex a feature hierarchy as ResNet. Models like MnasNet, MobileNetV2, and InceptionV3, designed for efficiency and speed with a compromise on depth, understandably score lower in transferability, as reflected by their NCTI scores.\u003c/p\u003e\n\u003cp\u003eThen, we evaluated the transferability of the supervised pre-trained models, in terms of weighted Kendall\u0026rsquo; τ, and obtained the exact same result as the one presented in the paper: \u003cstrong\u003e0.843\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eIt was not possible for us to run the experiment on the group of self-supervised pre-trained models as the authors\u0026rsquo; code included personal paths, and we were not able to find them online.\u003c/p\u003e\n\u003cp\u003eA Github repository with all the necessary modifications from the original code is at your disposal \u003ca href=\"https://github.com/marionchadal/NCTI\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch1 id=\"what-about-source-features\"\u003eWhat about source features?\u003c/h1\u003e\n\u003cp\u003eThrough extensive testing, authors have identified that two specific attributes related to neural collapse, observed in the source features, consistently predicted the model\u0026rsquo;s performance on new tasks. These attributes were the diversity within data categories and the compactness of category representations. Remarkably, models showing higher within-category diversity and more compact category representations in their source features tended to adapt better to new tasks. On the other hand, SELI did not consistently correlate with transferability.\u003c/p\u003e\n\u003ch1 id=\"challenges\"\u003eChallenges\u003c/h1\u003e\n\u003cp\u003eAuthors did experiments on the effectiveness of each individual component in NCTI. They used the three terms individually and removed them one at a time from the full system, and it turned out that for supervised learning, the NCTI without NCC achieved the best weighted Kendall\u0026rsquo; τ. Instead of having normalized the three NCTI components equally, it could have been interesting to tune hyperparameters.\nMoreover, the current implementation and validation of NCTI are confined to image classification tasks, suggesting its applicability may be limited to similar types of problems. Future work could extend the method\u0026rsquo;s applicability to a broader range of tasks beyond classification, such as detection or segmentation​​. Pre-trained language models could also be considered to measure their transferability based on Neural Collapse. For example, the Fair Collapse (FaCe) method \u003ca href=\"#ref7\"\u003e[7]\u003c/a\u003e considers both Computer Vision and Natural Language Processing tasks, using different proxies of Neural Collapse than NCTI, and producing a slightly less good τ on the CIFAR-10 dataset (0.81).\u003c/p\u003e\n\u003ch1 id=\"takeaways\"\u003eTakeaways\u003c/h1\u003e\n\u003cp\u003eKey points to remember are :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eCalculating model transferability and choosing the optimal pre-trained model is important for reasons of computational cost, environmental impact, and overall performance.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe authors have developed a new metric, the \u003cstrong\u003eNeural Collapse informed Transferability Index (NCTI)\u003c/strong\u003e, which is based on the concept of \u003cstrong\u003eneural collapse\u003c/strong\u003e and measures \u003cem\u003ethe gap between the current feature geometry and the geometry at the terminal stage after hypothetical fine-tuning on the downstream task.\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe NCTI metric integrates three aspects equally: \u003cstrong\u003eSELI geometry\u003c/strong\u003e, \u003cstrong\u003ewithin-class variability\u003c/strong\u003e, and \u003cstrong\u003enearest center classifier\u003c/strong\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThis method is \u003cem\u003elight to compute\u003c/em\u003e, enabling rapid evaluation of model transferability.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEmpirical results demonstrate that \u003cem\u003ethe ranking of model transferability has a very strong correlation with the ground truth ranking\u003c/em\u003e and \u003cstrong\u003ecompares with state-of-the-art methods\u003c/strong\u003e, highlighting its effectiveness in selecting pre-trained models for specific tasks.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn summary, the development of metrics such as NCTI is crucial for optimizing the use of pre-trained models, considering both performance and associated costs in real-world applications.\u003c/p\u003e\n\u003ch1 id=\"references\"\u003eReferences\u003c/h1\u003e\n\u003cp\u003e\u003ca id=\"ref1\"\u003e\u003c/a\u003e1. Z. Wang Y.Luo, L.Zheng, Z.Huang, M.Baktashmotlagh (2023), How far pre-trained models are from neural collapse on the target dataset informs their transferabilityWang, ICCV.\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"ref2\"\u003e\u003c/a\u003e2. V. Papyan,1 , X. Y. Hanb,1 , and D.L. Donoho (2020), Prevalence of neural collapse during the terminal phase of deep learning training, National Academy of Sciences.\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"ref3\"\u003e\u003c/a\u003e3. Galanti, T., György, A., \u0026amp; Hutter, M. (2021). On the role of neural collapse in transfer learning. arXiv preprint arXiv:2112.15121.\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"ref4\"\u003e\u003c/a\u003e4. Li, X., Liu, S., Zhou, J., Lu, X., Fernandez-Granda, C., Zhu, Z., \u0026amp; Qu, Q. (2022). Principled and efficient transfer learning of deep models via neural collapse. arXiv preprint arXiv:2212.12206.\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"ref5\"\u003e\u003c/a\u003e5. Vignesh Kothapalli, (2023). Neural Collapse: A Review on Modelling Principles and Generalization. arXiv preprint arXiv:2206.04041.\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"ref6\"\u003e\u003c/a\u003e6. Christos Thrampoulidis, Ganesh R Kini, Vala Vakilian, and Tina Behnia. (2022). Imbalance trouble: Revisiting neural-collapse\ngeometry. arXiv preprint arXiv:2208.05512.\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"ref7\"\u003e\u003c/a\u003e7. Yuhe Ding, Bo Jiang, Lijun Sheng, Aihua Zheng, Jian Liang. (2023). Unleashing the power of neural collapse for transferability estimation. arXiv preprint arXiv:2310.05754v1.\u003c/p\u003e\n\u003chr\u003e\u003c/hr\u003e\n\u003cp\u003eStart writing here !\u003c/p\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/transferability/","date_published":"8016-08-09T126:88:00+01:00","date_modified":"8016-08-09T126:88:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}},{"id":"94409f4b19cf119747df5e05d9507c6fc3ae2286","title":"Title of the article","summary":"","content_text":"Authors : John Smith and John Smith\nStart writing here !\n","content_html":"\u003cp\u003e\u003cstrong\u003eAuthors\u003c/strong\u003e : John Smith and John Smith\u003c/p\u003e\n\u003chr\u003e\u003c/hr\u003e\n\u003cp\u003eStart writing here !\u003c/p\u003e\n","url":"https://responsible-ai-datascience-ipParis.github.io/posts/my-second-blog/","date_published":"8016-08-09T126:88:00+01:00","date_modified":"8016-08-09T126:88:00+01:00","author":{"name":"Students from M2 Data Science IP Paris","url":"https://responsible-ai-datascience-ipParis.github.io/"}}]}